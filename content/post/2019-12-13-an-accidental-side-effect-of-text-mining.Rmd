---
title: An accidental side effect of text mining
author: ''
date: '2019-12-13'
slug: text-mining-tutorial
categories: []
tags:
  - texmining
  - nlp
  - sentimentanalysis
  - kindle
subtitle: ''
summary: ''
authors: []
lastmod: '2019-12-13T10:48:17+01:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

*Extended version of https://towardsdatascience.com/an-accidental-side-effect-of-text-mining-4b43f8ee1273*
![configtoml](/img/kindlebat.png)
***‚ÄúThe end of our exploring will be to arrive at where we started, and to know the place for the first time.‚Äù
‚Äî T. S. Eliot***

I was reading lately lots of books on productivity and self-development and often I found pieces of advice that I want to re-read later. The highlighting option on kindle makes this very easy. By reading and highlighting consistently **I accumulated a sizeable amount text which was a good representation of the books I read.**

Since I know the content very well. I want to apply **text mining** and **sentiment analysis** to this data so that I can compare the results to my real opinion about the books. **And if they match I will be more convinced to apply it to my next business problem.**

In part, this is also **a self-exploration,** since I could answer questions such as what kind of content I like more, what was their emotional charge?

**Let‚Äôs find the answers.**  

To be objective, I will create an independent character called ‚Äúthe bat‚Äù. His next mission is to hack my data and analyze it to gather insights for selling me more books. Unfortunately, reading is not his specialty.


The room was in a mess when he entered. As he plugged the USB drive on his laptop, he barely heard the radio which was still on. The reporter:

>From the moment we switch on the early morning our cell phones to deal with the flood of information in some form of a whatsapp or facebook message or a tweet until we fall asleep at night overwriting or reading a product review we leave bread crumbs to our personal flavors on internet.

>Many businesses use this unstructured data to drive their sales by better marketing through targeted product recommendations or to segregate their customers‚Ä¶

  
He squeezed his teeth when he saw the data of 21000 lines of text from 28 books. His first encounter was the book ‚ÄúMindset‚Äù by Carol Dweck, where she introduces the concept of **growth mindset.** The idea behind is people who have the growth mindset believe their abilities can be improved by putting effort whereas the people with the fixed mindset believe that their abilities are fixed at birth. As a consequence, people who has the fixed mindset miss the opportunity to get better at many things even though they can. **Simply, because they don‚Äôt believe in it in the first place.**

If you are not familiar with the concept, here is a [video](https://youtu.be/hiiEeMN7vbQ) of Carol Dweck explaining her research on growth mindset.

Long lines of text made him tired, he didn‚Äôt realize how the time passed. Now, he was a man of growth. **He decided to learn text mining.**

He started to learn the R packages for text mining, he didn‚Äôt like the package name tidytext but he was slightly losing his prejudices. It was a long night. He fell asleep on his table as the sun slowly rose.
...

It was lighting my back garden where I could glance from time to time to the trees painted by the snow overnight ‚õÑÔ∏è. Without an idea about how things went on the another part of the town, I continued to read and highlight my kindle as I zip from a glass of red wine üç∑.

...

We will know what happened to our hacker later. First, let‚Äôs go together through his notebook.

**The hacker‚Äôs notes**    

He noted down each step of his text mining plan carefully. Let me help you go through them.

This is how the exported kindle highlights look like.![configtoml](/img/kindleclip.png)


Reading and parsing the text file

```{r, message=FALSE, warning=FALSE}
# Use readLines function to parse the text file

highlights <- readLines("posts_data/Kindle_highlights_Serdar.Rmd", encoding = "UTF-8")

# Create a dataframe where each row is a line from the text

df <- data.frame(highlights)

# Packages

library(tidyverse)   # includes ggplot2, dplyr, tidyr, readr, purrr, tibble, stringr, forcats
library(tidytext)
library(wordcloud2)
```

Every data science project requires some sort of data preparation. **Stop words** are generally the most common words in a language and are usually filtered out before processing of text data.

Let‚Äôs look at the stop_words dataset from the tidytext package. Since it is a long list of words (>1K) **I will print every fifth word as an example.**

```{r, message=FALSE, warning=FALSE}
data(stop_words)

# print every 50th word 

stop_words_small <- stop_words[seq(1, nrow(stop_words), 50),]

stop_words_small %>% print(n=50)
```

Every data science project requires some sort of data preparation. Stop words are generally the most common words in a language and are usually filtered out before processing of text data. 

e.g. they'll  in stop_words

And how the word they‚Äôll appears in the text:

Yellow highlight | Page: 200
Memories are continually revised, along with the meaning we derive from them, so that in the future they‚Äôll be of even more use.

We have to make stop_words and our data compatible, otherwise some words such as they‚Äôll, don‚Äôt, can‚Äôt might appear in our results.

We can use **str_replace_all()** function from stringr package to find all the apostrophes and convert them into single quotes.


```{r, message=FALSE, warning=FALSE}
df$highlights <- str_replace_all(df$highlights, "‚Äô", "'")
```

Now, the text is ready for the frequency analysis. Words in a text mining project are called tokens. We can split the text into single words by unnest_tokens() function from tidytext package, filter the stop_words and count.

```{r, message=FALSE, warning=FALSE}
df <- df %>% unnest_tokens(word, highlights) %>%
             anti_join(stop_words) %>% 
             filter(!word %in% c("highlights","highlight", "page", 
                      "location", "yellow", "pink", "orange", "blue"))
```

He also added here some additional words which frequently appear in kindle highlights output.


**dplyr()** package functions are very useful for grouping and counting the words from the lists that are created.

```{r, message=FALSE, warning=FALSE}
top_kindle_highlights <- df %>% 
 group_by(word) %>% 
 count() %>% 
 arrange(desc(n))
```

He noted down his first insight. **10 most frequent words from my kindle highlights.**

```{r, message=FALSE, warning=FALSE}
top_kindle_highlights
```
If you don‚Äôt like to look at long list of words wordclouds are a good alternative. Wordcloud2 package gives additional customization options for your wordclouds, for example you can use any image as a markup.![configtoml](/img/h7.png)


```{r, message=FALSE, warning=FALSE, eval=FALSE}
wordcloud2(top_kindle_highlights, figPath = bat, size = 1, backgroundColor = "white", color = color_vector(data$freq) )
```

Some ideas started to get shaped in his mind. He thought who made those highlights is someone interested in storytelling, writing and good communication, good habits, and people. **Someone who wants to influence his life in a positive way. He was becoming more and more interested in the books.**

He wanted to dig deeper.

**Bigram Analysis**  

Single words are a good starting point what the books were about. **But they are limited without the context.** Frequency analysis can also be performed to measure how often word pairs (bigrams) occur in the text. This allows us to capture finer details in the text.

  
To do this he combined the unnested single tokens which is isolated above back into a continuous text and then performed bigram analysis. You can use **str_c()** function from stringr package to concatenate the single words.

```{r, message=FALSE, warning=FALSE}
# Recreate the df
df <- data.frame(highlights)
df$highlights <- str_replace_all(df$highlights, "‚Äô", "'")

df <- df %>% unnest_tokens(word, highlights) %>% 
  anti_join(stop_words) %>% 
 filter(!word %in% c("highlights","highlight", "page", 
                      "location", "yellow", "pink", "orange", "blue",
                      "export", "hidden", "truncated", "kindle", "note", "limits"))

df_com <- str_c(df$word, " ") 
df_com <- data.frame(df_com)
```

Let's split the text into bigrams and count the most common word pairs.

```{r,  message=FALSE, warning=FALSE}
df_bigram <- df_com %>% 
 unnest_tokens(bigram, df_com, token = "ngrams", 
 n = 3, n_min = 2)
top_bigrams <- df_bigram %>% 
 group_by(bigram) %>% 
 count() %>% 
 arrange(desc(n))%>% 
 print(n=20)

# And visualize them on a plot

top <- top_bigrams[1:25,]
```

```{r,  message=FALSE, warning=FALSE}
top %>% ungroup() %>% mutate(bigram = fct_reorder(bigram, n)) %>% 
 ggplot(aes(x=bigram, y=n)) + 
 geom_col() + 
 coord_flip() +
 theme_classic() + 
 theme(legend.position = "none",
 text = element_text(size=18))
```

For example, if you go back above in the top 10 most frequent words table 6th word was change. But we didn't know what the change was about. And here we see that one of the most common bigram is behavioral change. It is making more sense. But it can improve to look at each book individually.

We can also do what we did for the whole document for highlights from single books.

But how can we capture them individually?

Let's first look at the text once more.![](/img/kindleclip.png)Before each book **"Your Kindle Notes For:"** appears. 

Let's find out the line numbers for the beginning and the end of each book and use those indexes for fishing out each book.

We will reuse the data frame df we created above. **str_which()** function returns index numbers of the lines which contain a given pattern. In the last step, capturing the text between two consecutive indexes will give us the book between them.

```{r, message=FALSE, warning=FALSE}
# Since I modified df above. I will recreate it again.
df <- data.frame(highlights)
df$highlights <- str_replace_all(df$highlights, "‚Äô", "'")

# Getting the index number for each book

indexes <- str_which(df$highlights, pattern = fixed("Your Kindle Notes For"))
book_names <- df$highlights[indexes + 1]
indexes <-  c(indexes,nrow(df))

# Create an empty list 

books <- list()

# Now the trick. Capture each 28 book separately in a list. 

for(i in 1:(length(indexes)-1)) {
    books[[i]] <- data.frame(df$highlights[(indexes[i]:indexes[i+1]-1)])
    colnames(books[[i]]) <- "word_column"
    books[[i]]$word_column <- as.character(books[[i]]$word_column)
}

```

Let's check whether it worked, for example you can look up the 5th book on our list.

```{r}
head(books[[5]])
head(books[[15]])
```
Now, we have the individual books captured. I will repeat the procedure we used to analyse the whole text above to analyze each of the 28 books by using a for loop.

```{r, message=FALSE, warning=FALSE}
top <- list()
for(i in 1:28){
books[[i]] <- books[[i]] %>% unnest_tokens(word, word_column) %>%
             anti_join(stop_words) %>% 
             filter(!word %in% c("highlights","highlight", "page", 
                      "location", "yellow", "pink", "orange", "blue",
                      "export", "hidden", "truncated", "kindle", "note", "limits"))

# Find out the top words in each book and capture them in a list (top)

top[[i]] <- books[[i]] %>% 
              group_by(word) %>% 
              count() %>% 
              arrange(desc(n))
}
for(i in 1:28){
  print(book_names[[i]])
  print(top[[i]])
}

```

Now, looking at the frequent words from each book we can get more insights what they are about.

The bigrams for the same books.

```{r,  message=FALSE, warning=FALSE}
df <- data.frame(highlights)
df$highlights <- str_replace_all(df$highlights, "‚Äô", "'")

# Getting the index number for each book

indexes <- str_which(df$highlights, pattern = fixed("Your Kindle Notes For"))
book_names <- df$highlights[indexes + 1]
indexes <-  c(indexes,nrow(df))

# Capturing each book individually

books <- list()
for (i in 1:(length(indexes)-1)) {
    books[[i]] <- data.frame(df$highlights[(indexes[i]:indexes[i+1]-1)])
    colnames(books[[i]]) <- "word_column"
    books[[i]]$word_column <- as.character(books[[i]]$word_column)
}

# Next step in the plan was splitting the text into single words by unnest_tokens function.



for(i in 1:28){
books[[i]] <- books[[i]] %>% unnest_tokens(word, word_column) %>%
             anti_join(stop_words) %>% 
             filter(!word %in% c("highlights","highlight", "page", 
                      "location", "yellow", "pink", "orange", "blue",
                      "export", "hidden", "truncated", "kindle", "note", "limits"))
}

# After this preparation step I can combine the single words back into a continous text

for(i in 1:28){
books[[i]] <- str_c(books[[i]]$word, " ") 
books[[i]] <- data.frame(books[[i]]) 
}


df_bigram <- list()

for(i in 1:28){                      
df_bigram[[i]] <- books[[i]] %>% 
       unnest_tokens(bigram, books..i.., token = "ngrams", 
                                     n = 3, n_min = 2)
}

for (i in 1:28){
  print(book_names[i])
df_bigram[[i]] %>% 
  group_by(bigram) %>% 
  count() %>% 
  arrange(desc(n))%>% 
  print(n=10)
  
}
```

If you want to see another example of this capturing process you can have a look at my recent post [here](https://dataatomic.com/r/data-wrangling-text-mining/).


Looking at each book individually, he started to be more and more obsessed about the books in my kindle. He decided to order a couple of them.

**Sentiment analysis** is used to evaluate emotional charge in a text mining project. Most common uses are social media monitoring, customer experience management, and Voice of Customer, to understand how they feel.

The **bing** lexicon categorizes words into positive and negative categories, in a binary fashion. The **nrc** lexicon uses categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust.

**Using bing lexicon**

This gives us the top words contributed to each emotional category. Some examples to note are success, effective, for positive and bad, hard and limits.![](/img/sentiment1.png)

Here is how R produced the above plot:

```{r,  message=FALSE, warning=FALSE}
df <- data.frame(highlights)
df$highlights <- str_replace_all(df$highlights, "‚Äô", "'")
df <- df %>% unnest_tokens(word, highlights) %>% 
  anti_join(stop_words) %>% 
 filter(!word %in% c("highlights","highlight", "page", 
                      "location", "yellow", "pink", "orange", "blue",
                      "export", "hidden", "truncated", "kindle", "note", "limits"))

bing_word_counts <- df %>% inner_join(get_sentiments("bing")) %>% 
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts
```


```{r,  eval=FALSE, message=FALSE, warning=FALSE}
# Top contributors to positive and negative sentiment

bing <- bing_word_counts %>% 
  group_by(sentiment) %>% 
  top_n(10) %>% 
  ggplot(aes(reorder(word, n), n, fill=sentiment)) + 
  geom_bar(alpha=0.8, stat="identity", show.legend = FALSE)+
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y= "Contribution to sentiment", x = NULL) +
  coord_flip()
bing
```

**Using nrc lexion**  

We see that I am more likely to highlight if a text is charged with positive rather than negative sentiment, and individually trust, anticipation and joy rather than fear and sadness.

```{r}
df <- data.frame(highlights)
df$highlights <- str_replace_all(df$highlights, "‚Äô", "'")
df <- df %>% unnest_tokens(word, highlights) %>% 
  anti_join(stop_words) %>% 
 filter(!word %in% c("highlights","highlight", "page", 
                      "location", "yellow", "pink", "orange", "blue",
                      "export", "hidden", "truncated", "kindle", "note", "limits"))

sentiment <- df %>%
        left_join(get_sentiments("nrc")) %>%
        filter(!is.na(sentiment)) %>%
        count(sentiment, sort = TRUE)
sentiment
```


**Normalized sentiments**

One important thing to add, since each emotion category has different number of words in a language. Emotional categories with less words are less likely to appear in a given text. Thus, I would like to normalize them according to their numbers in the lexicon and see how it differs than the above results.

```{r}
# I will add numbers of each categories from the NRC lexicon

lexicon <- c(2317, 3338, 1234, 842, 1483, 691, 1250, 1195, 1060, 535)
polarity <-  c(1,1,1,1,1,0,0,0,0,0)
sentiment <- data.frame(sentiment, lexicon)
norm_sentiment <- sentiment %>% mutate( normalized = n/lexicon) %>% arrange(desc(normalized))
sentiment <- data.frame(norm_sentiment, polarity)
sentiment

# General findings

sentiment %>% group_by(polarity) %>% summarize(n2 = sum(lexicon))
```

Now, **anticipation** is the highest emotion found in the text that I highlighted. This does not seem a coincidence to me. Since most of the books in our analysis is about productivity and self-development. The productivity tips and tools usually contain words associated with anticipation.  

In a similar way, I can look at the sentiment for individual books

```{r, message=FALSE, warning=FALSE}
df <- data.frame(highlights)

# Kindle uses apostrophes (‚Äô), but stop_words uses sigle quotes (') 
# To be able to use all stop_words I should replace apostrophes with quotes
df$highlights <- str_replace_all(df$highlights, "‚Äô", "'")

# Getting the index number for each book

indexes <- str_which(df$highlights, pattern = fixed("Your Kindle Notes For"))
book_names <- df$highlights[indexes + 1]
indexes <-  c(indexes,nrow(df))

# Capturing each book individually

books <- list()
for (i in 1:(length(indexes)-1)) {
    books[[i]] <- data.frame(df$highlights[(indexes[i]:indexes[i+1]-1)])
    colnames(books[[i]]) <- "word_column"
    books[[i]]$word_column <- as.character(books[[i]]$word_column)
}

# Next step in the plan was splitting the text into single words by unnest_tokens function.


for(i in 1:28){
books[[i]] <- books[[i]] %>% unnest_tokens(word, word_column) %>%
             anti_join(stop_words) %>% 
             filter(!word %in% c("highlights","highlight", "page", 
                      "location", "yellow", "pink", "orange", "blue"))
}

sentiment <- list()
for (i in 1:28){
sentiment[[i]] <- books[[i]] %>%
        left_join(get_sentiments("nrc")) %>%
        filter(!is.na(sentiment)) %>%
        count(sentiment, sort = TRUE)
        print(book_names[i])
        print(sentiment[[i]])
}

for (i in 1:28){
sentiment[[i]] %>% 
    filter(sentiment %in% c('positive','negative')) %>% 
    mutate( n2 = n/sum(n)) %>% print()
  }
```


```{r}
books <- str_trunc(book_names, width=22)
all <- list()
for (i in 1:28) {
all[[i]] <- sentiment[[i]] %>% filter(sentiment %in% c('positive','negative')) %>% mutate(n2 = n/sum(n)) %>% print()
}
```


Positivty Map of the books.

```{r}
all_bound <- do.call("rbind", all) %>% filter(sentiment == "positive")

library(ggrepel)
all_bound %>% ggplot(aes(x= book_names, y=n2)) + 
  geom_point() + 
  geom_label_repel(aes(label=books, color = ifelse(n2 <0.55, "red", "blue")), size = 3) +
  theme_classic() +
  theme(legend.position = "none",
        text = element_text(size=18), 
        axis.text.x = element_blank()) + 
  xlab("Books") + 
  ylab("Positivity score")
```


Let‚Äôs look at the book with the lowest positivity score. **‚ÄúMan‚Äôs search for meaning‚Äù.** This [book](https://www.amazon.com/Mans-Search-Meaning-Viktor-Frankl/dp/080701429X) is based on Victor Frankl sufferings during the second world war. This is also kind of expected.

> I am getting more and more convinced text mining is giving good insights.


The book "The Outliers" appeared on the top of the positivity plot was a real outlier here. üòÆ


It is hard to know everything from the beginning and we will go back to make some additional cleaning. The word count from the book ‚ÄúThe Outliers‚Äù is 107. This is really low. So in the next iteration, I would remove it from the analysis since it will not be reliable.

```{r}
book_names[[27]]
top[[27]]
```

The word count from the book "The Outliers" below is 107. This is really low. So in the next iteration, I would remove it from the analysis since it will not be very informative. It is hard to know everything from the beginning and we will go back and make some additional cleaning. 

...

### Summary

It is not feasible to read millions of pages to check whether text mining is reliable. But here I got some data that I know the content and I applied text mining approaches and sentiment analysis.

Both the monograms or bigrams pointed to similar ideas what the books were about. And the sentiments made sense with the genres of the books in my kindle.

Let's come back to our hacker.

An unanticipated side effect of text mining changed him forever. Analyzing my books and gaining the insights made him more and more interested in reading. And he started to care about the world around him. The world was different.

What I was trying to do for myself, he did for himself. He transformed into a better version of himself.

The world was brighter. ‚òÄÔ∏è


The radio disrupted the silence.


>"brrring‚Ä¶..brrring‚Ä¶..brrring‚Ä¶.."


I woke up.


...

Thank you for reading. I hope you've learned something or got inspiration from this. Please feel free to leave comments, suggestions, and questions. (You can always contact me by email at serdar.korur@gmail.com)

You can find the data and the code in my [github](https://github.com/korur/textmining).  


Until next time!

Serdar

