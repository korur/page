---
title: Can machine learning change your life style? Predicting Diabetes
author: ''
date: '2019-10-01'
output:
    html_document:
        toc: true
slug: predict-diseases
categories: []
tags:
  - Machine learning
  - Blog
  - Data Science Process
  - Linear Regression
  - Logistic Regression
  - glmnet
  - Randomforest
  - Data Quality control
  - Feature engineering
  - Pima Indians
subtitle: ''
summary: ''
authors: []
lastmod: '2019-10-01T09:09:36+02:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

Would you be taking care of yourself better if your doctor told today that you have **high risk of diabetes?**

Advances in fields, such as omics and internet of things (sensors that collect data), and centralization of healthcare information (e.g. [OMOP common data model](https://www.ohdsi.org/data-standardization/the-common-data-model/)) enable us to access much wider data sources. 

Gaining insights from those we can improve our well being with better healthcare. Some applications of machine learning tools are;

* Diagnosing diseases earlier
* Identifiying drugs with reduced side effects
* Select patient groups responding to an experimental therapy
* Utilize existing therapies better

**Let's look at an example and try to help some doctors in diagnosing their patients.**

# Problem formulation

> Can we build a Machine learning algorithm to predict which patients will develop Diabetes?

The goal is to predict a Binary **Outcome**: ```Diabetes``` vs ```Healthy```
by using 8 medical indicators.

# Overview of the data
  
  
**The Pima Indians of Arizona and Mexico** have contributed to numerous scientific gains. Their involvement has led to significant findings on genetics of both **type 2 diabetes** and **obesity.** 

The medical indicators recorded are;

**Pregnancies:** Number of times pregnant  
**Glucose:** Plasma glucose concentration a 2 hours in an oral glucose tolerance test  
**BloodPressure:** Diastolic blood pressure (mm Hg)  
**SkinThickness:** Triceps skin fold thickness (mm)  
**Insulin:** 2-Hour serum insulin (mu U/ml)  
**BMI:** Body mass index (weight in kg/(height in m)^2)  
**DiabetesPedigreeFunction:** Diabetes pedigree function  
**Age:** Age (years)  
**Outcome:** Class variable (0 or 1)  

# Data acquisition

I downloaded the [Pima Indians Diabetes Dataset](https://www.kaggle.com/uciml/pima-indians-diabetes-database) from Kaggle and imported it from my local directory.

```{r import data and packages, message=FALSE, warning=FALSE}
diabetes <- read.csv("posts_data/diabetes.csv")

library(dplyr)
library(reshape2)
library(ggplot2)
library(ggcorrplot)
library(pROC)
library(lattice)
library(caret)

```


# Data Quality control

Before counting on any algorithm a good starting point is to **check obvious mistakes and abnormalities in your data.**

Here, I would look at **Missing values**, variable ranges (**min, max values**). A very extreme value might be basically a sign of typing error.

## Understand your Data

How big is the data? Checking the size and also type of variables. 
```{r dim}
dim(diabetes)
knitr::kable(sapply(diabetes, class))
```


Next, what catches my attention is **unexpected zero values in Insulin** below.
```{r head}
knitr::kable(head(diabetes))
```

## Missing Values

Summary gives a good overview of the variables. Any missing data will show up here listed as **"NA's"**. 

```{r summary}
summary(diabetes)
```

I will make visualizations of the variables to see how they are distributed.

```{r Check out how variables are distributed}
gg <- melt(diabetes)
ggplot(gg, aes(x=value, fill=variable)) +
  geom_histogram(binwidth=5)+ 
  facet_wrap(~variable) 
```

Peaks at zero of Skin Thickness and Insulin is obvious here. Technically it is not possible to have Blood Pressure or Glucose levels at 0. 

In cases where the numbers are small we might remove them. Here they seem plenty. **Let's figure it out with a for loop.** 

This will bring me the number of zero containing rows in variables from 2 to 6.
```{r check number of zeros in each variable}
zero_rows <- list()
for(i in 2:6){
zero_rows[[i]] <- length(which(diabetes[,i] == 0))  
}
rows_with_zero <- unlist(zero_rows)
df <- data.frame(rows_with_zero, row.names = names(diabetes[2:6]))
df
```

For instance, in Insulin **374** values are zero. It is unlikely that those are simply entry mistakes. It seems missing values are filled with **zeros** in the data collection phase. 


**How to circumwent this?**

### Convert all **zeroes** to **NAs** and then perform **Median Imputation**

I will use a for loop to remove zeros in all the predictors except Pregnancy and Pedigree function.

```{r Median Imputation}
for(i in 2:6){
# Convert zeros to NAs
diabetes[, i][diabetes[, i] == 0] <- NA
# Calculate median
median <- median(diabetes[, i], na.rm = TRUE)
diabetes[, i][is.na(diabetes[, i])] <- median
}
```

Check if it really happened
```{r head data}
knitr::kable(head(diabetes))
```

For instance, I see that zero values in the insulin variable is replaced with median of insulin which is 125. 

Now, the data is clean and ready for the modeling phase.

# Modeling the data (build, fit and validate a model)

Before going into any complicated model you can start with a simple model to gain some insights about the data. 

## Linear Regression Model

We will create two random subsets of our data in 80/20 proportion as **training and test data.**
Training data will be used to build our model and test data will be reserved to validate it. 
```{r Create Train/test split}
set.seed(22)
# Create train test split
sample_rows <- sample(nrow(diabetes), nrow(diabetes) * 0.8)
# Create the training dataset
dia_train <- diabetes[sample_rows, ]
# Create the test dataset
dia_test <- diabetes[-sample_rows, ]
```

```{r Build a simple model}
# Build a linear model with the train data
lm_dia <- lm(Outcome ~ .,data = dia_train)
summary(lm_dia)
```


```{r Predict test data model}
# We will predict the Outcome for the test data
p<-predict(lm_dia, dia_test)
# Choose a threshold 0.5 to calculate the accuracy of our model
p_05 <- ifelse(p > 0.5, 1, 0)
table(p_05, dia_test$Outcome)
```


```{r Confusion matrix}
conf_mat <- table(p_05, dia_test$Outcome)
accuracy <- sum(diag(conf_mat))/sum(conf_mat)
accuracy
```

By using the pROC package, I will calculate the AUC value for our model.
```{r Calculate AUC}
# Calculate AUC
roc(dia_test$Outcome, p)
```

If I repeat the above procedure, each time I will get a slightly different result depending on the given random train/test split. Depending on the outliers in the data you might get bigger differences. 

We can run above model with 10/100/1000 different random splits to have a good estimate of the real accuracy.

#### How to apply linear model with multiple train/test split

To do this, I will write a function where I can choose number of independent train/test splits.  
  
    
It will return me an average value of the model after chosen number of iteration.   The higher the number of random splits the more stable your estimated AUC.

Let's see how it will work out for our diabetes patients.

```{r multi_split function, warning=FALSE, message=FALSE}

# I will define my function as follows
multi_split <- function(x){
sample_rows <- list()
dia_train <- list()
dia_test <- list()
lm <- list()
p <-  list()
roc_auc <- list()
for(i in 1:x){
  sample_rows[[i]] <- sample(nrow(diabetes), nrow(diabetes) * 0.8)
  # Create the training dataset
  dia_train[[i]] <- diabetes[sample_rows[[i]], ]
  # Create the test dataset
  dia_test[[i]] <- diabetes[-sample_rows[[i]], ]
  lm[[i]] <- lm(Outcome ~ .,data = dia_train[[i]])
  p[[i]] <- predict(lm[[i]], dia_test[[i]])
  
  # Calculate AUC for all "x" number of random splits
  roc_auc[[i]] <- roc(dia_test[[i]]$Outcome, p[[i]])$auc[1]
  lm_mean <- mean(unlist(roc_auc))
}
print(mean(unlist(roc_auc)))
}
```

Let's calculate the average AUC of our model after different number of random splits.

I will run my multi_split functions 3x for 1, 30 and 1000 random train/test splits. I can then compare variances at each number of iteration.

```{r multi split function test, message=FALSE, warning=FALSE}
variance_1_1 <- multi_split(1)
variance_1_2 <- multi_split(1)
variance_1_3 <- multi_split(1)
variance_30_1 <- multi_split(30)
variance_30_2 <- multi_split(30)
variance_30_3 <- multi_split(30)
variance_1000_1 <- multi_split(1000)
variance_1000_2 <- multi_split(1000)
variance_1000_3 <- multi_split(1000)
```


Let's compare Variance levels at **1, 30 and 1000** random splits

```{r changes in variance, message=FALSE, warning=FALSE}
var(c(variance_1_1, variance_1_2, variance_1_3))
var(c(variance_30_1, variance_30_2, variance_30_3))
var(c(variance_1000_1, variance_1000_2, variance_1000_3))
```

What we see here as we increase the number of iterations to 30 and 1000 the variability
gradually stabilizes around a trustable AUC of ```r round(mean(c(variance_1000_1, variance_1000_2, variance_1000_3)), 3)```. 

## Logistic regression model

I will switch here to caret package. With the **Train()** function we can test different types of machine learning algorithms. 

I will start with a logistic regression model. But before I need to convert Outcome variable into a factor with 2 levels.
```{r Logistic Regression model function, warning=FALSE, message=FALSE}
# Convert Outcome to a factor with two levels
diabetes$Outcome <- factor(diabetes$Outcome)
levels(diabetes$Outcome) <- c("H", "D")

# MyControl
myControl <- trainControl(
  method = "cv", 
  number = 10,
  summaryFunction = twoClassSummary,
  classProbs = TRUE, # IMPORTANT!
  verboseIter = TRUE
)
# Model
model_glm <- train(
  Outcome~.,
  method = "glm",
  data = diabetes,
  trControl = myControl
)
model_glm
```

Here, My model performance is ```r model_glm$results[1,2]```

## Glmnet model


```{r Glmnet model, warning=FALSE, message=FALSE,  results="hide"}
# Model
model_glmnet <- train(
  Outcome~.,
  method = "glmnet",
  data = diabetes,
  trControl = myControl
)
```


```{r print Glmnet model, warning=FALSE, message=FALSE}
model_glmnet

```

Glmnet model performance is ```r max(model_glmnet$results$ROC)```

## Random forest model

```{r Random forest model, warning=FALSE, message=FALSE,  results="hide"}
# Random forest model
model_rf <- train(
  Outcome~.,
  method = "ranger",
  data = diabetes,
  trControl = myControl
  )
```


```{r print Random forest model, warning=FALSE, message=FALSE}

model_rf
```

Random forest performance is ```r max(model_rf$results$ROC)```


## Gradient boost model


```{r Gradient boost model, warning=FALSE, message=FALSE, results="hide"}
model_gbm <- train(
    Outcome~.,
    method = "gbm",
    data = diabetes,
    trControl = myControl
)
```


```{r print Gradient boost model, warning=FALSE, message=FALSE}
model_gbm
```

Gradient boost model performance is ```r max(model_gbm$results$ROC)```

## Naive Bayes model

```{r Naive Bayes model, warning=FALSE, message=FALSE, results="hide"}
model_nb <- train(
    Outcome~.,
    method = "nb",
    data = diabetes,
    trControl = myControl
)
```


```{r print Naive Bayes model, warning=FALSE, message=FALSE}
model_nb
```

Naive Bayes model performance is ```r max(model_nb$results$ROC)```

```{r Summary}
models <- c("lm", "glm", "glmnet", "rf", "gbm", "naive")
lm <- round(mean(c(variance_1000_1, variance_1000_2, variance_1000_3)), 3)
glm <- max(model_glm$results$ROC)
glmnet <- max(model_glmnet$results$ROC)
rf <- max(model_rf$results$ROC)
gbm <- max(model_gbm$results$ROC)
naive <- max(model_nb$results$ROC)
AUC <- c(lm, glm, glmnet, rf, gbm, naive)
df <- data.frame(models, AUC)
knitr::kable(df[order(df[,2], decreasing=TRUE), ])
```


Here, we found Gradient boost model performed the best, and also there are not big differences between the models.



## Future thoughts

I used different machine learning algorithms to predict Diabetes. Models showed similar performances except the naives bayes which performed worst here.  **As we saw, sometimes a simple model can get us far.**   

We can help doctors to predict **Diabetes with accuracy around 84%** by using 8 simple medical parameters.
  
Given current speed in generation and collection of types data by including additonal predictors we can build even better models. 

Until next time!

Serdar
 