[{"authors":["admin"],"categories":null,"content":"Things get easier after the first step. Because the mind actually stops wandering and starts building things. How can you run a marathon if you don\u0026rsquo;t know where the start line is?\nWhatever your ambition is take that first step now. Imagining creates the vision, doing creates the product.\nHi, I am Serdar! here is my camp for Data science projects.\nHere, I try to solve problems that I find interesting using Data Science. It is my starting line. What is more is the challenges I faced and the lessons learned in my journey. There were situations it was not easy to find the relevant guides or ways of doing. So I had to figure out from scratch. I share them too. And doing so spark new ideas as well! Most done in R. Some Python.\nLet’s dive in.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Things get easier after the first step. Because the mind actually stops wandering and starts building things. How can you run a marathon if you don\u0026rsquo;t know where the start line is?\nWhatever your ambition is take that first step now. Imagining creates the vision, doing creates the product.\nHi, I am Serdar! here is my camp for Data science projects.\nHere, I try to solve problems that I find interesting using Data Science.","tags":null,"title":"Serdar Korur","type":"authors"},{"authors":[],"categories":[],"content":"\rWhere are we standing on fight against cancer?\rFive-year survival rates is a good indicator of improvement in cancer medicine.\nI am using the article by Jemal et. al. published on the Journal of the National Cancer institute. You can find the original publication here: https://academic.oup.com/jnci/article/109/9/djx030/3092246\nFinal take home messages in this article were:\n\rCancer death rates continue to decrease in the United States\rBut progress is very limited in some cancers\r\rWhat is needed?\n\rNew strategies for prevention, early detection and treatment is crucial.\r\rThe authors made an extensive study to investigate changes in five year cancer survival rates between the years 1975-77 to 2006-12. I want to create data visualizations to have an overview on the progress we made so far and also compare different types of cancers.\nYou can access the data on my Github repository at https://github.com/korur/healthcareinformatics.\nSetting up and loading in data\nlibrary(readxl)\rlibrary(tidyverse) # ggplot2, dplyr, tidyr, readr, # purrr, tibble, stringr, forcat\rlibrary(ggdark)\rlibrary(animation)\rlibrary(waffle)\rcancer \u0026lt;- read_excel(\u0026quot;posts_data/cancersurvivalstatistics.xls\u0026quot;, sheet = 3)\rcancer\r## # A tibble: 42 x 13\r## Info `1975-1977...2` `2006-2012...3` `Absolute (%)..~ `Proportional (~\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 All ~ 50.29999999999~ 66.40000000000~ 16 31.899999999999~\r## 2 (cas~ (50.1, 50.6) (66.2, 66.5) (15.7, 16.3) (31.1, 32.6) ## 3 Oral~ 52.5 67 14.4 27.399999999999~\r## 4 \u0026lt;NA\u0026gt; (51.1, 54.0) (66.1, 67.9) (12.7, 16.1) (23.5, 31.4) ## 5 Esop~ 5 20.5 15.5 308.10000000000~\r## 6 \u0026lt;NA\u0026gt; (4.0, 6.2) (19.4, 21.7) (13.9, 17.1) (217.6, 398.6) ## 7 Stom~ 15.19999999999~ 31.10000000000~ 15.9 104.7 ## 8 \u0026lt;NA\u0026gt; (14.1, 16.3) (30.1, 32.2) (14.4, 17.4) (88.2, 121.1) ## 9 Colo~ 49.79999999999~ 66.20000000000~ 16.399999999999~ 32.899999999999~\r## 10 \u0026lt;NA\u0026gt; (49.1, 50.6) (65.7, 66.7) (15.5, 17.3) (30.7, 35.1) ## # ... with 32 more rows, and 8 more variables: `1975-1977...6` \u0026lt;chr\u0026gt;,\r## # `2006-2012...7` \u0026lt;chr\u0026gt;, `Absolute (%)...8` \u0026lt;chr\u0026gt;, `Proportional\r## # (%)...9` \u0026lt;chr\u0026gt;, `1975-1977...10` \u0026lt;chr\u0026gt;, `2006-2012...11` \u0026lt;chr\u0026gt;,\r## # `Absolute (%)...12` \u0026lt;chr\u0026gt;, `Proportional (%)...13` \u0026lt;chr\u0026gt;\rI will use lolipop charts to plot the change in the survival rates. We need some data preparation first. You can see the NAs in the first row. These actually contain the confidence intervals for the survival rates. Since I will not use them I can use na.omit() function to remove them.\nI will also change column names and some long cancer types for easier typing.\ncancer \u0026lt;- na.omit(cancer)[-2, 1:5]\rcolnames(cancer) \u0026lt;- c(\u0026quot;type\u0026quot;, \u0026quot;Y1977\u0026quot;, \u0026quot;Y2012\u0026quot;, \u0026quot;Absolute\u0026quot;, \u0026quot;Proportional\u0026quot;)\rcancer[,2:5] \u0026lt;- sapply(cancer[,2:5], as.numeric)\rcancer$type[19] \u0026lt;- \u0026quot;Uterus\u0026quot;\rcancer$type[16] \u0026lt;- \u0026quot;Prostate (Men)\u0026quot;\rcancer$type[1] \u0026lt;- \u0026quot;All Cancers\u0026quot;\rcancer$type[12] \u0026lt;- \u0026quot;Brain / Nervous System\u0026quot;\rcancer$type[6] \u0026lt;- \u0026quot;Liver\u0026quot;\rhead(cancer)\r## # A tibble: 6 x 5\r## type Y1977 Y2012 Absolute Proportional\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 All Cancers 50.3 66.4 16 31.9\r## 2 Oral Cavity and Pharynx 52.5 67 14.4 27.4\r## 3 Esophagus 5 20.5 15.5 308. ## 4 Stomach 15.2 31.1 15.9 105. ## 5 Colon and Rectum 49.8 66.2 16.4 32.9\r## 6 Liver 3.4 18.1 14.6 428.\rMost often I prefer data in tidy format which is:\n\rEach observation has its own row.\n\rEach variable has its own column.\nFor an example post where I tidied my data with gather() function.\r\rIn my data although two variables Y1977 and Y2012 are in two separate columns instead of one, I leave it as it is since it is better this way for lolipop charts and similar line charts.\nfct_reorder() function from forcats package is great for ordering factor variables according to a numeric vector. This comes with the tidyverse package we installed in the beginning. I will order my graph so that cancers with highest survival will be at the top of the graph.\ncancer %\u0026gt;% mutate(type = fct_reorder(type, Y2012)) %\u0026gt;% ggplot() + # Define the start and end positions of the line of the lolipop\rgeom_segment(aes(x=Y1977, xend=Y2012-1, y=type, yend=type), color=\u0026quot;#00AFBB\u0026quot;, size=1, arrow = arrow(length = unit(0.3,\u0026quot;cm\u0026quot;), type = \u0026quot;closed\u0026quot;)) +\r# Two geom_point for placing at beginning and end geom_point(aes(x=Y1977, y=type), color=\u0026quot;#E7B800\u0026quot;, size=2) + geom_point(aes(x=Y2012, y=type), size=2.5, color = \u0026quot;#FC4E07\u0026quot;) +\r# Two Geom_point and two geom_tezt for defining the legend for points\rgeom_point(aes(x=100, y=5), size = 2, color = \u0026quot;#E7B800\u0026quot;) +\rgeom_point(aes(x=100, y=4), size =2, color = \u0026quot;#FC4E07\u0026quot;) + geom_text(aes(x=95, y=5), color =\u0026quot;#B2B2B2\u0026quot;,label =\u0026quot;1975-77\u0026quot;) + geom_text(aes(x=95, y=4), color =\u0026quot;#B2B2B2\u0026quot;, label =\u0026quot;2006-12\u0026quot;) +\r# Apply dark theme from ggdark package dark_theme_gray() +\r# Describe additional theme parameters theme(plot.margin=unit(c(1,1,1.5,1.2),\u0026quot;cm\u0026quot;),\rtext = element_text(size=16),\rlegend.position = \u0026quot;none\u0026quot;,\raxis.text.y = element_text(size=16),\raxis.title.y = element_blank(),\raxis.title.x = element_blank(),\raxis.line.y = element_blank(),\raxis.ticks.y = element_blank(),\rplot.caption = element_text(size= 12, hjust = 0, vjust = -10),\rplot.subtitle=element_text(size=12, face=\u0026quot;italic\u0026quot;)) +\r# Text for placing survival %s # And I need a small trick here by using an ifelse statement # Since in some cancers survival rate decreased and points are in reverse order geom_text(mapping = aes(x = ifelse(cancer$type != \u0026quot;Uterus\u0026quot;, Y1977-1, Y1977+2), y=type, label=Y1977), hjust = ifelse(cancer$type != \u0026quot;Uterus\u0026quot;,\u0026quot;right\u0026quot;, \u0026quot;left\u0026quot;), vjust=0.28) +\rgeom_text(mapping = aes(x = ifelse(cancer$type != \u0026quot;Uterus\u0026quot;, Y2012+2, Y2012-1), y=type, label=Y2012), hjust = ifelse(cancer$type != \u0026quot;Uterus\u0026quot;, \u0026quot;left\u0026quot;,\u0026quot;right\u0026quot;), vjust=0.28) +\rcoord_cartesian(xlim = c(0, 110), expand =1) +\rscale_x_continuous(labels = function(x) paste0(x, \u0026quot;%\u0026quot;)) + labs( caption= \u0026quot;Data: https://doi.org/10.1093/jnci/djx030 \\nVisualization: Serdar Korur\u0026quot;,\rtitle = \u0026quot;Improvement in cancer survival rates in US\u0026quot;, subtitle=\u0026quot;Five year survival rates of most common cancer types \\ncompared between 1975-77 and 2006-12\u0026quot;)\r\rVisualize Cancer statistics with waffle plots\rNow, to make the waffle plot I need my data in the tidy format. I will use gather function to bring together the year variables. Plot p1 will be for years 1975-77 and p2 is for the years 2006-12.\nwaffle_77 \u0026lt;- cancer %\u0026gt;% mutate(Y1977 = round(Y1977, 0), Y2012=round(Y2012,0)) waffle_77 \u0026lt;- waffle_77 %\u0026gt;% mutate(Yes = Y1977)\rwaffle_77 \u0026lt;- waffle_77 %\u0026gt;% mutate(No = 100-Yes)\rwaffle_77 \u0026lt;- waffle_77[ ,c(1,6,7)]\r# Gather the values waffle_tall \u0026lt;- waffle_77 %\u0026gt;% gather(survived, n, -type)\rwaffle_tall\r## # A tibble: 42 x 3\r## type survived n\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 All Cancers Yes 50\r## 2 Oral Cavity and Pharynx Yes 52\r## 3 Esophagus Yes 5\r## 4 Stomach Yes 15\r## 5 Colon and Rectum Yes 50\r## 6 Liver Yes 3\r## 7 Pancreas Yes 2\r## 8 Lung and Bronchus Yes 12\r## 9 Melanoma of the Skin Yes 82\r## 10 Urinary Bladder Yes 72\r## # ... with 32 more rows\r# Final 1\r# cancer Survival rates in 20 most common cancers\rp1 \u0026lt;- waffle_tall %\u0026gt;%\rggplot(aes(fill=survived, values=n)) + geom_waffle(color = \u0026quot;white\u0026quot;, size = .25, n_rows = 10,\rflip = TRUE) + facet_wrap(~type, nrow = 5, strip.position = \u0026quot;top\u0026quot;) +\rtheme( plot.title = element_text(size=24, color= \u0026quot;black\u0026quot;, hjust=0.5),\rlegend.position = c(0.55,0.1), text = element_text(size=18),\raxis.text.x =element_blank(),\raxis.title.x = element_blank(), axis.text.y=element_blank(),\raxis.ticks= element_blank()) +\rscale_fill_manual(values = c(\u0026quot;#dfdedc\u0026quot;,\u0026quot;#16a1c6\u0026quot;)) +\rlabs(title = \u0026quot;Five-year survival rates in most common cancers - 1975-77\u0026quot;)\rwaffle_12 \u0026lt;- cancer %\u0026gt;% mutate(Y2012 = round(Y2012, 0), Yes=round(Y2012,0)) waffle_12 \u0026lt;- waffle_12 %\u0026gt;% mutate(Yes =Y2012)\rwaffle_12 \u0026lt;- waffle_12 %\u0026gt;% mutate(No = 100-Y2012)\rwaffle_12 \u0026lt;- waffle_12[ ,c(1,6,7)]\r# Gather the values waffle_tall_12 \u0026lt;- waffle_12 %\u0026gt;% gather(survived, n, -type)\rwaffle_tall_12\r## # A tibble: 42 x 3\r## type survived n\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 All Cancers Yes 66\r## 2 Oral Cavity and Pharynx Yes 67\r## 3 Esophagus Yes 20\r## 4 Stomach Yes 31\r## 5 Colon and Rectum Yes 66\r## 6 Liver Yes 18\r## 7 Pancreas Yes 8\r## 8 Lung and Bronchus Yes 19\r## 9 Melanoma of the Skin Yes 93\r## 10 Urinary Bladder Yes 78\r## # ... with 32 more rows\r# Final 1\r# Cancer survival rates in 20 most common cancers\rp2 \u0026lt;- waffle_tall_12 %\u0026gt;% ggplot(aes(fill=survived, values=n)) + geom_waffle(color = \u0026quot;white\u0026quot;, size = .25, n_rows = 10,\rflip = TRUE) + facet_wrap(~type, nrow = 5, strip.position = \u0026quot;top\u0026quot;) +\rtheme( plot.title = element_text(size=24, color= \u0026quot;black\u0026quot;, hjust=0.5),\rlegend.position = c(0.55,0.1), text = element_text(size=18),\raxis.text.x =element_blank(),\raxis.title.x = element_blank(), axis.text.y=element_blank(),\raxis.ticks= element_blank()) +\rscale_fill_manual(values = c(\u0026quot;#dfdedc\u0026quot;,\u0026quot;#16a1c6\u0026quot;)) +\rlabs(title = \u0026quot;Five-year survival rates in most common cancers - 2006-12\u0026quot;)\rTo animate my graphs I will use R package “animation” created by Yihui Xie.\nFor more information you can read the original paper published in the Journal of Statistical Software here: An R Package for Creating Animations and Demonstrating Statistical Methods.\nYou can install from CRAN, or the development version from GitHub:\ninstall.packages(\u0026quot;animation\u0026quot;)\r# or development version\r# devtools::install_github(\u0026#39;yihui/animation\u0026#39;)\r\rAnimate waffle plots (years 1975-77 and 2006-2012)\rp \u0026lt;- list(p1,p2)\rsaveGIF({\rfor(i in 1:2) plot(p[[i]])\r},movie.name = \u0026quot;survival.gif\u0026quot;, interval = 0.25, nmax = 30, ani.width = 800)\r## Output at: survival.gif\r## [1] TRUE\r\rApply ggdark theme\rI will use ggdark package to apply a dark theme. This package contains a couple of beautiful themes. p1 will be for years 1975-77 and p2 is for the years 2006-12.\n# Final 1\r# cancer Survival rates in 20 most common cancers\rp1 \u0026lt;- waffle_tall %\u0026gt;% filter(survived %in% c(\u0026quot;Yes\u0026quot;, \u0026quot;No\u0026quot;)) %\u0026gt;% ggplot(aes(fill=survived, values=n)) + geom_waffle(color = \u0026quot;white\u0026quot;, size = .25, n_rows = 10,\rflip = TRUE) + facet_wrap(~type, nrow = 5, strip.position = \u0026quot;top\u0026quot;) + dark_theme_gray() +\rtheme( plot.title = element_text(size=24, color= \u0026quot;white\u0026quot;, hjust=0.5),\rlegend.position = c(0.55,0.1), text = element_text(size=18),\raxis.text.x =element_blank(),\raxis.title.x = element_blank(), axis.text.y=element_blank(),\raxis.ticks= element_blank()) +\rscale_fill_manual(values = c(\u0026quot;#dfdedc\u0026quot;,\u0026quot;#16a1c6\u0026quot;)) +\rlabs(title = \u0026quot;Five-year survival rates in most common cancers - 1975-77\u0026quot;)\r# Final 1\r# cancer Survival rates in 20 most common cancers\rp2 \u0026lt;- waffle_tall_12 %\u0026gt;% filter(survived %in% c(\u0026quot;Yes\u0026quot;, \u0026quot;No\u0026quot;)) %\u0026gt;% ggplot(aes(fill=survived, values=n)) + geom_waffle(color = \u0026quot;white\u0026quot;, size = .25, n_rows = 10,\rflip = TRUE) + facet_wrap(~type, nrow = 5, strip.position = \u0026quot;top\u0026quot;) + dark_theme_gray() +\rtheme( plot.title = element_text(size=24, color= \u0026quot;white\u0026quot;, hjust=0.5),\rlegend.position = c(0.55,0.1), text = element_text(size=18),\raxis.text.x =element_blank(),\raxis.title.x = element_blank(), axis.text.y=element_blank(),\raxis.ticks= element_blank()) +\rscale_fill_manual(values = c(\u0026quot;#dfdedc\u0026quot;,\u0026quot;#16a1c6\u0026quot;)) +\rlabs(title = \u0026quot;Five-year survival rates in most common cancers - 2006-12\u0026quot;)\rp \u0026lt;- list(p1,p2)\rsaveGIF({\rfor(i in 1:2) plot(p[[i]])\r},movie.name = \u0026quot;survival_black.gif\u0026quot;, interval = 0.25, nmax = 30, ani.width = 800)\r## Output at: survival_black.gif\r## [1] TRUE\r\rFuture thoughts / Conclusions\rHere, I made two different charts, lolipop and waffle plots by using ggplot2 and animated them with the ‘Animation’ R package.\nCreating visuals to have a good overview of data helps to understand it better and helps us to think about future directions.\nIn some type of cancers such as lung and pancreas survival rates remained very low.\rHow can we make it better?\nPlease comment below what do you think. What are the emerging data science applications / AI in healthcare?\nUntil next time!\nSerdar\n\r","date":1572912000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572965143,"objectID":"7d2e9fe5916ac229b700e486b705b31f","permalink":"/r/ggplot2-waffle/","publishdate":"2019-11-05T00:00:00Z","relpermalink":"/r/ggplot2-waffle/","section":"post","summary":"Where are we standing on fight against cancer?\rFive-year survival rates is a good indicator of improvement in cancer medicine.\nI am using the article by Jemal et. al. published on the Journal of the National Cancer institute. You can find the original publication here: https://academic.oup.com/jnci/article/109/9/djx030/3092246\nFinal take home messages in this article were:\n\rCancer death rates continue to decrease in the United States\rBut progress is very limited in some cancers\r\rWhat is needed?","tags":[],"title":"Cleaning and visualizing Five-year cancer survival statistics with ggplot2 and animation","type":"post"},{"authors":[],"categories":[],"content":"\rLast week, I came across a data that I thought it is a great opportunity to write about Binomial probability distributions.\nWhat is a binomial distribution and why we need to know it?\rBinomial distributions are formed when we repeat a set of events and each single event in a set has two possible outcomes. Bi- in binomial distributions refers to those outcomes. Two possibilities are usually described as Success or no Success. A “yes” or “no”.\nFor example, in flipping coins, the two possibilities are getting a head (success) or tails (no success) or vice versa. And one set can be, e.g. 10 or 50 times coin flipping. We can repeat this set as many times as we like and record how many times we got heads (success) in each repetition.\nWhy is this interesting?\rWhat is the probability of getting 25 heads out of 50 coin flips? This is not very unusual. But how about getting 49 heads out of 50 flips? Is this really possible?\nLet’s figure it out. We can repeat this set of 50 times coin flipping 100.000 times and record the results of each set.\nLuckily, we can simulate this in R.\nWe can simulate a given number of repeated (here 100.000) sets (50 times of coin flipping) of experiments with rbinom() function. It takes 3 arguments.\nrbinom(\nn = number of repetitions = 100.000,\nsize = sample size = 50,\np = the probability of success (chance of throwing heads is 0.5))\nLet’s compare the probabilities of getting more than 25, 35 or even 49 heads. You can combine rbinom with mean function to find the percentage of the events with a chosen outcome.\n# Probability of getting 25 or more heads\rmean(rbinom(100000, 50, .5) \u0026gt;= 25)\r## [1] 0.55489\r# Probability of getting 35 or more heads\rmean(rbinom(100000, 50, .5) \u0026gt;= 35)\r## [1] 0.00366\r# Probability of getting 49 or more heads\rmean(rbinom(100000, 50, .5) \u0026gt;= 49)\r## [1] 0\rWe found the probability of throwing 49 or more heads to be 0. But to be technically precise it is one in 375 trillion times\r(= \\(1/((1/(2^{49}) + (1/2^{50})))\\)).\nLet’s visualize our simulation. The bars in red represents the sets which had 35 or more heads.\nlibrary(tidyverse) # ggplot2, dplyr, tidyr, readr, # purrr, tibble, stringr, forcats\rlibrary(viridis)\rheads \u0026lt;- rbinom(100000, 50, 0.5)\rheads \u0026lt;- data.frame(heads)\rheads \u0026lt;- heads %\u0026gt;% mutate(events = ifelse(heads \u0026gt; 35, \u0026quot;\u0026gt; 35\u0026quot;, \u0026quot;\u0026lt; 35\u0026quot;))\rheads %\u0026gt;% ggplot(aes(x=heads, fill = events)) + geom_histogram(binwidth = 0.5) + scale_fill_manual(values = c(\u0026quot;black\u0026quot;, \u0026quot;red\u0026quot;)) +\rtheme_classic() +\rtheme(text = element_text(size = 18),\rlegend.position = c(0.85, 0.85)) +\rlabs(x = \u0026quot;Number of heads in 50 coin flips\u0026quot;)\rEven to get 35 or more heads has very low probability. Most of the events will be an integer between 15 and 35.\nBinomial probability distributions help us to understand the likelihood of such rare events.\nYou can also see here a key difference of a binomial distribution with normal distribution is that they can take only discrete values. It is not possible to have 35.5 heads. Although, there are differences, when the sample size is large enough their shape will be similar and normal distributions can be used to estimate binomial probabilities.\nSo what are all those will be useful for?\nCoin flipping expertise may have limited real life applications but let’s give some other examples.\n\rBinomial distributions in machine learning\r\rYou built a machine learning model with a binary outcome. Let’s say pathological image recognition algorithm for liver cancer that works with 90% accuracy. You tested 100 patients and you want to know your 95% confidence interval? Or your new results showed that your model detected less than 70 patients correctly. Is it possible? Or you should start optimizing your parameters again?\n\rNumber of patients responding to a treatment.\r\rLet’s say you have a new therapy for cancer which has 10% probability to cure a patient. You have 500 patients which took the drug. The expected number of recovering patients is 50. But you found that 75 patients responded. Is that due to chance or a significant effect? Or you should start looking underlying factors if there is something about the therapy or the patient group?\n\rThink about a hospital emergency station.\r\rYou are a hospital manager and you want to organize the staff numbers correctly for different weekdays. You know total number of patients came in to a emergency station because of alcohol poisoning in a given time period. You can analyse the distribution of patient numbers for each day of the week. Most likely you will have more such cases in the weekend and you need larger staff.\nThis will be also true for other businesses. They can use binomial distributions to calculate changes in demand and plan accordingly.\n\rIf you are running a webserver.\r\rYou can allocate your resources better by identifying times when traffic will be higher.\nSome other questions in which binomial distributions will come in handy are:\n\rNumber of people who answered ‘yes’ to a survey question\rHow many games a team will win in one season?\rVote counts for a candidate in an election.\rNumber of defective products in a production run.\r\r\rBinomial distributions are common and they have many applications of real life situations.\n\rLet’s use some real life data to apply our knowledge so far. The data comes from TidyTuesday which I introduced in my last post.\nIt contains information about Horror Movies released since 2012. And the question I asked was whether horror movies are more likely be released at the 13th each month?\nBeing in the right mindset for anything gives us a better feeling of it. If you go to the cinema at the 13th to watch a horror movie, do you like it more than other days? We don’t know if this is true but movie makers might be counting on that. I analyzed Horror movies data and calculated number of releases in different days of the month. This is a good example of a binomial probability distribution. Let’s look at the data.\nhorror_movies \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-22/horror_movies.csv\u0026quot;)\rdim(horror_movies)\r## [1] 3328 12\rhead(horror_movies)\r## # A tibble: 6 x 12\r## title genres release_date release_country movie_rating review_rating\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Gut ~ Drama~ 26-Oct-12 USA \u0026lt;NA\u0026gt; 3.9\r## 2 The ~ Horror 13-Jan-17 USA \u0026lt;NA\u0026gt; NA ## 3 Slee~ Horror 21-Oct-17 Canada \u0026lt;NA\u0026gt; NA ## 4 Trea~ Comed~ 23-Apr-13 USA NOT RATED 3.7\r## 5 Infi~ Crime~ 10-Apr-15 USA \u0026lt;NA\u0026gt; 5.8\r## 6 In E~ Horro~ 2017 UK \u0026lt;NA\u0026gt; NA ## # ... with 6 more variables: movie_run_time \u0026lt;chr\u0026gt;, plot \u0026lt;chr\u0026gt;, cast \u0026lt;chr\u0026gt;,\r## # language \u0026lt;chr\u0026gt;, filming_locations \u0026lt;chr\u0026gt;, budget \u0026lt;chr\u0026gt;\rI need some data pre-processing before I can make my visualizations. Dates are given in day:month:year format. I need to split them to individual columns. Also couple of movies do not have the day of the month. I will remove them.\nhorror_date \u0026lt;- horror_movies %\u0026gt;% separate(\rrelease_date, c(\u0026quot;day\u0026quot;, \u0026quot;month\u0026quot;, \u0026quot;year\u0026quot;),\rsep = \u0026quot;-\u0026quot;)\rhorror_date$day \u0026lt;- as.numeric(horror_date$day)\r# Remove rows without Date of the month\rhorror_date \u0026lt;- horror_date %\u0026gt;% filter(day \u0026lt; 32) # I am excluding Day 1 from the analysis (Most aggreements starts at 1st)\rhorror_date_table \u0026lt;- horror_date %\u0026gt;% filter(day \u0026gt; 1)\r# Let\u0026#39;s check what is the most common day in the month for a horror movie release\rhorror_date_table \u0026lt;- horror_date_table %\u0026gt;%\rgroup_by(day) %\u0026gt;% count() %\u0026gt;% arrange(desc(n))\rhorror_date_table\r## # A tibble: 30 x 2\r## # Groups: day [30]\r## day n\r## \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt;\r## 1 13 124\r## 2 18 119\r## 3 25 119\r## 4 21 110\r## 5 31 107\r## 6 28 102\r## 7 10 100\r## 8 20 100\r## 9 5 99\r## 10 7 98\r## # ... with 20 more rows\rLet’s visualize the data.\n# Final\rhorror_date_table$day \u0026lt;- as.character(horror_date_table$day)\rmy_title \u0026lt;- \u0026quot;Highest number of Horror movies are released at the 13th\u0026quot;\rp \u0026lt;- horror_date_table %\u0026gt;% ungroup() %\u0026gt;% mutate(day=fct_reorder(day, n, .desc=TRUE)) %\u0026gt;% ggplot(aes(x=day, y=n)) +\rgeom_col(aes(fill=n)) +\rscale_fill_viridis( direction =-1) + theme(\rplot.title = element_text(size=24, color= \u0026quot;black\u0026quot;, hjust=0.5, vjust = -1),\rplot.subtitle = element_text(size=36, color= \u0026quot;red\u0026quot;, hjust=0.5, vjust = -1),\rpanel.background = element_rect(fill = \u0026quot;white\u0026quot;), plot.background = element_rect(fill = \u0026quot;white\u0026quot;),\rpanel.grid = element_blank(),\rlegend.position = \u0026quot;none\u0026quot;, text = element_text(size=18), axis.text.x =element_text(vjust=12, size=17, colour= \u0026quot;white\u0026quot;, face= \u0026quot;bold\u0026quot;),\raxis.title.x = element_text(vjust=9.5), axis.text.y=element_blank(),\raxis.ticks= element_blank(), plot.caption = element_text(hjust = 1, vjust = 10)) +\rlabs(\rcaption= \u0026quot;Source: IMDb, Plot: @dataatomic\u0026quot;,\rx = \u0026quot;Day of the Month\u0026quot;, y = \u0026quot;Number of movies released\u0026quot;,\rtitle = my_title) +\rgeom_label(aes(label = n), size=5, fill=\u0026quot;yellow\u0026quot;, alpha=0.9) p\r\r\rIs this significant?\rIn the data, there were 2782 movies associated with a release date. So expected movie release per day is 92 (2782 / 30).\nBut, we see above that some days are much higher than that. What we want to know is, which days are in the range of random chance and which days there is a significant preference or an aversion to release a movie. So that we can conclude our insight about movie makers.\n# Size of our distribution is the total number of movies released n_value \u0026lt;- horror_date_table %\u0026gt;% ungroup() %\u0026gt;% summarize(n2 = sum(n))\rsize \u0026lt;- n_value$n2\rsize\r## [1] 2782\r# The probability (= success rate = a given outcome to occur = movie released)\rp \u0026lt;- 1/30 # Since it can occur any of the 30 days in a months\rYou can simulate this 2782 movie release dates events 100.000 times with rbinom function and calculate the mean and variance.\n# Simulated statistics\restimates \u0026lt;- rbinom(100000, 2782, 1/30)\r# Simulated mean\rmean(estimates)\r## [1] 92.73647\r# Simulated variance\rvar(estimates)\r## [1] 89.9432\rThe average value is around 92 movies released in one day.\nWe can also calculate theoretical values by the derived mathematical formulas that define the binomial function:\nMean = size * p\nVariance = size * p * (1 - p)\nLet’s calculate.\n# Theoretical statistics\r# Expected mean = size * p\rmean_theoretical \u0026lt;- 2782 * 1/30\rmean_theoretical\r## [1] 92.73333\r# Expected Variance = size * p * (1-p)\rvar_theoretical \u0026lt;- size * 1/30 * (1-1/30)\rvar_theoretical\r## [1] 89.64222\rGreat, simulated and theoretical values are almost the same. So, I can use my simulations to find out 95% confidence interval which will contain the values that can happen due to random chance.\nI will define an interval that contains 95% of probabilities in our simulated distributions. And the values outside will be the ones which were not due to random chance. To do this I need 2.5th and 97.5th quantiles of the distribution.\nWe can do this by the qbinom() function in r. For example qbinom(0.975, size, p) will return the value which will define the cut off which contains 0.975 of the probabilities. And our confidence interval will be the interval between:\nqbinom(0.025, size, p) \u0026lt; Confidence Interval \u0026lt; qbinom(0.975, size, p)\n# Boundaries for p values smaller than 0.05\rlower \u0026lt;- qbinom(0.975, 2782, 1/30)\rlower\r## [1] 112\rupper \u0026lt;- qbinom(0.025, 2782, 1/30)\rupper\r## [1] 75\rmovies \u0026lt;- rbinom(100000, 2782, 1/30)\rmovies \u0026lt;- data.frame(movies)\rmovies \u0026lt;- movies %\u0026gt;% mutate(events = ifelse(movies \u0026gt; qbinom(.025, 2782, 1/30) \u0026amp; movies \u0026lt; qbinom(.975, 2782, 1/30), \u0026quot;95% Conf. Int.\u0026quot;, \u0026quot;significant\u0026quot;))\rmovies %\u0026gt;% ggplot(aes(x=movies, fill = events)) + geom_histogram(binwidth = 0.5) + scale_fill_manual(values = c(\u0026quot;black\u0026quot;, \u0026quot;red\u0026quot;)) +\rtheme_classic() +\rtheme(text = element_text(size = 18),\rlegend.position = c(0.85, 0.85)) +\rlabs(x = \u0026quot;Number of movie releases in a day\u0026quot;)\r95% of the time, days in one month will have between 75 and 112 movie releases. Higher or lower values than this range can not happen due to random chance according to our binomial distribution.\n\rHow to calculate the p-value for a binomial test using pbinom?\rOur observed value for the 13th is 124. Above the 97.5th quantile. So it is significant. But what is the exact p value? Let’s define p value first.\n P value is the sum of the probability of that event plus the sum of the probabilities of similar events that are equally likely or less likely.   In coin flipping probability of heads is (0.5) and following our definition p value is the sum of the probability of that event (0.5) and similar event which is equally or less likely i.e. tails (0.5) which is 0.5 + 0.5 = 1\nAnd in our horror movie data, this will be the sum of the probabilities of getting 124 movie releases and higher.\nIn R, pbinom function defines the cumulative probabilities. For example, pbinom(124, 2782, 1/30) will give us the cumulative probabilities of any number of movie releases up to 124. By using 1-pbinom(124, 2782, 1/30) we can find the sum of the probabilities with equal or lower chance than having 124.\nThus, p value for getting at least 124 movie release is;\n# Probability of getting 124 movie releases in a day like here it happened on the 13th. # Probability of getting at least 124 success\rp_val_binom \u0026lt;- 2 * (1 - pbinom(124, 2782, 1/30))\rp_val_binom\r## [1] 0.001335455\rWe multiplied by two because same rare events can happen in the left side of our confidence interval as well.\nLet’s put those p values on our barplot to highlight the significant days.\n# I will add a new column so I can separately define values outside the 95% confidence interval.\rhorror_date_table \u0026lt;- horror_date_table %\u0026gt;% mutate(p_val = 2 * (1 - pbinom(n, 2782, 1/30, lower.tail = ifelse(n\u0026gt;93, TRUE, FALSE)))) %\u0026gt;% mutate(p_val = cut(p_val, breaks = c(-Inf, 0.001, 0.01, 0.05, Inf), labels = c(\u0026quot;\u0026lt; 0.001\u0026quot;,\u0026quot;\u0026lt; 0.01\u0026quot;, \u0026quot;\u0026lt; 0.05\u0026quot;, \u0026quot;NS\u0026quot;))) # Visualize the significant days p_ci \u0026lt;- horror_date_table %\u0026gt;% ungroup() %\u0026gt;% mutate(day=fct_reorder(day, n, .desc=TRUE)) %\u0026gt;% ggplot(aes(x=day, y=n)) +\rgeom_col(aes(fill=p_val)) +\rscale_fill_manual(values = viridis(4)) + theme(\rplot.title = element_text(size=24, color= \u0026quot;black\u0026quot;, hjust=0.5, vjust = -1),\rplot.subtitle = element_text(size=36, color= \u0026quot;red\u0026quot;, hjust=0.5, vjust = -1),\rpanel.background = element_rect(fill = \u0026quot;white\u0026quot;), plot.background = element_rect(fill = \u0026quot;white\u0026quot;),\rpanel.grid = element_blank(),\rtext = element_text(size=18), axis.text.x =element_text(vjust=12, size=17, colour= \u0026quot;white\u0026quot;, face= \u0026quot;bold\u0026quot;),\raxis.title.x = element_text(vjust=9.5), axis.text.y=element_blank(),\raxis.ticks= element_blank(), plot.caption = element_text(hjust = 1, vjust = 10)) +\rlabs(\rcaption= \u0026quot;\u0026quot;,\rx = \u0026quot;Day of the Month\u0026quot;, y = \u0026quot;Number of movies released\u0026quot;,\rtitle = \u0026quot;Calculating p values in binomial distributions\u0026quot;) +\rgeom_label(aes(label = n), size=5, fill=\u0026quot;yellow\u0026quot;, alpha=0.9) p_ci\rWe performed a hypothesis testing by calculating the p value by using the pbinom() function and found couple of other days where movies are more or less likely to be released.\nHowever, another widely used way to do so is to calculate the mean (the expected probability) of our distribution and its standard deviation and to verify how many standard deviations the observed value is away from the mean (the z score).\nCalculating the p-value by normal approximation\rWhen the sample size is large, binomial distributions can be approximated by a normal distribution. To build the normal distribution, I need mean and standard deviation.\nsample_mean \u0026lt;- horror_date_table %\u0026gt;% ungroup() %\u0026gt;% summarise(n=mean(n))\rsample_mean\r## # A tibble: 1 x 1\r## n\r## \u0026lt;dbl\u0026gt;\r## 1 92.7\rp \u0026lt;- 1/30\rsample_variance \u0026lt;- 2782 * p * (1-p)\rsample_variance\r## [1] 89.64222\rsample_sd \u0026lt;- sqrt(sample_variance)\r# Calculate z-score for observation 13th of the month = 124 movies are # released\robservation \u0026lt;- 124\rz_score \u0026lt;- (observation - sample_mean) / sample_sd\rz_score\r## n\r## 1 3.302367\r# Calculate the p-value of observing 124 or more movie releases in a day\rp_val_nor \u0026lt;- 2 * pnorm(3.302, lower.tail = FALSE)\rp_val_nor\r## [1] 0.0009599807\rP values found by using an approximation of a normal distribution and with a simulation of a binomial distribution are very close, Normal: 0.00095, Binomial 0.00133.\n\r\rFuture thoughts / Conclusions\rAs we saw, many events in real life can be explained by binomial probability distributions, and they allow us to calculate whether or not the events happened due to random chance and test different hypotheses.\nUntil next time!\nSerdar\n\r","date":1572220800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572292101,"objectID":"05a4c9e3b048645ce413f94577a15c00","permalink":"/r/probability-distributions/","publishdate":"2019-10-28T00:00:00Z","relpermalink":"/r/probability-distributions/","section":"post","summary":"Last week, I came across a data that I thought it is a great opportunity to write about Binomial probability distributions.\nWhat is a binomial distribution and why we need to know it?\rBinomial distributions are formed when we repeat a set of events and each single event in a set has two possible outcomes. Bi- in binomial distributions refers to those outcomes. Two possibilities are usually described as Success or no Success.","tags":["ggplot2","binomial distribution","statistics","probability","pvalue"],"title":"An intuitive real life example of a binomial distribution and how to simulate it in R","type":"post"},{"authors":[],"categories":[],"content":"\rThis week, I will analyze Car Fuel Economy dataset from TidyTuesday.\nWhat is TidyTuesday?\rTidyTuesday is a weekly social data project in R organized by the R for Data Science community.\nIt is a great way of improving your Data wrangling and visualization techniques, sharing and learning from others.\nYou can find more information on their github.\nFuel economy data are the result of the work done by the US Environmental Protection Agency. Full data dictionary can be found at fueleconomy.gov.\nThe data contains 83 parameters of more than 40.000 Vehicles. That’s a lot of information!\n\rGoing for your next family camping adventure? First, check your car model.\rBetter Fuel economy and recent developments on longer running electric car batteries are great. But one thing which does not change in families’ lives is the need for space.\nIf you don’t want crying kids running around because of a missing teddy bear which did not fit in the baggage. Check, which brands will serve you best.\nEspecially, if you have a daughter who likes to travel with a lot of toys.Let’s figure out a solution for peaceful weekend trip.\n\rWhich brand produce most family friendly cars? In terms of baggage volume.\rlibrary(tidyverse) # ggplot2, dplyr, tidyr, readr, # purrr, tibble, stringr, forcats\rbig_epa_cars \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-15/big_epa_cars.csv\u0026quot;)\rdim(big_epa_cars)\r## [1] 41804 83\rI will subset my data for easier computation.\nLet’s keep the following columns:\n\ryear - model year\rmake - manufacturer (division)\rmodel - model name (carline)\rVClass - EPA vehicle size class\rhlv - hatchback luggage volume (cubic feet)\rhpv - hatchback passenger volume (cubic feet)\rdispl - engine displacement in liters\rlv4 - 4 door luggage volume (cubic feet)\rpv4 - 4-door passenger volume\r\rbig_sub \u0026lt;- big_epa_cars %\u0026gt;% select(fuelType, year, make, model, VClass, hlv, hpv,lv4,pv4,displ)\rI will start exploring the data. For the moment, first I will focus on Midsize cars (VClass).\nI will filter for the main pool of Midsize cars with 4 door and luggage volume of bigger than 6 and passenger volume larger than 75.\nposn.j \u0026lt;- position_jitter(width=0.2)\rbig_sw \u0026lt;- big_sub %\u0026gt;% filter(VClass == \u0026quot;Midsize Cars\u0026quot; \u0026amp; pv4 \u0026gt; 75 \u0026amp; lv4 \u0026gt; 6) big_sw %\u0026gt;%\rggplot(aes(x=pv4, y=lv4)) + geom_point(shape=21,\ralpha=0.4,size =3, position = posn.j) + theme(plot.caption=element_text(size=11), text = element_text(size=18),\rplot.title = element_text(size=32), legend.position = \u0026quot;none\u0026quot;) +\rgeom_smooth(method = \u0026quot;lm\u0026quot;, color =\u0026quot;red\u0026quot;) + coord_fixed() +\rlabs(x = \u0026quot;Passenger Vol (Cubic feet)\u0026quot;, y = \u0026quot;Luggage Vol (Cubic feet)\u0026quot;, title = \u0026quot;Luggage space negatively \\ncorrelates with passenger space\u0026quot;)\rThis is not unexpected. But good to see.\n\rInsight 1: Negative correlation suggests that producers sacrifice passenger space to produce bigger room for the luggage or vice versa.\n\rFirst, I will look at the luggage volume in Mid sized cars and I will order them according to highest average.\npp \u0026lt;- big_sw %\u0026gt;% mutate(make = fct_reorder(make, lv4)) %\u0026gt;%\rggplot(aes(x=make, y=lv4, col=make)) + geom_boxplot(varwidth=TRUE) +\rtheme(plot.caption=element_text(size=11), text = element_text(size=18), plot.title = element_text(size=32), legend.position = \u0026quot;none\u0026quot;) +\rcoord_flip() + labs(x = element_blank(), y = \u0026quot;Luggage size (cubic feet)\u0026quot;, title = \u0026quot;Average luggage volumes in Midsized cars\u0026quot;)\rpp\rIf you follow the mean lines from bottom to top, you will see that cars cluster into three groups according to their mean of luggage sizes. But differences are not huge.\n\rInsight 2: Cars cluster into three groups according to their mean luggage size.\n\rLet’s focus. I am looking for the car with the biggest luggage space. Let’s see what other VClass types are in our dataset that we can include our exploration.\nThere are 34 types of vehicle classes (Vlass) in our dataset. I will subset all the relevant ones, leaving some specialty vehicles and vans aside.\nYou can have a look at other VClass types with this code here.\nbig_epa_cars %\u0026gt;% group_by(VClass) %\u0026gt;% count() %\u0026gt;% arrange(desc(n))\nI will also remove minor brands with less than 10 models in total.\nbig_filtered \u0026lt;- big_sub %\u0026gt;% filter(VClass %in% c(\u0026quot;Large Cars\u0026quot;, \u0026quot;Compact Cars\u0026quot;, \u0026quot;Midsize Cars\u0026quot;, \u0026quot;Midsize Station Wagons\u0026quot;, \u0026quot;Midsize-Large Station Wagons\u0026quot;,\r\u0026quot;Minivan - 2WD\u0026quot;, \u0026quot;Minivan - 4WD\u0026quot;)) %\u0026gt;% group_by(make) %\u0026gt;% mutate(n=n()) %\u0026gt;% filter(n \u0026gt; 10) %\u0026gt;% ungroup()\rdim(big_filtered)\r## [1] 14710 11\rTo be on the safe side for the family trip, I will choose cars not older than 5 years.\n# Cars ordered with luggage volume, but not older than 5 years # and lv4 bigger than 5\rq \u0026lt;- big_filtered %\u0026gt;% filter(year \u0026gt; 2016, lv4 \u0026gt; 5) %\u0026gt;%\rmutate(make = fct_reorder(make, lv4)) %\u0026gt;%\rggplot(aes(x=make, y=lv4, col=make)) + geom_boxplot(varwidth=TRUE) +\rtheme(text = element_text(size=15),\rlegend.position = \u0026quot;none\u0026quot;) +\rcoord_flip()\rq\rThere are not big differences between average luggage size of different brands. Although, you will probably get more space if you choose a Volkswagen or Ford rather than a BMW or Chevrolet.\nThe real XL luggage volume cars are plenty and seem to be more outlier models. To find our dream car let’s focus on those outliers.\nI will create a new data frame boot_space containing the top 50 cars according to the luggage volume.\nboot_space \u0026lt;- big_filtered %\u0026gt;% filter(year \u0026gt; 2016) %\u0026gt;% arrange(desc(lv4)) %\u0026gt;% top_n(50, lv4)\r# Top family cars - geom_point()\rbs \u0026lt;- boot_space %\u0026gt;% mutate(model = fct_reorder(model, lv4)) %\u0026gt;%\rmutate(make = fct_reorder(make, lv4)) %\u0026gt;% ggplot(aes(x=make,y= model, size=lv4, col=VClass)) + geom_point() +\rtheme(plot.caption=element_text(size=12),\raxis.text.x=element_text(angle=45, hjust=1),\rtext = element_text(size=18), plot.title = element_text(size=32)) +\rlabs(caption= \u0026quot;Data: https://fueleconomy.gov\u0026quot;, size=\u0026quot;Luggage Vol\\n(Cubic feet)\u0026quot;,\rx = element_blank(), y = element_blank(), title = \u0026quot;Which are the best family cars?\u0026quot;) + guides(size = guide_legend(order = 1), shape = guide_legend(order = 2)) +\rscale_size(range=c(2, 9))\rbs\rMercedes AMG GLA45 is the winner with 42 cubic feet space!\nHere is another presentation, for easier comparision.\n# Top family cars - geom_Col()\rbs_col \u0026lt;- boot_space %\u0026gt;% mutate(model = fct_reorder(model, lv4)) %\u0026gt;%\rmutate(make = fct_reorder(make, lv4)) %\u0026gt;% ggplot(aes(x=model, y=lv4, fill=make)) + geom_col(position=\u0026quot;dodge\u0026quot;)+coord_flip() + theme(plot.caption=element_text(size=11), text = element_text(size=18), plot.title = element_text(size=32)) +\rlabs(caption= \u0026quot;Data source: https://fueleconomy.gov\u0026quot;, size=\u0026quot;Luggage Vol\\n(Cubic feet)\u0026quot;, x = element_blank(), y = \u0026quot;Luggage Vol (Cubic feet)\u0026quot;, title = \u0026quot;Which are the best family cars?\u0026quot;) +\rscale_size(range=c(2, 9)) bs_col\rWe found our answer and our camping gear is ready. Let’s tackle some other questions. We hear a lot about them but how does the future looks like for Electric cars?\n\rHow do Electric cars evolving in the last years compared to non electric cars?\rThere are many of different types of engines capable of using one or two different fuel sources. Let’s look at how their numbers compare during the last years.\n# Using Varwidth: Ordered\rpp \u0026lt;- big_epa_cars %\u0026gt;% mutate(fuelType=fct_reorder(fuelType, year)) %\u0026gt;% ggplot(aes(x=fuelType, y =year, fill=fuelType)) + geom_boxplot(varwidth=TRUE) + coord_flip() + theme(legend.position = \u0026quot;none\u0026quot;, text = element_text(size=18), plot.title = element_text(size=32),\raxis.text.x = element_text(angle = 45, hjust = 1)) + labs(x = \u0026quot;Fuel Type\u0026quot;,\ry = \u0026quot;Year\u0026quot;, title = \u0026quot;How does prominence of Fuel Types \\nchange with the year?\u0026quot;)\rpp\rGroup Electric vs Non Electric cars\rI will group cars whehter or not they can use electricity.\n# Grouped: Electric vs no electric:\rbig_epa_cars$fuelType \u0026lt;- ifelse(big_epa_cars$fuelType %in%\rc(\u0026quot;Regular Gas and Electricity\u0026quot;,\r\u0026quot;Premium Gas or Electricity\u0026quot;,\r\u0026quot;Premium and Electricity\u0026quot;, \u0026quot;Regular Gas or Electricity\u0026quot;,\r\u0026quot;Electricity\u0026quot;), \u0026quot;Electric\u0026quot;, \u0026quot;Non-Electric\u0026quot;)\rpp \u0026lt;- big_epa_cars %\u0026gt;% mutate(fuelType=fct_reorder(fuelType, year)) %\u0026gt;% ggplot(aes(x=fuelType, y =year, fill=fuelType)) + geom_boxplot(varwidth=TRUE) +\rcoord_flip() + theme(text = element_text(size=18), plot.title = element_text(size=32), legend.position = \u0026quot;none\u0026quot;)+\rtheme(text = element_text(size=15)) +\rlabs(x = \u0026quot;Fuel Type\u0026quot;,\ry = \u0026quot;Year\u0026quot;,\rtitle = \u0026quot;How does prominence of Fuel Types \\nchange with the year?\u0026quot;)\rpp\rIn the last couple of years, number of electric car models are increasing but they are still a minority.\nbig3 \u0026lt;- big_epa_cars %\u0026gt;% group_by(year, fuelType) %\u0026gt;% mutate(n = n())\rbig3 %\u0026gt;% ggplot(aes(x=n, y =year, col=fuelType)) +\rgeom_point(size=4) +\rtheme(legend.position = c(0.9,0.9),\rlegend.title= element_blank(), legend.background = element_blank(),\rplot.title = element_text(size=32), text = element_text(size=15)) + coord_flip() +\rlabs(x = \u0026quot;Number of Car models\u0026quot;, y = \u0026quot;Year\u0026quot;, title = \u0026quot;How does the Numbers of Electric vs Non Electric cars \\nchange by year?\u0026quot;)\rBoth Electric and Non Electric car models follows a similar increase in the last 10 years\nIncreases in Electric car models in the last years might be a reflection of a general increase in total number of model types\n\r\rConclusions / Future thoughts\rThis was a huge dataset. You can answer many other questions such as mileage of different car models, carbon dioxide emissions, fuel savings.\nI have selected some car models which might be a good option if luggage space is a priority for you!\nTo see other examples of how people used this dataset follow the Twitter hashtag #TidyTuesday.\nPlease share if you have other ideas in the comments below!\nUntil next time!\nSerdar\n\r","date":1571788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571817889,"objectID":"7733bc2d1f73958b0aa6dfa81584317c","permalink":"/r/tidytuesday-which-are-the-best-family-cars/","publishdate":"2019-10-23T00:00:00Z","relpermalink":"/r/tidytuesday-which-are-the-best-family-cars/","section":"post","summary":"This week, I will analyze Car Fuel Economy dataset from TidyTuesday.\nWhat is TidyTuesday?\rTidyTuesday is a weekly social data project in R organized by the R for Data Science community.\nIt is a great way of improving your Data wrangling and visualization techniques, sharing and learning from others.\nYou can find more information on their github.\nFuel economy data are the result of the work done by the US Environmental Protection Agency.","tags":["ggplot2","tidytuesday","RStudio","tidyverse","data wrangling","data visualization"],"title":"#TidyTuesday: Which are the best family cars for your weekend trip?","type":"post"},{"authors":[],"categories":[],"content":"\rIt is hard to understand your data by looking at the numbers on a csv file. You need to plot it. And adding statistics to your plots will make it more informative.\nTo evaluate data, we typically use mean and median to define its central tendency and range, quartiles, variance and standard deviation to define how spread it is.\nMean and standard deviation is a good representation of the data if we don’t have extreme values that result in a skewed distribution. But, if we have outliers they might misguide us. In those conditions, median and quartiles will serve us better.\nMedian is the central point which divides the data into half. Quartiles are used to describe the spread of the data. The word comes from the Medieval Latin “quartilis” which means fourth and quartiles break the data into four equal parts.\n The advantage is that they are much less effected by the outliers or skeweness of the data.   For this reason, quartiles are often used along with the median as the best measures of spread.\nThey are often expressed as an Interquartile range (IQR), which is the interval between first and third quartiles and represents 50% of the data points.\nFor example, you measured height of adults in a population, with Interquartile range you can describe a discrete interval centered around the median including 50% of the measurements.\nHere is a representative graph.[source:wikipedia]\nHow to include statistics in ggplot2\rStats make it easier to grasp the data. And different statistics are suited for different data types. For example, you may want to show a 95% confidence interval? or mean? median? or any other statistics which captures the details best for your data.\nSo let’s go through on an example data to understand how statistics can be overlayed in ggplot2. The data is about the effects of two Herbicides (glyphosate \u0026amp; bentazone) on the yield of white mustard (Sinapis alba) seeds.\nFirst, import ggplot2 package and read in the data.\nlibrary(ggplot2)\rpath \u0026lt;- \u0026quot;C:/Users/serda/Downloads/S.alba.csv\u0026quot;\rdata \u0026lt;- read.csv(path)\rstr(data)\r## \u0026#39;data.frame\u0026#39;: 68 obs. of 4 variables:\r## $ X : int 1 2 3 4 5 6 7 8 9 10 ...\r## $ Dose : int 0 0 0 0 0 0 0 0 10 10 ...\r## $ Herbicide: Factor w/ 2 levels \u0026quot;Bentazone\u0026quot;,\u0026quot;Glyphosate\u0026quot;: 2 2 2 2 2 2 2 2 2 2 ...\r## $ DryMatter: num 4.7 4.6 4.1 4.4 3.2 3 3.8 3.9 3.8 3.8 ...\rThese chemicals used at 8 different doses and the yield is measured. I will convert the dose variable as a factor.\ndata$Dose \u0026lt;- as.factor(data$Dose)\rlevels(data$Dose)\r## [1] \u0026quot;0\u0026quot; \u0026quot;10\u0026quot; \u0026quot;20\u0026quot; \u0026quot;40\u0026quot; \u0026quot;80\u0026quot; \u0026quot;160\u0026quot; \u0026quot;320\u0026quot; \u0026quot;640\u0026quot;\rWe can make an initial plot to visualize the data. We’ll plot the yield variable DryMatter against Dose of the Herbicides. We can assign col argument to map different chemical compounds to different colors.\n# I will define a dodge and jitterdodge object to avoid overlapping data points # or stats that we will overlay later\rposn.d \u0026lt;- position_dodge(width=0.2)\rposn.jd \u0026lt;- position_jitterdodge(jitter.width = 0.1, dodge.width=0.2)\rp \u0026lt;- ggplot(data, aes(x=Dose, y=DryMatter, col=Herbicide, fill=Herbicide, group=Herbicide ))\rp + geom_point(position =posn.jd)\rWhat we see here is that, at low doses both Herbicides led similar yields but starting from Dose 40 we see a drastic negative impact of Benzoate on yield.\nOn top of that plot, I want to overlay the min, max and also median and Interquartile range for each set of yield measurements.\nThere are many default functions in ggplot2 which can be used directly such as mean_sdl(), mean_cl_normal() to add stats in stat_summary() layer. But, I will create custom functions here so that we can grasp better what is happening behind the scenes on ggplot2.\nI will create one function to calculate the median and the interquartile range(IQR) 1-3, and another to calculate min(), max() values.\n In order to use the results of a function directly in ggplot2 we need to ensure that the names of the variables match the aesthetics needed for our respective geoms.   # Function for median and IQR\rmedian_IQR \u0026lt;- function(x) {\rdata.frame(y = median(x), # Median\rymin = quantile(x)[2], # 1st quartile\rymax = quantile(x)[4]) # 3rd quartile\r}\r# Function for min, max values\rrange \u0026lt;- function(x) {\rdata.frame(ymin=min(x),\rymax=max(x))\r}\rLet’s replot with the statistics we wanted to overlay. You can use two stat_summary() layers to add our stats. You can set the fun.data argument to the specific function defined above.\n# Updated plot\rp + stat_summary(geom = \u0026quot;linerange\u0026quot;,\rfun.data = median_IQR, position = posn.d, size=3) + stat_summary(geom = \u0026quot;linerange\u0026quot;, fun.data = range, position = posn.d, size=3, alpha=0.5)+ stat_summary(geom = \u0026quot;point\u0026quot;, fun.y = \u0026quot;median\u0026quot;, position = posn.d, size = 3, col = \u0026quot;black\u0026quot;, shape = \u0026quot;X\u0026quot;)\rIn contrast, to the first plot here we easily see where the median and IQR lays. We can make our comparisions easier. Starting from dose 20, we see that glyphosate clearly outperforms bentazone. Big separation between the IQRs are obvious at doses 40 and 80. At higher doses, differences in IQRs starts to disappear.\nConclusions\rGgplot2 is a flexible package and knowing its intricacies will help you level up your visuals. To understand your data and to convey the insights you want to point out, you can include your choice of custom functions in ggplot stat_summary() layer similarly as we did above or use the default functions.\nThe data we have here was small. With bigger data, it is more crucial to overlay summary statistics of interest for effective visuals.\nUntil next time!\nSerdar\n\r\r","date":1571184000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571217962,"objectID":"5d4fac75b4e8241c2f9554ea127b44be","permalink":"/r/stats-ggplot/","publishdate":"2019-10-16T00:00:00Z","relpermalink":"/r/stats-ggplot/","section":"post","summary":"It is hard to understand your data by looking at the numbers on a csv file. You need to plot it. And adding statistics to your plots will make it more informative.\nTo evaluate data, we typically use mean and median to define its central tendency and range, quartiles, variance and standard deviation to define how spread it is.\nMean and standard deviation is a good representation of the data if we don’t have extreme values that result in a skewed distribution.","tags":["ggplot2","Writing functions","Data Visualization","Interquartile range"],"title":"Add custom summary statistics in ggplot2","type":"post"},{"authors":null,"categories":null,"content":"\rAccessing different data sources\rSometimes, the data you need is available on the web. Accessing those will ease your life as a data scientist.\nI want to perform an exploratory data analysis on 2018/19 Season of England Premier league.\n\rAre there changes in team performances during the season timeline?\rDoes some teams cluster?\rWhich is the earliest week we can predict team’s final positions?\r\rI need the standings table for each week of the season and integrate them in a way that will allow me to plot the graphs that I want.\rWe will scrap those tables from https://www.weltfussball.de/.For example standings table for the Week 1 is at the url:\nhttps://www.weltfussball.de/spielplan/eng-premier-league-2018-2019-spieltag/1\nFor the consequent weeks only the number at the end changes e.g.\n../spielplan/eng-premier-league-2018-2019-spieltag/2 ←\n../spielplan/eng-premier-league-2018-2019-spieltag/3 ←\n# Pull the necessary packages library(rvest) # xml2\rlibrary(tidyverse) # ggplot2, dplyr, tidyr, readr, # purrr, tibble, stringr, forcats\rlibrary(gganimate)\rlibrary(RColorBrewer)\rlibrary(kableExtra)\r# Define the remote url\rbaseUrl \u0026lt;- \u0026quot;https://www.weltfussball.de/\u0026quot;\rpath \u0026lt;- \u0026quot;spielplan/eng-premier-league-2018-2019-spieltag/\u0026quot;\rfileName \u0026lt;- 1\rurl \u0026lt;- paste0(baseUrl, path, fileName)\rurl\r## [1] \u0026quot;https://www.weltfussball.de/spielplan/eng-premier-league-2018-2019-spieltag/1\u0026quot;\rWe start by downloading and parsing the file with read_html() function from the rvest package.\ntables \u0026lt;- read_html(url)\rTo extract the html table individually you can use XPath syntax which defines parts on XML documents.\nTo get the XPath for standings table open the url on google chrome,\n\rhover the mouse over the table \u0026gt; right click \u0026gt; inspect\n# This will open inspector\rMove your mouse a few lines up or down to find the line where whole table is highlighted\rRight click \u0026gt; Copy \u0026gt; Copy full XPath\r\rWe can feed that XPath we copied to html_nodes() function and extract the node which contains the table.\nxpath = \u0026quot;/html/body/div[3]/div[2]/div[4]/div[2]/div[1]/div/div[7]/div/table[1]\u0026quot;\rnodes \u0026lt;- html_nodes(tables, xpath = xpath)\rAt the end, html_table() function will extract us the individual table.\nhtml_table(nodes)\r## [[1]]\r## # Mannschaft Mannschaft Sp. S. U. N. Tore Dif. Pk.\r## 1 1 NA Liverpool FC 1 1 0 0 4:0 4 3\r## 2 2 NA Chelsea FC 1 1 0 0 3:0 3 3\r## 3 3 NA AFC Bournemouth 1 1 0 0 2:0 2 3\r## 4 NA NA Crystal Palace 1 1 0 0 2:0 2 3\r## 5 NA NA Manchester City 1 1 0 0 2:0 2 3\r## 6 NA NA Watford FC 1 1 0 0 2:0 2 3\r## 7 7 NA Manchester United 1 1 0 0 2:1 1 3\r## 8 NA NA Tottenham Hotspur 1 1 0 0 2:1 1 3\r## 9 9 NA Everton FC 1 0 1 0 2:2 0 1\r## 10 NA NA Wolverhampton Wanderers 1 0 1 0 2:2 0 1\r## 11 11 NA Burnley FC 1 0 1 0 0:0 0 1\r## 12 NA NA Southampton FC 1 0 1 0 0:0 0 1\r## 13 13 NA Leicester City 1 0 0 1 1:2 -1 0\r## 14 NA NA Newcastle United 1 0 0 1 1:2 -1 0\r## 15 15 NA Arsenal FC 1 0 0 1 0:2 -2 0\r## 16 NA NA Brighton \u0026amp; Hove Albion 1 0 0 1 0:2 -2 0\r## 17 NA NA Cardiff City 1 0 0 1 0:2 -2 0\r## 18 NA NA Fulham FC 1 0 0 1 0:2 -2 0\r## 19 19 NA Huddersfield Town 1 0 0 1 0:3 -3 0\r## 20 20 NA West Ham United 1 0 0 1 0:4 -4 0\rWonderful, we scraped the standings table for the first week, but we want tables for each 38 week of the season.\nYou can make this easily by packing what we have done so far in a for loop.\nAs only the last number in our url link changes, we can code different url addresses as in url[[i]] \u0026lt;- paste0(baseUrl, path, i)\n# Create emtpy lists\rurl \u0026lt;- list()\rpages \u0026lt;- list()\rnodes \u0026lt;- list()\rfinal \u0026lt;- list()\rstart \u0026lt;- Sys.time()\r# For loop.\r# It will connect one by one to 38 different url links predefined # by the line starting with url[[i]]\r# Collect the information with read_html(), html_nodes() and html_table()\r# Finally each table will be converted to a data frame\rfor(i in 1:38){\rurl[[i]] \u0026lt;- paste0(baseUrl, path, i)\rpages[[i]] \u0026lt;- read_html(url[[i]])\rnodes[[i]] \u0026lt;- html_nodes(pages[[i]], xpath = xpath)\rfinal[[i]] \u0026lt;- data.frame(html_table(nodes[[i]]))\r}\r# By coding start and end times of the whole process # I can keep an eye on how fast my code is.\rend \u0026lt;- Sys.time()\rend-start\r## Time difference of 22.62705 secs\rFor example, final[[19]] will give me standings of mid season:\nfinal[[19]]\r## X. Mannschaft Mannschaft.1 Sp. S. U. N. Tore Dif. Pk.\r## 1 1 NA Liverpool FC 19 16 3 0 43:7 36 51\r## 2 2 NA Tottenham Hotspur 19 15 0 4 42:18 24 45\r## 3 3 NA Manchester City 19 14 2 3 51:15 36 44\r## 4 4 NA Chelsea FC 19 12 4 3 37:16 21 40\r## 5 5 NA Arsenal FC 19 11 5 3 41:25 16 38\r## 6 6 NA Manchester United 19 9 5 5 37:31 6 32\r## 7 7 NA Leicester City 19 8 4 7 24:22 2 28\r## 8 8 NA Everton FC 19 7 6 6 31:29 2 27\r## 9 9 NA West Ham United 19 8 3 8 27:28 -1 27\r## 10 10 NA Watford FC 19 8 3 8 26:27 -1 27\r## 11 11 NA Wolverhampton Wanderers 19 7 5 7 20:22 -2 26\r## 12 12 NA AFC Bournemouth 19 8 2 9 27:33 -6 26\r## 13 13 NA Brighton \u0026amp; Hove Albion 19 6 4 9 21:27 -6 22\r## 14 14 NA Crystal Palace 19 5 4 10 17:25 -8 19\r## 15 15 NA Newcastle United 19 4 5 10 14:26 -12 17\r## 16 16 NA Southampton FC 19 3 6 10 20:35 -15 15\r## 17 17 NA Cardiff City 19 4 3 12 18:38 -20 15\r## 18 18 NA Burnley FC 19 3 3 13 17:41 -24 12\r## 19 19 NA Fulham FC 19 2 5 12 17:43 -26 11\r## 20 20 NA Huddersfield Town 19 2 4 13 12:34 -22 10\rDon’t mind the NAs in the second column, we will remove them soon.\rNow, we have all 38 table in our list final, we can combine them to a new data frame which will contain standings of the whole season.\nTo be able to plot e.g. timeline, let’s keep the tidy data principles:\nEach observation has its own row.\rEach variable has its own column.\r\rSince we have same column names in each table, we can use rbind function to add rows of each table to the bottom of the first one. How to do that? We can’t use lapply() function here. It will not combine elements in a list. We can use do.call() function to perform the rbind() operation and combine all data frames we have*.\nuk18 \u0026lt;- do.call(\u0026quot;rbind\u0026quot;, final)\rdim(uk18)\r## [1] 760 10\rhead(uk18)\r## X. Mannschaft Mannschaft.1 Sp. S. U. N. Tore Dif. Pk.\r## 1 1 NA Liverpool FC 1 1 0 0 4:0 4 3\r## 2 2 NA Chelsea FC 1 1 0 0 3:0 3 3\r## 3 3 NA AFC Bournemouth 1 1 0 0 2:0 2 3\r## 4 NA NA Crystal Palace 1 1 0 0 2:0 2 3\r## 5 NA NA Manchester City 1 1 0 0 2:0 2 3\r## 6 NA NA Watford FC 1 1 0 0 2:0 2 3\rColumn names/shorcuts were in German, let’s replace them with the English words.\n# Correct final table\ruk18 \u0026lt;- uk18 %\u0026gt;% select(3:10)\rnew_names \u0026lt;- c(\u0026quot;team\u0026quot;, \u0026quot;week\u0026quot;, \u0026quot;won\u0026quot;, \u0026quot;drawn\u0026quot;, \u0026quot;lost\u0026quot;, \u0026quot;goals\u0026quot;, \u0026quot;difference\u0026quot;, \u0026quot;points\u0026quot;)\rcolnames(uk18) \u0026lt;- new_names\rGoals variable is contains two different data separated with “:”. E.g. (4:0). Those represent goals scored:goals scored against. Let’s split goals column into two by separate() function from tidyr.\nuk18 \u0026lt;- uk18 %\u0026gt;% separate(goals, c(\u0026quot;scored\u0026quot;, \u0026quot;against\u0026quot;), sep=\u0026quot;\\\\:\u0026quot;)\rhead(uk18)\r## team week won drawn lost scored against difference points\r## 1 Liverpool FC 1 1 0 0 4 0 4 3\r## 2 Chelsea FC 1 1 0 0 3 0 3 3\r## 3 AFC Bournemouth 1 1 0 0 2 0 2 3\r## 4 Crystal Palace 1 1 0 0 2 0 2 3\r## 5 Manchester City 1 1 0 0 2 0 2 3\r## 6 Watford FC 1 1 0 0 2 0 2 3\rI want to order my legend with the same order of teams final positions. Let’s filter for the last week of the season and arrange them in descending order. I will assign this list to the factor levels of the team variable.\n# Extract team names in the order as the season end\ruk18_filt \u0026lt;- uk18 %\u0026gt;% filter(week == 38) %\u0026gt;%\rarrange(desc(points))\rknitr::kable(uk18_filt)\r\r\r\rteam\r\rweek\r\rwon\r\rdrawn\r\rlost\r\rscored\r\ragainst\r\rdifference\r\rpoints\r\r\r\r\r\rManchester City\r\r38\r\r32\r\r2\r\r4\r\r95\r\r23\r\r72\r\r98\r\r\r\rLiverpool FC\r\r38\r\r30\r\r7\r\r1\r\r89\r\r22\r\r67\r\r97\r\r\r\rChelsea FC\r\r38\r\r21\r\r9\r\r8\r\r63\r\r39\r\r24\r\r72\r\r\r\rTottenham Hotspur\r\r38\r\r23\r\r2\r\r13\r\r67\r\r39\r\r28\r\r71\r\r\r\rArsenal FC\r\r38\r\r21\r\r7\r\r10\r\r73\r\r51\r\r22\r\r70\r\r\r\rManchester United\r\r38\r\r19\r\r9\r\r10\r\r65\r\r54\r\r11\r\r66\r\r\r\rWolverhampton Wanderers\r\r38\r\r16\r\r9\r\r13\r\r47\r\r46\r\r1\r\r57\r\r\r\rEverton FC\r\r38\r\r15\r\r9\r\r14\r\r54\r\r46\r\r8\r\r54\r\r\r\rLeicester City\r\r38\r\r15\r\r7\r\r16\r\r51\r\r48\r\r3\r\r52\r\r\r\rWest Ham United\r\r38\r\r15\r\r7\r\r16\r\r52\r\r55\r\r-3\r\r52\r\r\r\rWatford FC\r\r38\r\r14\r\r8\r\r16\r\r52\r\r59\r\r-7\r\r50\r\r\r\rCrystal Palace\r\r38\r\r14\r\r7\r\r17\r\r51\r\r53\r\r-2\r\r49\r\r\r\rNewcastle United\r\r38\r\r12\r\r9\r\r17\r\r42\r\r48\r\r-6\r\r45\r\r\r\rAFC Bournemouth\r\r38\r\r13\r\r6\r\r19\r\r56\r\r70\r\r-14\r\r45\r\r\r\rBurnley FC\r\r38\r\r11\r\r7\r\r20\r\r45\r\r68\r\r-23\r\r40\r\r\r\rSouthampton FC\r\r38\r\r9\r\r12\r\r17\r\r45\r\r65\r\r-20\r\r39\r\r\r\rBrighton \u0026amp; Hove Albion\r\r38\r\r9\r\r9\r\r20\r\r35\r\r60\r\r-25\r\r36\r\r\r\rCardiff City\r\r38\r\r10\r\r4\r\r24\r\r34\r\r69\r\r-35\r\r34\r\r\r\rFulham FC\r\r38\r\r7\r\r5\r\r26\r\r34\r\r81\r\r-47\r\r26\r\r\r\rHuddersfield Town\r\r38\r\r3\r\r7\r\r28\r\r22\r\r76\r\r-54\r\r16\r\r\r\r\rfinallevels \u0026lt;- as.character(uk18_filt$team)\ruk18$team \u0026lt;- factor(uk18$team, levels = finallevels)\rYou can also create a color palette which fits to your needs.\n# We need a color palette with 20 colors\rcolorCount \u0026lt;- length(unique(uk18$team))\r# colorRampPalette creatas a getPalette() function\r# This can modify an existing palette to include as many colors we want\rgetPalette \u0026lt;- colorRampPalette(brewer.pal(9, \u0026quot;Set1\u0026quot;))\rgetPalette(colorCount)\r## [1] \u0026quot;#E41A1C\u0026quot; \u0026quot;#9B445D\u0026quot; \u0026quot;#526E9F\u0026quot; \u0026quot;#3C8A9B\u0026quot; \u0026quot;#469F6C\u0026quot; \u0026quot;#54A453\u0026quot; \u0026quot;#747B78\u0026quot;\r## [8] \u0026quot;#94539E\u0026quot; \u0026quot;#BD6066\u0026quot; \u0026quot;#E97422\u0026quot; \u0026quot;#FF990A\u0026quot; \u0026quot;#FFCF20\u0026quot; \u0026quot;#FAF632\u0026quot; \u0026quot;#D4AE2D\u0026quot;\r## [15] \u0026quot;#AF6729\u0026quot; \u0026quot;#BF6357\u0026quot; \u0026quot;#E17597\u0026quot; \u0026quot;#E884B9\u0026quot; \u0026quot;#C08EA9\u0026quot; \u0026quot;#999999\u0026quot;\r# Plot season timeline using the palette we just created\ruk \u0026lt;- ggplot(uk18, aes(x=week, y=points, col=team)) + geom_point(size=3) + theme(text = element_text(size=15)) + scale_color_manual(values = getPalette(colorCount))\rLet’s plot the regression lines\n# Plot season timeline\ruk \u0026lt;- ggplot(uk18, aes(x=week, y=points, col=team)) + geom_smooth(se=TRUE) + theme(text = element_text(size=15)) + scale_color_manual(values = getPalette(colorCount))\ruk\r## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula \u0026#39;y ~ x\u0026#39;\ruk_facet \u0026lt;- ggplot(uk18, aes(x=week, y=points, col=team)) + geom_smooth(se=FALSE) + theme(text = element_text(size=10)) + scale_color_manual(values = getPalette(colorCount)) + facet_wrap(ncol = 4, team~.)\ruk_facet\r## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula \u0026#39;y ~ x\u0026#39;\rSome insights from the plots:\nI see three clusters here. Two teams (Man. City and Liverpool) competed head to head for the championship and next three teams (Chelsea, Tottenham and Arsenal) for the 3rd position.\n\rWe can predict 4 out of 5 teams which will take first 5 place at the end of the season early as week 10.\rManchester United showed peak performance mid season, Everton have improved performances while Tottenham slowed down (which costed them 3rd position) in the second part of the season.\r\rI can plot points against goal differences in the same plot. Same clusters pop up here as well.\nuk \u0026lt;- ggplot(uk18, aes(x=difference, y=points, col=team)) + geom_point(size=2) + scale_color_manual(values = getPalette(colorCount)) + theme(text = element_text(size=15))\ruk\rLet’s visualize this in a small animation. You can create an animated plot of the teams progress during the season. Gganimate does good job.`\n# Add a shadow tail\r# anim + shadow_wake(wake_length = 0.3, alpha = FALSE)\ranim \u0026lt;- uk + transition_time(week) + labs(title = \u0026quot;week: {round(frame_time,0)}\u0026quot;) + shadow_wake(wake_length = 0.1, alpha = 0.5)\rfullanimation \u0026lt;- animate(anim, fps= 7, nframes=100, height=500, width=800, res=0.8)\rfullanimation\r\rConclusions / Future Thoughts\rOne of the most important steps to answer a research question is gathering and pre-processing data that fits best for the planned analysis.\nSome of the questions we tackled were:\n\rHow to find the XPath for an html table in a website?\rHow to combine data frames from a list?\rHow to split columns containing more than one variable?\r\rThe earliest time, we can predict top teams final positions was around 10th. We can collect data from previous years or compare other countries leagues to check if we can generalize this finding.\nWhat else we can ask? For example, we can connect performance changes to new transfers. Or whether changing coaches benefited any team.\nPlease share if you have other ideas in the comments below!\nUntil next time!\nSerdar\nPS: If you are looking for more blogs to learn R you might check also:\n\rhttps://www.r-bloggers.com\rhttps://rweekly.org\r\r\r","date":1570665600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570682065,"objectID":"00bf982dbf6dc64b6e4ea4f05cfa3239","permalink":"/r/scrape-tables-rvest/","publishdate":"2019-10-10T00:00:00Z","relpermalink":"/r/scrape-tables-rvest/","section":"post","summary":"Accessing different data sources\rSometimes, the data you need is available on the web. Accessing those will ease your life as a data scientist.\nI want to perform an exploratory data analysis on 2018/19 Season of England Premier league.\n\rAre there changes in team performances during the season timeline?\rDoes some teams cluster?\rWhich is the earliest week we can predict team’s final positions?\r\rI need the standings table for each week of the season and integrate them in a way that will allow me to plot the graphs that I want.","tags":["data wrangling","rvest","Web scraping","data integration","XPath","gganimate","ggplot2","tidyr","premier league","Blog"],"title":"Data Preparation: Web Scraping html tables with rvest","type":"post"},{"authors":null,"categories":null,"content":"\rggplot2 is a powerful data visualization tool of R. Make quick visualizations to explore or share your insights.\nLearning how aesthetics and attributes are defined in ggplot will give you an edge to develop your skills quickly.\nggplot2 tips: distinction between aesthetics and attributes\rAesthetics are defined inside aes() in ggplot syntax and attributes are outside the aes().\n\re.g. ggplot(data, aes(x, y, color=var1) + geom_point(size=6)\n\rWe typically understand aesthetics as how something looks, color, size etc.\nBut in ggplot’s world how things look is just an attribute.\n Aesthetics do not refer how something looks, but to which variable is mapped onto it.   I will create an imaginary data frame to apply those concepts.\nlibrary(ggplot2)\rpoints \u0026lt;- 500\r# Defining the Golden Angle\rangle \u0026lt;- pi*(3-sqrt(5))\rt \u0026lt;- (1:points) * angle\rx \u0026lt;- sin(t/2)\ry \u0026lt;-cos(t/2)\rz \u0026lt;- rep(c(1,2,3,4,5,6,7,8,9,10), times=50)\rw \u0026lt;- rep(c(1,2), times=250)\rdf \u0026lt;- data.frame(t, x, y, z, w)\r# Have a look at the data\rhead(df)\r## t x y z w\r## 1 2.399963 0.9320324 0.36237489 1 1\r## 2 4.799926 0.6754903 -0.73736888 2 2\r## 3 7.199890 -0.4424710 -0.89678282 3 1\r## 4 9.599853 -0.9961710 0.08742572 4 2\r## 5 11.999816 -0.2795038 0.96014460 5 1\r## 6 14.399779 0.7936008 0.60843886 6 2\rThe dataframe we created has 3 numeric (t, x, y) variables and 2 discrete variables (z, w). With ggplot2 I can map any of the variables on my plot by defining them inside the aes().\n# Make a scatter plot of points of a spiral\rp \u0026lt;- ggplot(df, aes(x*t, y*t))\rp + geom_point()\r\rExample use of an aesthetics\rBy defining col=factor(z) inside aes(), I can map z to colors. So now the graph shows x, y and also values z.\n# Make a scatter plot of points in a spiral\rp \u0026lt;- ggplot(df, aes(x*t, y*t, col=factor(z)))\rp + geom_point()\rEach different color now represents different values of z.\n\rExample use of an attribute\rAttribute is how somethings looks. e.g. you can the points bigger by defining size=4. But it does not give any extra information about data.\n# Make a scatter plot of points in a spiral\rp \u0026lt;- ggplot(df, aes(x*t, y*t, col=factor(z)))\rp + geom_point(size = 4)\r\rUse shape as an attribute\rSame goes here. I am changing how something looks like. The data point shape change to 24 which defines a empty triangle. But nothing is mapped onto it. It is just an attribute.\n# Make a scatter plot of points in a spiral\rp \u0026lt;- ggplot(df, aes(x*t, y*t, color=factor(z)))\rp + geom_point(shape=24, size=4)\rHere, xt, yt and factor(z) is mapped on to our graph.\n\rUsing shape as an aesthetics\rBy defining shape and color inside aes() I can map w and z to my plot as well.\npoints \u0026lt;- 500\r# Defining the Golden Angle\rangle \u0026lt;- pi*(3-sqrt(5))\rt \u0026lt;- (1:points) * angle\rx \u0026lt;- sin(t)\ry \u0026lt;-cos(t)\rz \u0026lt;- rep(c(1,2,3,4,5,6,7,8,9,10), times=50)\rw \u0026lt;- rep(c(1,2), times=250)\rdf \u0026lt;- data.frame(t, x, y, z, w)\rp \u0026lt;- ggplot(df, aes(x*t, y*t, shape=factor(w), color=factor(z)))\rp + geom_point(size=3)\rSpirals look nice and we got some basics of ggplot. Now let’s use it to create a pattern designer, with Shiny. Many patterns in Nature can be explained by mathematical terms, Shapes of sunflowers, dandelions or snowflakes etc.\nI will tell the rest of the story in the next update. Now you can play with the app to create your patterns!\n\rUntil next time!\nSerdar\n\r","date":1570492800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570531082,"objectID":"94e6999b719862233973a4fed37879bb","permalink":"/r/ggplot-shiny-app/","publishdate":"2019-10-08T00:00:00Z","relpermalink":"/r/ggplot-shiny-app/","section":"post","summary":"ggplot2 is a powerful data visualization tool of R. Make quick visualizations to explore or share your insights.\nLearning how aesthetics and attributes are defined in ggplot will give you an edge to develop your skills quickly.\nggplot2 tips: distinction between aesthetics and attributes\rAesthetics are defined inside aes() in ggplot syntax and attributes are outside the aes().\n\re.g. ggplot(data, aes(x, y, color=var1) + geom_point(size=6)\n\rWe typically understand aesthetics as how something looks, color, size etc.","tags":["aesthetics","attributes","data visualization","ggplot2","pattern designer","phyllotaxis","RStudio","Shiny","web app"],"title":"What is aesthetics and attributes in ggplot's world?","type":"post"},{"authors":[],"categories":[],"content":"\rWould you be taking care of yourself better if your doctor told today that you have high risk of diabetes?\nAdvances in fields, such as omics and internet of things (sensors that collect data), and centralization of healthcare information (e.g. OMOP common data model) enable us to access much wider data sources.\nGaining insights from those we can improve our well being with better healthcare. Some applications of machine learning tools are;\n\rDiagnosing diseases earlier\rIdentifying drugs with reduced side effects\rSelect patient groups responding to an experimental therapy\rUtilize existing therapies better\r\rLet’s look at an example and try to help some doctors in diagnosing their patients.\nProblem formulation\r\rCan we build a Machine learning algorithm to predict which patients will develop Diabetes?\n\rThe goal is to predict a Binary Outcome: Diabetes vs Healthy\rby using 8 medical indicators.\n\rOverview of the data\rThe Pima Indians of Arizona and Mexico have contributed to numerous scientific gains. Their involvement has led to significant findings on genetics of both type 2 diabetes and obesity.\nThe medical indicators recorded are;\nPregnancies: Number of times pregnant\nGlucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\nBloodPressure: Diastolic blood pressure (mm Hg)\nSkinThickness: Triceps skin fold thickness (mm)\nInsulin: 2-Hour serum insulin (mu U/ml)\nBMI: Body mass index (weight in kg/(height in m)^2)\nDiabetesPedigreeFunction: Diabetes pedigree function\nAge: Age (years)\nOutcome: Class variable (0 or 1)\n\rData acquisition\rI downloaded the Pima Indians Diabetes Dataset from Kaggle and imported it from my local directory.\ndiabetes \u0026lt;- read.csv(\u0026quot;posts_data/diabetes.csv\u0026quot;)\rlibrary(tidyverse) # ggplot2, dplyr, tidyr, readr, # purrr, tibble, stringr, forcats\rlibrary(reshape2)\rlibrary(ggcorrplot)\rlibrary(pROC)\rlibrary(lattice)\rlibrary(caret)\rlibrary(waffle)\r\rData Quality control\rBefore counting on any algorithm a good starting point is to check obvious mistakes and abnormalities in your data.\nHere, I would look at Missing values, variable ranges (min, max values). A very extreme value might be basically a sign of typing error.\nUnderstand your Data\rHow big is the data? Classes of variables?\ndim(diabetes)\r## [1] 768 9\rknitr::kable(sapply(diabetes, class))\r\r\r\rx\r\r\r\rPregnancies\rinteger\r\rGlucose\rinteger\r\rBloodPressure\rinteger\r\rSkinThickness\rinteger\r\rInsulin\rinteger\r\rBMI\rnumeric\r\rDiabetesPedigreeFunction\rnumeric\r\rAge\rinteger\r\rOutcome\rinteger\r\r\r\rNext, what catches my attention is unexpected zero values in Insulin. Look below.\nknitr::kable(head(diabetes))\r\r\rPregnancies\rGlucose\rBloodPressure\rSkinThickness\rInsulin\rBMI\rDiabetesPedigreeFunction\rAge\rOutcome\r\r\r\r6\r148\r72\r35\r0\r33.6\r0.627\r50\r1\r\r1\r85\r66\r29\r0\r26.6\r0.351\r31\r0\r\r8\r183\r64\r0\r0\r23.3\r0.672\r32\r1\r\r1\r89\r66\r23\r94\r28.1\r0.167\r21\r0\r\r0\r137\r40\r35\r168\r43.1\r2.288\r33\r1\r\r5\r116\r74\r0\r0\r25.6\r0.201\r30\r0\r\r\r\r\rMissing Values\rSummary gives a good overview of the variables. Any missing data will show up here listed as “NA’s”.\nsummary(diabetes)\r## Pregnancies Glucose BloodPressure SkinThickness ## Min. : 0.000 Min. : 0.0 Min. : 0.00 Min. : 0.00 ## 1st Qu.: 1.000 1st Qu.: 99.0 1st Qu.: 62.00 1st Qu.: 0.00 ## Median : 3.000 Median :117.0 Median : 72.00 Median :23.00 ## Mean : 3.845 Mean :120.9 Mean : 69.11 Mean :20.54 ## 3rd Qu.: 6.000 3rd Qu.:140.2 3rd Qu.: 80.00 3rd Qu.:32.00 ## Max. :17.000 Max. :199.0 Max. :122.00 Max. :99.00 ## Insulin BMI DiabetesPedigreeFunction Age ## Min. : 0.0 Min. : 0.00 Min. :0.0780 Min. :21.00 ## 1st Qu.: 0.0 1st Qu.:27.30 1st Qu.:0.2437 1st Qu.:24.00 ## Median : 30.5 Median :32.00 Median :0.3725 Median :29.00 ## Mean : 79.8 Mean :31.99 Mean :0.4719 Mean :33.24 ## 3rd Qu.:127.2 3rd Qu.:36.60 3rd Qu.:0.6262 3rd Qu.:41.00 ## Max. :846.0 Max. :67.10 Max. :2.4200 Max. :81.00 ## Outcome ## Min. :0.000 ## 1st Qu.:0.000 ## Median :0.000 ## Mean :0.349 ## 3rd Qu.:1.000 ## Max. :1.000\rI will make visualizations of the variables to see how they are distributed.\ngg \u0026lt;- melt(diabetes)\r## No id variables; using all as measure variables\rggplot(gg, aes(x=value, fill=variable)) +\rgeom_histogram(binwidth=5)+ facet_wrap(~variable) \rPeaks at zero of Skin Thickness and Insulin is obvious here.\nIn cases where the numbers are small we might remove them. Let’s figure it out with a for loop. and then visualize on a waffle plot.\nThis will bring me the number of zero containing rows in variables from 2 to 6.\nzero_rows \u0026lt;- list()\rfor(i in 2:6){\rzero_rows[[i]] \u0026lt;- length(which(diabetes[,i] == 0)) }\rrows_with_zero \u0026lt;- unlist(zero_rows)\rFeed those numbers to a waffle plot.\nzeros \u0026lt;- c(\u0026quot;Glucose\u0026quot; =rows_with_zero[1], \u0026quot;Blood Pressure\u0026quot; = rows_with_zero[2], \u0026quot;Skin Thickness\u0026quot;= rows_with_zero[3], \u0026quot;Insulin\u0026quot; =rows_with_zero[4], \u0026quot;BMI\u0026quot; = rows_with_zero[5])\rwaffle(zeros, rows=20) + theme(text = element_text(size=15)) + ggtitle(\u0026quot;Number of rows with zero\u0026quot;)\rdf \u0026lt;- data.frame(rows_with_zero, row.names = names(diabetes[2:6]))\rdf\r## rows_with_zero\r## Glucose 5\r## BloodPressure 35\r## SkinThickness 227\r## Insulin 374\r## BMI 11\rFor instance, 374 Insulin values are zero. Other variables also contain zeros. It is impossible to have Blood Pressure or Glucose levels at 0. It is unlikely that those are simply entry mistakes. It seems missing values are filled with zeros in the data collection phase.\nHow to circumvent this?\nConvert all zeroes to NAs and then perform Median Imputation\rMost models require numbers, and can’t handle missing data. Throwing out rows is not a good idea since it can lead to biases in your dataset and generate overconfident models.\nMedian imputation lets you model data with missing values. By replacing them with their medians.\nTo do this, I need to change zeros to missing values. I will do this for all the predictors which zero is not plausible(columns 2 to 6).\nfor(i in 2:6){\r# Convert zeros to NAs\rdiabetes[, i][diabetes[, i] == 0] \u0026lt;- NA\r# Calculate median\rmedian \u0026lt;- median(diabetes[, i], na.rm = TRUE)\rdiabetes[, i][is.na(diabetes[, i])] \u0026lt;- median\r}\rCheck if it really happened.\nknitr::kable(head(diabetes))\r\r\rPregnancies\rGlucose\rBloodPressure\rSkinThickness\rInsulin\rBMI\rDiabetesPedigreeFunction\rAge\rOutcome\r\r\r\r6\r148\r72\r35\r125\r33.6\r0.627\r50\r1\r\r1\r85\r66\r29\r125\r26.6\r0.351\r31\r0\r\r8\r183\r64\r29\r125\r23.3\r0.672\r32\r1\r\r1\r89\r66\r23\r94\r28.1\r0.167\r21\r0\r\r0\r137\r40\r35\r168\r43.1\r2.288\r33\r1\r\r5\r116\r74\r29\r125\r25.6\r0.201\r30\r0\r\r\r\rFor instance, I see that zero values in the insulin variable is replaced with median of insulin which is 125.\nNow, the data is clean and ready for the modeling phase.\n\r\r\rModeling the data (build, fit and validate a model)\rBefore going into any complicated model starting with a simple model is a good idea. It might do surprisingly well and will give us more insights.\nLogistic Regression Model\rWe will create two random subsets of our data in 80/20 proportion as training and test data. Training data will be used to build our model and test data will be reserved to validate it.\nset.seed(22)\r# Create train test split\rsample_rows \u0026lt;- sample(nrow(diabetes), nrow(diabetes) * 0.8)\r# Create the training dataset\rdia_train \u0026lt;- diabetes[sample_rows, ]\r# Create the test dataset\rdia_test \u0026lt;- diabetes[-sample_rows, ]\r# Build a logistic regression model with the train data\rglm_dia \u0026lt;- glm(Outcome ~ .,data = dia_train)\rsummary(glm_dia)\r## ## Call:\r## glm(formula = Outcome ~ ., data = dia_train)\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.01862 -0.28264 -0.07603 0.29983 0.86403 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -1.0441425 0.1180483 -8.845 \u0026lt; 2e-16 ***\r## Pregnancies 0.0173561 0.0056433 3.076 0.002196 ** ## Glucose 0.0063034 0.0006150 10.250 \u0026lt; 2e-16 ***\r## BloodPressure -0.0013212 0.0014700 -0.899 0.369137 ## SkinThickness 0.0008966 0.0023291 0.385 0.700395 ## Insulin -0.0003233 0.0002088 -1.548 0.122077 ## BMI 0.0150019 0.0030523 4.915 1.15e-06 ***\r## DiabetesPedigreeFunction 0.1661357 0.0489591 3.393 0.000736 ***\r## Age 0.0036157 0.0017283 2.092 0.036845 * ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for gaussian family taken to be 0.1572889)\r## ## Null deviance: 139.41 on 613 degrees of freedom\r## Residual deviance: 95.16 on 605 degrees of freedom\r## AIC: 617.69\r## ## Number of Fisher Scoring iterations: 2\rThe summary shows us not all the variables play a role in predicting outcome. The signicant correlations was found for Pregnancies, Glucose, BMI and Pedigree function.\nThe predict function will give us probabilities. To compute our model accuracy we need to convert them to class predictions by setting a threshold level.\n# We will predict the Outcome for the test data\rp\u0026lt;-predict(glm_dia, dia_test)\r# Choose a threshold 0.5 to calculate the accuracy of our model\rp_05 \u0026lt;- ifelse(p \u0026gt; 0.5, 1, 0)\rtable(p_05, dia_test$Outcome)\r## ## p_05 0 1\r## 0 85 17\r## 1 15 37\rWe will build a confusion matrix to calculate how accurate our model is in this particular random train/test split and at 0.5 threshold level.\nconf_mat \u0026lt;- table(p_05, dia_test$Outcome)\raccuracy \u0026lt;- sum(diag(conf_mat))/sum(conf_mat)\raccuracy\r## [1] 0.7922078\rroc function pROC package, can plot us a ROC curve which tests accuracy of our model at multiple threshold levels and is a good estimate on how well our model is performing.\n# Calculate AUC(Area under the curve)\rroc(dia_test$Outcome, p)\r## Setting levels: control = 0, case = 1\r## Setting direction: controls \u0026lt; cases\r## ## Call:\r## roc.default(response = dia_test$Outcome, predictor = p)\r## ## Data: p in 100 controls (dia_test$Outcome 0) \u0026lt; 54 cases (dia_test$Outcome 1).\r## Area under the curve: 0.8498\rHowever, this process is little fragile, presence or absence of a single outlier might vastly change the results you might get from a given random train/test split.\nA better approach than a simple train/test split is using multiple test sets and averaging their accuracies.\nLet’s test that. I will create 1, 30 or 1000 random test sets, build models and compare their accuracies.\nHow to apply multiple train/test split\rTo do this, I will write a function where I can choose number of independent train/test splits.\nIt will return me an average value of the accuracy(auc) of the model after chosen number of iteration. The higher the number of random splits the more stable your estimated AUC will be.\nLet’s see how it will work out for our diabetes patients.\n# I will define my function as follows\rmulti_split \u0026lt;- function(x){\rsample_rows \u0026lt;- list()\rdia_train \u0026lt;- list()\rdia_test \u0026lt;- list()\rglm \u0026lt;- list()\rp \u0026lt;- list()\rroc_auc \u0026lt;- list()\rfor(i in 1:x){\rsample_rows[[i]] \u0026lt;- sample(nrow(diabetes), nrow(diabetes) * 0.8)\r# Create the training dataset\rdia_train[[i]] \u0026lt;- diabetes[sample_rows[[i]], ]\r# Create the test dataset\rdia_test[[i]] \u0026lt;- diabetes[-sample_rows[[i]], ]\rglm[[i]] \u0026lt;- glm(Outcome ~ .,data = dia_train[[i]])\rp[[i]] \u0026lt;- predict(glm[[i]], dia_test[[i]])\r# Calculate AUC for all \u0026quot;x\u0026quot; number of random splits\rroc_auc[[i]] \u0026lt;- roc(dia_test[[i]]$Outcome, p[[i]])$auc[1]\rglm_mean \u0026lt;- mean(unlist(roc_auc))\r}\rprint(mean(unlist(roc_auc)))\r}\rLet’s calculate the average AUC of our model after different number of random splits.\nI will run my multi_split() function 3x for 1, 30 and 1000 random train/test splits. I can then compare variances at each level of sampling.\nHere are the results from my multi_site function at each randomization.\nauc_1_1 \u0026lt;- multi_split(1)\r## [1] 0.8769133\rauc_1_2 \u0026lt;- multi_split(1)\r## [1] 0.8283688\rauc_1_3 \u0026lt;- multi_split(1)\r## [1] 0.8216108\rauc_30_1 \u0026lt;- multi_split(30)\r## [1] 0.840713\rauc_30_2 \u0026lt;- multi_split(30)\r## [1] 0.8501071\rauc_30_3 \u0026lt;- multi_split(30)\r## [1] 0.8294741\rauc_1000_1 \u0026lt;- multi_split(1000)\r## [1] 0.8362065\rauc_1000_2 \u0026lt;- multi_split(1000)\r## [1] 0.8364364\rauc_1000_3 \u0026lt;- multi_split(1000)\r## [1] 0.8350323\rLet’s compare Variance levels at 1, 30 and 1000 random splits\nvar(c(auc_1_1, auc_1_2, auc_1_3))\r## [1] 0.0009101001\rvar(c(auc_30_1, auc_30_2, auc_30_3))\r## [1] 0.0001067148\rvar(c(auc_1000_1, auc_1000_2, auc_1000_3))\r## [1] 5.672518e-07\rWhat we see here as we increase the number of iterations to 30 and 1000 the variability\rgradually stabilizes around a trustable AUC of 0.836.\nSeeing is believing. Let’s plot it.\n# Create a data.frame containing accuracies\rrandom_1X \u0026lt;- c(auc_1_1, auc_1_2, auc_1_3)\rrandom_30X \u0026lt;- c(auc_30_1, auc_30_2, auc_30_3)\rrandom_1000X \u0026lt;- c(auc_1000_1, auc_1000_2, auc_1000_3)\rdf_r \u0026lt;- data.frame(random_1X, random_30X, random_1000X)\rdf_r\r## random_1X random_30X random_1000X\r## 1 0.8769133 0.8407130 0.8362065\r## 2 0.8283688 0.8501071 0.8364364\r## 3 0.8216108 0.8294741 0.8350323\r# Here, I will reformat my data for easy plotting by using gather() function from tidyr\r# It takes multiple columns, and gathers them into key-value pairs: it makes “wide” data longer.\rdf_long \u0026lt;- gather(df_r, sampling, auc)\rdf_long\r## sampling auc\r## 1 random_1X 0.8769133\r## 2 random_1X 0.8283688\r## 3 random_1X 0.8216108\r## 4 random_30X 0.8407130\r## 5 random_30X 0.8501071\r## 6 random_30X 0.8294741\r## 7 random_1000X 0.8362065\r## 8 random_1000X 0.8364364\r## 9 random_1000X 0.8350323\rdf_long$sampling \u0026lt;- factor(df_long$sampling, levels = c(\u0026quot;random_1X\u0026quot;, \u0026quot;random_30X\u0026quot;, \u0026quot;random_1000X\u0026quot;))\r# model_variation \u0026lt;- ggplot(df_long, aes(y=auc, x=sampling, fill=sampling)) + geom_boxplot() + theme(text = element_text(size=15), axis.title.x=element_blank(), legend.position = \u0026quot;none\u0026quot;) + ggtitle(\u0026quot;Variation in model performance\u0026quot;)\rmodel_variation\rGreat. We have an estimate of our model performance after 1000 random train/test splits. This process is also called Monte-Carlo Cross validation. This approach might give you a less variable, but more biased estimate.\nA more common approach to estimate model performance is k-Fold cross Validation. Where the samples divided into k-folds and one fold is used as a test set, and the remaining k-1 as the training set. This process is run k times until all folds appear once in the test sample.\n\r\rLogistic regression model with k-fold Cross Validation\rI will switch here to caret package. With the Train() function we can test different types of machine learning algorithms and set the cross validation parameters.\nTo make the models below comparable I will create a custom cross validation fold object (d_folds) that I can apply to multiple models.\nI will repeat the logistic regression model with 5 fold cross validation and then we can compare it to monte carlo cross validation.\n# Convert Outcome to a factor with two levels\rdiabetes$Outcome \u0026lt;- ifelse(diabetes$Outcome == 1, \u0026quot;Yes\u0026quot;, \u0026quot;No\u0026quot;)\routcome \u0026lt;- diabetes$Outcome\rd_folds \u0026lt;- createFolds(outcome, k=5)\r# Create a dataframe without the outcome column\rdiab \u0026lt;- diabetes[,-9]\r# MyControl\rmyControl \u0026lt;- trainControl(\rsummaryFunction = twoClassSummary,\rclassProbs = TRUE,\rverboseIter = TRUE,\rsavePredictions = TRUE,\rindex = d_folds\r)\r# Model_glm\rmodel_glm \u0026lt;- train(x = diab, y = outcome,\rmetric = \u0026quot;ROC\u0026quot;,\rmethod = \u0026quot;glm\u0026quot;,\rtrControl = myControl\r)\r## + Fold1: parameter=none ## - Fold1: parameter=none ## + Fold2: parameter=none ## - Fold2: parameter=none ## + Fold3: parameter=none ## - Fold3: parameter=none ## + Fold4: parameter=none ## - Fold4: parameter=none ## + Fold5: parameter=none ## - Fold5: parameter=none ## Aggregating results\r## Fitting final model on full training set\rmodel_glm\r## Generalized Linear Model ## ## 768 samples\r## 8 predictor\r## 2 classes: \u0026#39;No\u0026#39;, \u0026#39;Yes\u0026#39; ## ## No pre-processing\r## Resampling: Bootstrapped (5 reps) ## Summary of sample sizes: 154, 153, 153, 154, 154 ## Resampling results:\r## ## ROC Sens Spec ## 0.8136093 0.844 0.577431\rHere, My model performance is 0.8136093\n\rGlmnet model\r# Model\rmodel_glmnet \u0026lt;- train(x = diab, y = outcome,\rmetric = \u0026quot;ROC\u0026quot;,\rmethod = \u0026quot;glmnet\u0026quot;, tuneGrid = expand.grid(\ralpha = 0:1,\rlambda = seq(0.0001, 1, length = 20)\r),\rtrControl = myControl\r)\rmodel_glmnet\r## glmnet ## ## 768 samples\r## 8 predictor\r## 2 classes: \u0026#39;No\u0026#39;, \u0026#39;Yes\u0026#39; ## ## No pre-processing\r## Resampling: Bootstrapped (5 reps) ## Summary of sample sizes: 154, 153, 153, 154, 154 ## Resampling results across tuning parameters:\r## ## alpha lambda ROC Sens Spec ## 0 0.00010000 0.8201210 0.8615 0.554096935\r## 0 0.05272632 0.8227469 0.8725 0.541977831\r## 0 0.10535263 0.8239613 0.8880 0.506537709\r## 0 0.15797895 0.8242214 0.9005 0.482290806\r## 0 0.21060526 0.8240544 0.9115 0.444025212\r## 0 0.26323158 0.8238034 0.9210 0.420725929\r## 0 0.31585789 0.8233937 0.9290 0.395548794\r## 0 0.36848421 0.8230260 0.9365 0.357313627\r## 0 0.42111053 0.8228119 0.9430 0.331201913\r## 0 0.47373684 0.8224972 0.9505 0.306955010\r## 0 0.52636316 0.8222639 0.9575 0.276166051\r## 0 0.57898947 0.8220894 0.9615 0.254705499\r## 0 0.63161579 0.8219008 0.9670 0.238852423\r## 0 0.68424211 0.8217353 0.9690 0.217396218\r## 0 0.73686842 0.8215489 0.9705 0.200604216\r## 0 0.78949474 0.8214394 0.9725 0.178235166\r## 0 0.84212105 0.8212995 0.9755 0.157704847\r## 0 0.89474737 0.8210990 0.9785 0.138109107\r## 0 0.94737368 0.8209896 0.9800 0.125976962\r## 0 1.00000000 0.8208614 0.9805 0.111980004\r## 1 0.00010000 0.8141708 0.8450 0.575553141\r## 1 0.05272632 0.8232373 0.9040 0.505646599\r## 1 0.10535263 0.8064789 0.9490 0.348028689\r## 1 0.15797895 0.7982626 0.9865 0.118687242\r## 1 0.21060526 0.7913587 0.9995 0.002803738\r## 1 0.26323158 0.5552114 1.0000 0.000000000\r## 1 0.31585789 0.5000000 1.0000 0.000000000\r## 1 0.36848421 0.5000000 1.0000 0.000000000\r## 1 0.42111053 0.5000000 1.0000 0.000000000\r## 1 0.47373684 0.5000000 1.0000 0.000000000\r## 1 0.52636316 0.5000000 1.0000 0.000000000\r## 1 0.57898947 0.5000000 1.0000 0.000000000\r## 1 0.63161579 0.5000000 1.0000 0.000000000\r## 1 0.68424211 0.5000000 1.0000 0.000000000\r## 1 0.73686842 0.5000000 1.0000 0.000000000\r## 1 0.78949474 0.5000000 1.0000 0.000000000\r## 1 0.84212105 0.5000000 1.0000 0.000000000\r## 1 0.89474737 0.5000000 1.0000 0.000000000\r## 1 0.94737368 0.5000000 1.0000 0.000000000\r## 1 1.00000000 0.5000000 1.0000 0.000000000\r## ## ROC was used to select the optimal model using the largest value.\r## The final values used for the model were alpha = 0 and lambda = 0.1579789.\rplot(model_glmnet)\rAs we see in the plot, ridge regression (alpha = 0) performed better than the lasso at all lambda values.\nGlmnet model performance is 0.8242214\n\rRandom forest model\rOne of the big diferences between random forest and linear models is that they require “tuning.”\nHyperparameters –\u0026gt; How the model is fit. Selected by hand.\nadvantages: no need to log transform or normalize,\rbut they are less interpretable and slower than glmnet.\nRandom forests capture threshold effects and variable interactions. both of which occur often in real world data\nmtry is the number of variables used at each split point in individual decision tree that make up the rf. Default is 3, I will use here 8.\ntuneLength = how many different mtry values to be tested.\n# Random forest model\rmodel_rf \u0026lt;- train(x = diab, y = outcome,\rtuneLength = 8,\rmetric = \u0026quot;ROC\u0026quot;,\rmethod = \u0026quot;ranger\u0026quot;,\rtrControl = myControl\r)\rmodel_rf\r## Random Forest ## ## 768 samples\r## 8 predictor\r## 2 classes: \u0026#39;No\u0026#39;, \u0026#39;Yes\u0026#39; ## ## No pre-processing\r## Resampling: Bootstrapped (5 reps) ## Summary of sample sizes: 154, 153, 153, 154, 154 ## Resampling results across tuning parameters:\r## ## mtry splitrule ROC Sens Spec ## 2 gini 0.8155919 0.8565 0.5410389\r## 2 extratrees 0.8247223 0.8730 0.5419474\r## 3 gini 0.8142703 0.8490 0.5596827\r## 3 extratrees 0.8241664 0.8650 0.5550185\r## 4 gini 0.8098886 0.8440 0.5559617\r## 4 extratrees 0.8244122 0.8595 0.5671376\r## 5 gini 0.8096393 0.8475 0.5596914\r## 5 extratrees 0.8244900 0.8550 0.5708498\r## 6 gini 0.8075402 0.8460 0.5578309\r## 6 extratrees 0.8231397 0.8530 0.5755401\r## 7 gini 0.8063892 0.8415 0.5662117\r## 7 extratrees 0.8213101 0.8515 0.5652728\r## 8 gini 0.8058247 0.8335 0.5717931\r## 8 extratrees 0.8209827 0.8485 0.5773788\r## ## Tuning parameter \u0026#39;min.node.size\u0026#39; was held constant at a value of 1\r## ROC was used to select the optimal model using the largest value.\r## The final values used for the model were mtry = 2, splitrule =\r## extratrees and min.node.size = 1.\rRandom forest performance is 0.8247223\n\rGradient boost model\rI will define manualy a grid to test hyperparameter values wider than set in default.\ngrid \u0026lt;- expand.grid(interaction.depth = c(1, 2, 3, 4, 5),\rn.trees = (1:20)*50, shrinkage = 0.01,\rn.minobsinnode = 10)\rmodel_gbm \u0026lt;- train(x = diab, y = outcome,\rmetric = \u0026quot;ROC\u0026quot;,\rmethod = \u0026quot;gbm\u0026quot;,\rtuneGrid = grid,\rtrControl = myControl\r)\rmodel_gbm\r## Stochastic Gradient Boosting ## ## 768 samples\r## 8 predictor\r## 2 classes: \u0026#39;No\u0026#39;, \u0026#39;Yes\u0026#39; ## ## No pre-processing\r## Resampling: Bootstrapped (5 reps) ## Summary of sample sizes: 154, 153, 153, 154, 154 ## Resampling results across tuning parameters:\r## ## interaction.depth n.trees ROC Sens Spec ## 1 50 0.7996312 0.9850 0.09252336\r## 1 100 0.8103617 0.9505 0.32756357\r## 1 150 0.8153748 0.9275 0.42350793\r## 1 200 0.8180549 0.9100 0.48598131\r## 1 250 0.8213254 0.8965 0.51863943\r## 1 300 0.8221740 0.8860 0.52983699\r## 1 350 0.8210830 0.8815 0.53729189\r## 1 400 0.8218884 0.8740 0.55409694\r## 1 450 0.8222721 0.8680 0.55874375\r## 1 500 0.8212831 0.8625 0.55593567\r## 1 550 0.8222189 0.8595 0.56525973\r## 1 600 0.8210507 0.8540 0.56620300\r## 1 650 0.8206810 0.8510 0.56806781\r## 1 700 0.8197731 0.8495 0.56806781\r## 1 750 0.8194073 0.8495 0.57180178\r## 1 800 0.8179990 0.8460 0.57459683\r## 1 850 0.8173591 0.8435 0.57459683\r## 1 900 0.8167653 0.8400 0.57272332\r## 1 950 0.8159870 0.8395 0.57366225\r## 1 1000 0.8155800 0.8375 0.57738752\r## 2 50 0.8097645 0.9705 0.19896110\r## 2 100 0.8153114 0.9225 0.42633341\r## 2 150 0.8182821 0.9040 0.47947837\r## 2 200 0.8214800 0.8910 0.51491415\r## 2 250 0.8209545 0.8780 0.53262769\r## 2 300 0.8209278 0.8655 0.54569007\r## 2 350 0.8203395 0.8595 0.55967398\r## 2 400 0.8193341 0.8560 0.56620735\r## 2 450 0.8187081 0.8520 0.57274071\r## 2 500 0.8177495 0.8475 0.58300369\r## 2 550 0.8172341 0.8470 0.58206912\r## 2 600 0.8160518 0.8430 0.58579005\r## 2 650 0.8150557 0.8335 0.58580309\r## 2 700 0.8142389 0.8285 0.58765486\r## 2 750 0.8129551 0.8265 0.59045860\r## 2 800 0.8114235 0.8235 0.58859378\r## 2 850 0.8106544 0.8230 0.58951967\r## 2 900 0.8098655 0.8225 0.58952402\r## 2 950 0.8089927 0.8195 0.59231471\r## 2 1000 0.8085903 0.8185 0.59044990\r## 3 50 0.8124471 0.9675 0.23434471\r## 3 100 0.8166505 0.9160 0.43008042\r## 3 150 0.8183159 0.8875 0.50374701\r## 3 200 0.8170682 0.8700 0.52424690\r## 3 250 0.8167332 0.8610 0.54665942\r## 3 300 0.8170788 0.8535 0.55690067\r## 3 350 0.8155400 0.8470 0.57181917\r## 3 400 0.8140688 0.8410 0.57181482\r## 3 450 0.8124477 0.8355 0.57367529\r## 3 500 0.8114718 0.8325 0.57367963\r## 3 550 0.8100906 0.8275 0.57740926\r## 3 600 0.8092354 0.8230 0.58113888\r## 3 650 0.8082420 0.8220 0.58859813\r## 3 700 0.8065477 0.8190 0.58766355\r## 3 750 0.8062263 0.8160 0.58858944\r## 3 800 0.8044913 0.8130 0.58765486\r## 3 850 0.8029262 0.8125 0.58765486\r## 3 900 0.8020108 0.8110 0.58858944\r## 3 950 0.8016405 0.8105 0.59045860\r## 3 1000 0.8011373 0.8090 0.59232775\r## 4 50 0.8139509 0.9640 0.24734188\r## 4 100 0.8150242 0.9120 0.43657900\r## 4 150 0.8179537 0.8850 0.49533145\r## 4 200 0.8176002 0.8710 0.53356227\r## 4 250 0.8162216 0.8615 0.55220170\r## 4 300 0.8159643 0.8490 0.56901543\r## 4 350 0.8141087 0.8445 0.57553141\r## 4 400 0.8131285 0.8430 0.57832645\r## 4 450 0.8114524 0.8385 0.58020430\r## 4 500 0.8104583 0.8335 0.58671158\r## 4 550 0.8089872 0.8265 0.58764182\r## 4 600 0.8078067 0.8240 0.59137144\r## 4 650 0.8064856 0.8230 0.59418387\r## 4 700 0.8051274 0.8200 0.59044121\r## 4 750 0.8038530 0.8205 0.59324495\r## 4 800 0.8030680 0.8205 0.59699196\r## 4 850 0.8023061 0.8190 0.59793088\r## 4 900 0.8011695 0.8185 0.59606607\r## 4 950 0.8000060 0.8160 0.59326668\r## 4 1000 0.7988982 0.8140 0.59045860\r## 5 50 0.8124004 0.9640 0.23430124\r## 5 100 0.8128960 0.9085 0.43658770\r## 5 150 0.8164498 0.8875 0.50560313\r## 5 200 0.8159795 0.8755 0.53263638\r## 5 250 0.8154252 0.8595 0.55782221\r## 5 300 0.8146149 0.8500 0.56622473\r## 5 350 0.8132774 0.8420 0.57460987\r## 5 400 0.8123588 0.8405 0.58207781\r## 5 450 0.8111443 0.8355 0.58207781\r## 5 500 0.8090119 0.8295 0.58767225\r## 5 550 0.8078390 0.8255 0.58488589\r## 5 600 0.8068005 0.8255 0.58954575\r## 5 650 0.8047019 0.8230 0.59045425\r## 5 700 0.8033723 0.8215 0.59231906\r## 5 750 0.8024794 0.8210 0.58951967\r## 5 800 0.8019000 0.8190 0.59232341\r## 5 850 0.8006418 0.8185 0.59512715\r## 5 900 0.7998935 0.8170 0.59605303\r## 5 950 0.7984835 0.8170 0.59698326\r## 5 1000 0.7970453 0.8165 0.59418822\r## ## Tuning parameter \u0026#39;shrinkage\u0026#39; was held constant at a value of 0.01\r## ## Tuning parameter \u0026#39;n.minobsinnode\u0026#39; was held constant at a value of 10\r## ROC was used to select the optimal model using the largest value.\r## The final values used for the model were n.trees = 450,\r## interaction.depth = 1, shrinkage = 0.01 and n.minobsinnode = 10.\rGradient boost model performance is 0.8222721\n\rNaive Bayes model\rmodel_nb \u0026lt;- train(x = diab, y = outcome,\rmetric = \u0026quot;ROC\u0026quot;,\rmethod = \u0026quot;nb\u0026quot;,\rtrControl = myControl\r)\rmodel_nb\r## Naive Bayes ## ## 768 samples\r## 8 predictor\r## 2 classes: \u0026#39;No\u0026#39;, \u0026#39;Yes\u0026#39; ## ## No pre-processing\r## Resampling: Bootstrapped (5 reps) ## Summary of sample sizes: 154, 153, 153, 154, 154 ## Resampling results across tuning parameters:\r## ## usekernel ROC Sens Spec ## FALSE 0.8029047 0.8280 0.5801608\r## TRUE 0.7895701 0.8145 0.5745447\r## ## Tuning parameter \u0026#39;fL\u0026#39; was held constant at a value of 0\r## Tuning\r## parameter \u0026#39;adjust\u0026#39; was held constant at a value of 1\r## ROC was used to select the optimal model using the largest value.\r## The final values used for the model were fL = 0, usekernel = FALSE\r## and adjust = 1.\rNaive Bayes model performance is 0.8029047\nmodels \u0026lt;- c(\u0026quot;glm\u0026quot;, \u0026quot;glmnet\u0026quot;, \u0026quot;rf\u0026quot;, \u0026quot;gbm\u0026quot;, \u0026quot;naive\u0026quot;)\rglm \u0026lt;- max(model_glm$results$ROC)\rglmnet \u0026lt;- max(model_glmnet$results$ROC)\rrf \u0026lt;- max(model_rf$results$ROC)\rgbm \u0026lt;- max(model_gbm$results$ROC)\rnaive \u0026lt;- max(model_nb$results$ROC)\rAUC \u0026lt;- c(glm, glmnet, rf, gbm, naive)\rdf \u0026lt;- data.frame(models, AUC)\rdf\u0026lt;- df[order(df[,2], decreasing=TRUE), ]\rknitr::kable(df)\r\r\r\rmodels\rAUC\r\r\r\r3\rrf\r0.8247223\r\r2\rglmnet\r0.8242214\r\r4\rgbm\r0.8222721\r\r1\rglm\r0.8136093\r\r5\rnaive\r0.8029047\r\r\r\rHere, we found rf model performed the best, and also there are not big differences between the models.\n\rFuture thoughts\rI used different machine learning algorithms to predict Diabetes. Models showed similar performances except the naives bayes which performed worst. As we saw, our simple glm model performance was very close to other more advanced algorithms.\nWe can help doctors to predict Diabetes with accuracy around 83% by using 8 simple medical parameters.\nGiven current speed in generation and collection of types data by including additional predictors we can build even better models.\nUntil next time!\nSerdar\n\r\r","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569913776,"objectID":"d65888227da21884a21306ab2bafa358","permalink":"/r/predict-diseases/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/r/predict-diseases/","section":"post","summary":"Would you be taking care of yourself better if your doctor told today that you have high risk of diabetes?\nAdvances in fields, such as omics and internet of things (sensors that collect data), and centralization of healthcare information (e.g. OMOP common data model) enable us to access much wider data sources.\nGaining insights from those we can improve our well being with better healthcare. Some applications of machine learning tools are;","tags":["Machine learning","Blog","Data Science Process","Linear Regression","Logistic Regression","glmnet","Randomforest","Data Quality control","Feature engineering","Pima Indians"],"title":"Can machine learning change your lifestyle? Predicting Diabetes","type":"post"},{"authors":[],"categories":[],"content":"\rMy ambitious goal is to write a machine learning algorithm that predicts authors. But let’s start with something simpler. An important part in a Data Science workflow is data preparation. Clean it, reformat it and make it usable for further analysis.\nI will work on a Poetry book called “New Poems” from D. H. Lawrence. You can download it from Project Gutenberg website which is a library of over 60,000 free eBooks.\nThe goal is to isolate each poem individually for text mining analysis.\nLet’s figure out a solution.\n I will use the table of contents section to fish out each poem separately by using two for loops.   Install required packages\rlibrary(dplyr)\rlibrary(stringr)\rlibrary(stringi)\r\rCopy the book from DH Lawrence “New Poems”\rAt gutenberg website there are couple of slightly different formats of the book. Since there were some mistakes in the .txt file I used\rthe html version here. I copied the text and pasted it in a text editor and saved to my working directory.\nlawrence \u0026lt;- readLines(\u0026quot;posts_data/lawrence_new_poems.txt\u0026quot;)\rOur file contains 2181 lines. With square brackets [ ] we can view the lines we want. Let’s look at the first few lines;\nlawrence[1:5]\r## [1] \u0026quot;\u0026quot; ## [2] \u0026quot;The Project Gutenberg EBook of New Poems, by D. H. Lawrence\u0026quot; ## [3] \u0026quot;\u0026quot; ## [4] \u0026quot;This eBook is for the use of anyone anywhere at no cost and with\u0026quot; ## [5] \u0026quot;almost no restrictions whatsoever. You may copy it, give it away or\u0026quot;\rThe book has 42 poems in total. Table of contents (TOC) starts with the line “CONTENTS” and ends with the line “ON THAT DAY”.\nI will use those lines to extract the TOC. Stringr package comes in handy here. str_which() function returns line index numbers for a given term.\nstart \u0026lt;- str_which(lawrence, pattern = fixed(\u0026quot;CONTENTS\u0026quot;))\rstart\r## [1] 53\rlawrence[start]\r## [1] \u0026quot;CONTENTS\u0026quot;\r# We are choosing first appearance of \u0026quot;ON THAT DAY\u0026quot; with [1] because it appears # also in the Poem title later.\rend \u0026lt;- str_which(lawrence, pattern = fixed(\u0026quot;ON THAT DAY\u0026quot;))[1]\rend\r## [1] 137\rlawrence[end]\r## [1] \u0026quot;ON THAT DAY\u0026quot;\rSlicing the lines from 54 to 137 will give us the TOC.\nTOC \u0026lt;- lawrence[(start+1):(end)]\rTo remove empty spaces I will use here stri_remove_empty() function from stringi package.\nTOC \u0026lt;- stri_remove_empty(TOC)\rLet’s look at how the clean TOC looks.\nTOC \r## [1] \u0026quot;APPREHENSION\u0026quot; \u0026quot;COMING AWAKE\u0026quot; ## [3] \u0026quot;FROM A COLLEGE WINDOW\u0026quot; \u0026quot;FLAPPER\u0026quot; ## [5] \u0026quot;BIRDCAGE WALK\u0026quot; \u0026quot;LETTER FROM TOWN: THE\u0026quot; ## [7] \u0026quot;FLAT SUBURBS, S.W., IN THE\u0026quot; \u0026quot;THIEF IN THE NIGHT\u0026quot; ## [9] \u0026quot;LETTER FROM TOWN: ON A\u0026quot; \u0026quot;SUBURBS ON A HAZY DAY\u0026quot; ## [11] \u0026quot;HYDE PARK AT NIGHT, BEFORE\u0026quot; \u0026quot;GIPSY\u0026quot; ## [13] \u0026quot;TWO-FOLD\u0026quot; \u0026quot;UNDER THE OAK\u0026quot; ## [15] \u0026quot;SIGH NO MORE\u0026quot; \u0026quot;LOVE STORM\u0026quot; ## [17] \u0026quot;PARLIAMENT HILL IN THE\u0026quot; \u0026quot;PICCADILLY CIRCUS AT NIGHT\u0026quot;\r## [19] \u0026quot;TARANTELLA\u0026quot; \u0026quot;IN CHURCH\u0026quot; ## [21] \u0026quot;PIANO\u0026quot; \u0026quot;EMBANKMENT AT NIGHT,\u0026quot; ## [23] \u0026quot;PHANTASMAGORIA\u0026quot; \u0026quot;NEXT MORNING\u0026quot; ## [25] \u0026quot;PALIMPSEST OF TWILIGHT\u0026quot; \u0026quot;EMBANKMENT AT NIGHT,\u0026quot; ## [27] \u0026quot;WINTER IN THE BOULEVARD\u0026quot; \u0026quot;SCHOOL ON THE OUTSKIRTS\u0026quot; ## [29] \u0026quot;SICKNESS\u0026quot; \u0026quot;EVERLASTING FLOWERS\u0026quot; ## [31] \u0026quot;THE NORTH COUNTRY\u0026quot; \u0026quot;BITTERNESS OF DEATH\u0026quot; ## [33] \u0026quot;SEVEN SEALS\u0026quot; \u0026quot;READING A LETTER\u0026quot; ## [35] \u0026quot;TWENTY YEARS AGO\u0026quot; \u0026quot;INTIME\u0026quot; ## [37] \u0026quot;TWO WIVES\u0026quot; \u0026quot;HEIMWEH\u0026quot; ## [39] \u0026quot;DEBACLE\u0026quot; \u0026quot;NARCISSUS\u0026quot; ## [41] \u0026quot;AUTUMN SUNSHINE\u0026quot; \u0026quot;ON THAT DAY\u0026quot;\rNext, we will extract main text containing only the poems without TOC and other metadata. We need to slice the document starting from the end of the contents (end) till end of the last poem.\n# After the last poem some metadata starts with \u0026quot;End of the Project...\u0026quot;\r# We will slice until this line\rend_main \u0026lt;- str_which(lawrence, \u0026quot;End of the Project Gutenberg EBook of New Poems, by D. H. Lawrence\u0026quot;)\r# Capture main text\rlawrence_body \u0026lt;- lawrence[(end+1):(end_main -1)]\rNow, we have TOC and main body of the book as two separate objects.\n\rFirst for loop\rWe will use TOC and a for loop to get the index numbers of the title’s of each poem.\n# First initiate an empty list\rindex \u0026lt;- list()\r# For loop\rfor (i in 1:42) {\rindex[[i]] \u0026lt;- str_which(lawrence_body, pattern = TOC[i])\r}\rindex\u0026lt;- unlist(index)\rindex\r## [1] 9 37 59 82 110 126 164 192 209 253 276 314 332 347\r## [15] 387 428 473 496 536 570 593 621 768 664 707 745 621 768\r## [29] 901 933 958 990 1057 1100 1193 1253 1286 1313 1376 1502 1527 1571\r## [43] 1606 1644\rThe for loop we created here uses each title in TOC as a pattern inside a str_which() function to find the index number where it detects this pattern.\nFor example TOC[1] will use the title of first poem as a pattern and it will return the line number where the poem starts. At the end, we will have a list of starting lines of each poem.\nTOC[1]\r## [1] \u0026quot;APPREHENSION\u0026quot;\rstr_which(lawrence_body, pattern = TOC[1])\r## [1] 9\r# e.g. The poem Apprehension starts at line index number 9\rSelecting the lines from the beginning of the first poem until the beginning of the second poem will give us the first poem. By iterating everything by +1 we will capture all 42 poems.\nSince the title EMBANKMENT AT NIGHT appears in the titles of two poems we will do a slight correction here. To correct this, I will remove first appearance of index 768 and second appearance of 621.\nindex \u0026lt;- index[-c(23,27)]\rindex\r## [1] 9 37 59 82 110 126 164 192 209 253 276 314 332 347\r## [15] 387 428 473 496 536 570 593 621 664 707 745 768 901 933\r## [29] 958 990 1057 1100 1193 1253 1286 1313 1376 1502 1527 1571 1606 1644\rlength(index)\r## [1] 42\r# Not to miss the last poem, I have to add the line index of the\r# end of the main text. We can use the end of the main body as above.\rindex[43] \u0026lt;- end_main -1\rNow, we have 42 index numbers matching the title of each poem 1 index number to label the end of the main text. We will use those to extract poems separately.\n\rSecond for loop\rIt’s time for the trick. Finally we can capture each 42 poem separately in a list by using a second for loop.\n# Create an empty list: poems\rpoems \u0026lt;- list()\rfor (i in 1:42) {\rpoems[[i]] \u0026lt;- lawrence_body[(index[i]:index[i+1]-1)] }\r# Visualize the first poem\rwriteLines(poems[[1]])\r## ## APPREHENSION\r## AND all hours long, the town\r## Roars like a beast in a cave\r## That is wounded there\r## And like to drown;\r## While days rush, wave after wave\r## On its lair.\r## ## An invisible woe unseals\r## The flood, so it passes beyond\r## All bounds: the great old city\r## Recumbent roars as it feels\r## The foamy paw of the pond\r## Reach from immensity.\r## ## But all that it can do\r## Now, as the tide rises,\r## Is to listen and hear the grim\r## Waves crash like thunder through\r## The splintered streets, hear noises\r## Roll hollow in the interim.\rLet’s check if we got what we wanted.\nstr(poems)\r## List of 42\r## $ : chr [1:29] \u0026quot;\u0026quot; \u0026quot;APPREHENSION\u0026quot; \u0026quot;AND all hours long, the town\u0026quot; \u0026quot; Roars like a beast in a cave\u0026quot; ...\r## $ : chr [1:23] \u0026quot;\u0026quot; \u0026quot;COMING AWAKE\u0026quot; \u0026quot;WHEN I woke, the lake-lights were quivering on the\u0026quot; \u0026quot; wall,\u0026quot; ...\r## $ : chr [1:24] \u0026quot;\u0026quot; \u0026quot;FROM A COLLEGE WINDOW\u0026quot; \u0026quot;THE glimmer of the limes, sun-heavy, sleeping,\u0026quot; \u0026quot; Goes trembling past me up the College wall.\u0026quot; ...\r## $ : chr [1:29] \u0026quot;\u0026quot; \u0026quot;FLAPPER\u0026quot; \u0026quot;LOVE has crept out of her sealéd heart\u0026quot; \u0026quot; As a field-bee, black and amber,\u0026quot; ...\r## $ : chr [1:17] \u0026quot;\u0026quot; \u0026quot;BIRDCAGE WALK\u0026quot; \u0026quot;WHEN the wind blows her veil\u0026quot; \u0026quot; And uncovers her laughter\u0026quot; ...\r## $ : chr [1:39] \u0026quot;\u0026quot; \u0026quot;LETTER FROM TOWN: THE\u0026quot; \u0026quot;ALMOND TREE\u0026quot; \u0026quot;YOU promised to send me some violets. Did you\u0026quot; ...\r## $ : chr [1:29] \u0026quot;\u0026quot; \u0026quot;FLAT SUBURBS, S.W., IN THE\u0026quot; \u0026quot;MORNING\u0026quot; \u0026quot;THE new red houses spring like plants\u0026quot; ...\r## $ : chr [1:18] \u0026quot;\u0026quot; \u0026quot;THIEF IN THE NIGHT\u0026quot; \u0026quot;LAST night a thief came to me\u0026quot; \u0026quot; And struck at me with something dark.\u0026quot; ...\r## $ : chr [1:45] \u0026quot;\u0026quot; \u0026quot;LETTER FROM TOWN: ON A\u0026quot; \u0026quot;GREY EVENING IN MARCH\u0026quot; \u0026quot;THE clouds are pushing in grey reluctance slowly\u0026quot; ...\r## $ : chr [1:24] \u0026quot;\u0026quot; \u0026quot;SUBURBS ON A HAZY DAY\u0026quot; \u0026quot; O STIFFLY shapen houses that change not,\u0026quot; \u0026quot; What conjuror\u0026#39;s cloth was thrown across you,\u0026quot; ...\r## $ : chr [1:39] \u0026quot;\u0026quot; \u0026quot;HYDE PARK AT NIGHT, BEFORE\u0026quot; \u0026quot;THE WAR\u0026quot; \u0026quot; Clerks.\u0026quot; ...\r## $ : chr [1:19] \u0026quot;\u0026quot; \u0026quot;GIPSY\u0026quot; \u0026quot; I, THE man with the red scarf,\u0026quot; \u0026quot; Will give thee what I have, this last week\u0026#39;s earn-\u0026quot; ...\r## $ : chr [1:16] \u0026quot;\u0026quot; \u0026quot;TWO-FOLD\u0026quot; \u0026quot; How gorgeous that shock of red lilies, and larkspur\u0026quot; \u0026quot; cleaving\u0026quot; ...\r## $ : chr [1:41] \u0026quot;\u0026quot; \u0026quot;UNDER THE OAK\u0026quot; \u0026quot; You, if you were sensible,\u0026quot; \u0026quot; When I tell you the stars flash signals, each one\u0026quot; ...\r## $ : chr [1:42] \u0026quot;\u0026quot; \u0026quot;SIGH NO MORE\u0026quot; \u0026quot;THE cuckoo and the coo-dove\u0026#39;s ceaseless calling,\u0026quot; \u0026quot; Calling,\u0026quot; ...\r## $ : chr [1:46] \u0026quot;\u0026quot; \u0026quot;LOVE STORM\u0026quot; \u0026quot;MANY roses in the wind\u0026quot; \u0026quot; Are tapping at the window-sash.\u0026quot; ...\r## $ : chr [1:24] \u0026quot;\u0026quot; \u0026quot;PARLIAMENT HILL IN THE\u0026quot; \u0026quot;EVENING\u0026quot; \u0026quot;THE houses fade in a melt of mist\u0026quot; ...\r## $ : chr [1:41] \u0026quot;\u0026quot; \u0026quot;PICCADILLY CIRCUS AT NIGHT\u0026quot; \u0026quot; Street-Walkers.\u0026quot; \u0026quot;WHEN into the night the yellow light is roused like\u0026quot; ...\r## $ : chr [1:35] \u0026quot;\u0026quot; \u0026quot;TARANTELLA\u0026quot; \u0026quot;SAD as he sits on the white sea-stone\u0026quot; \u0026quot; And the suave sea chuckles, and turns to the moon,\u0026quot; ...\r## $ : chr [1:24] \u0026quot;\u0026quot; \u0026quot;IN CHURCH\u0026quot; \u0026quot;IN the choir the boys are singing the hymn.\u0026quot; \u0026quot; The morning light on their lips\u0026quot; ...\r## $ : chr [1:29] \u0026quot;\u0026quot; \u0026quot;PIANO\u0026quot; \u0026quot; Softly, in the dusk, a woman is singing to me;\u0026quot; \u0026quot; Taking me back down the vista of years, till I see\u0026quot; ...\r## $ : chr [1:44] \u0026quot;\u0026quot; \u0026quot;EMBANKMENT AT NIGHT,\u0026quot; \u0026quot;BEFORE THE WAR\u0026quot; \u0026quot; Charity.\u0026quot; ...\r## $ : chr [1:44] \u0026quot;\u0026quot; \u0026quot;PHANTASMAGORIA\u0026quot; \u0026quot;RIGID sleeps the house in darkness, I alone\u0026quot; \u0026quot; Like a thing unwarrantable cross the hall\u0026quot; ...\r## $ : chr [1:39] \u0026quot;\u0026quot; \u0026quot;NEXT MORNING\u0026quot; \u0026quot; How have I wandered here to this vaulted room\u0026quot; \u0026quot; In the house of life?—the floor was ruffled with gold\u0026quot; ...\r## $ : chr [1:24] \u0026quot;\u0026quot; \u0026quot;PALIMPSEST OF TWILIGHT\u0026quot; \u0026quot;DARKNESS comes out of the earth\u0026quot; \u0026quot; And swallows dip into the pallor of the west;\u0026quot; ...\r## $ : chr [1:134] \u0026quot;\u0026quot; \u0026quot;EMBANKMENT AT NIGHT,\u0026quot; \u0026quot;BEFORE THE WAR\u0026quot; \u0026quot; Outcasts.\u0026quot; ...\r## $ : chr [1:33] \u0026quot;\u0026quot; \u0026quot;WINTER IN THE BOULEVARD\u0026quot; \u0026quot;THE frost has settled down upon the trees\u0026quot; \u0026quot; And ruthlessly strangled off the fantasies\u0026quot; ...\r## $ : chr [1:26] \u0026quot;\u0026quot; \u0026quot;SCHOOL ON THE OUTSKIRTS\u0026quot; \u0026quot; How different, in the middle of snows, the great\u0026quot; \u0026quot; school rises red!\u0026quot; ...\r## $ : chr [1:33] \u0026quot;\u0026quot; \u0026quot;SICKNESS\u0026quot; \u0026quot;WAVING slowly before me, pushed into the dark,\u0026quot; \u0026quot; Unseen my hands explore the silence, drawing the\u0026quot; ...\r## $ : chr [1:68] \u0026quot;\u0026quot; \u0026quot;EVERLASTING FLOWERS\u0026quot; \u0026quot;WHO do you think stands watching\u0026quot; \u0026quot; The snow-tops shining rosy\u0026quot; ...\r## $ : chr [1:44] \u0026quot;\u0026quot; \u0026quot;THE NORTH COUNTRY\u0026quot; \u0026quot;IN another country, black poplars shake them-\u0026quot; \u0026quot; selves over a pond,\u0026quot; ...\r## $ : chr [1:94] \u0026quot;\u0026quot; \u0026quot;BITTERNESS OF DEATH\u0026quot; \u0026quot; I\u0026quot; \u0026quot;AH, stern, cold man,\u0026quot; ...\r## $ : chr [1:61] \u0026quot;\u0026quot; \u0026quot;SEVEN SEALS\u0026quot; \u0026quot;SINCE this is the last night I keep you home,\u0026quot; \u0026quot; Come, I will consecrate you for the journey.\u0026quot; ...\r## $ : chr [1:34] \u0026quot;\u0026quot; \u0026quot;READING A LETTER\u0026quot; \u0026quot;SHE sits on the recreation ground\u0026quot; \u0026quot; Under an oak whose yellow buds dot the pale\u0026quot; ...\r## $ : chr [1:28] \u0026quot;\u0026quot; \u0026quot;TWENTY YEARS AGO\u0026quot; \u0026quot;ROUND the house were lilacs and strawberries\u0026quot; \u0026quot; And foal-foots spangling the paths,\u0026quot; ...\r## $ : chr [1:64] \u0026quot;\u0026quot; \u0026quot;INTIME\u0026quot; \u0026quot;RETURNING, I find her just the same,\u0026quot; \u0026quot; At just the same old delicate game.\u0026quot; ...\r## $ : chr [1:127] \u0026quot;\u0026quot; \u0026quot;TWO WIVES\u0026quot; \u0026quot; I\u0026quot; \u0026quot;INTO the shadow-white chamber silts the white\u0026quot; ...\r## $ : chr [1:26] \u0026quot;\u0026quot; \u0026quot;HEIMWEH\u0026quot; \u0026quot;FAR-OFF the lily-statues stand white-ranked in the\u0026quot; \u0026quot; garden at home.\u0026quot; ...\r## $ : chr [1:45] \u0026quot;\u0026quot; \u0026quot;DEBACLE\u0026quot; \u0026quot;THE trees in trouble because of autumn,\u0026quot; \u0026quot; And scarlet berries falling from the bush,\u0026quot; ...\r## $ : chr [1:36] \u0026quot;\u0026quot; \u0026quot;NARCISSUS\u0026quot; \u0026quot;WHERE the minnows trace\u0026quot; \u0026quot; A glinting web quick hid in the gloom of the brook,\u0026quot; ...\r## $ : chr [1:39] \u0026quot;\u0026quot; \u0026quot;AUTUMN SUNSHINE\u0026quot; \u0026quot;THE sun sets out the autumn crocuses\u0026quot; \u0026quot; And fills them up a pouring measure\u0026quot; ...\r## $ : chr [1:173] \u0026quot;\u0026quot; \u0026quot;ON THAT DAY\u0026quot; \u0026quot; ON that day\u0026quot; \u0026quot; I shall put roses on roses, and cover your grave\u0026quot; ...\r\rFinal Thoughts\rData Preparation is a crucial step in Data Science as data comes rarely ready to use.\nHere, starting from a Poetry Book I isolated each poem separately in a list. Hard part is done. Now, I can identify how many rhymes each poem contains, word usage across different poems, the similarities between them and many more to gain insights about the author.\nI could also analyze the whole book as a single document but by isolating each element I will gain much deeper insight from the data.\nDo you apply similar techniques to isolate chapters or sections from the book or documents to compare and contrast different parts?\nThank you for reading this post. Please feel free to comment below with your thoughts/feedback.\n\r","date":1569369600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569398567,"objectID":"e756c1ea7185e60e047eadfed311b26f","permalink":"/r/data-wrangling-text-mining/","publishdate":"2019-09-25T00:00:00Z","relpermalink":"/r/data-wrangling-text-mining/","section":"post","summary":"My ambitious goal is to write a machine learning algorithm that predicts authors. But let’s start with something simpler. An important part in a Data Science workflow is data preparation. Clean it, reformat it and make it usable for further analysis.\nI will work on a Poetry book called “New Poems” from D. H. Lawrence. You can download it from Project Gutenberg website which is a library of over 60,000 free eBooks.","tags":["RStudio","R Markdown","Text mining","Stringr","Data preparation","For loop"],"title":"Data Wrangling for Text mining: Extract individual elements from a Book","type":"post"},{"authors":[],"categories":[],"content":"\rAdvances in microfluidic technologies enabled us to barcode single cells in lipid droplets and to resolve genomes of individual cells from a sequencing mixture (e.g, 10X Genomics). By using Single cell RNA sequencing (scRNA-seq) we can discover rare cell populations and genes that are specifically acting in those. Potential is high and the list of publications growing daily.\n\rIf you are a scientist in a biotech exploring novel targets those might be a great source to gather specific information.\r\rSeurat package is a great tool for digging into single cell datasets. It will open you access beyond what is in the publications. You can find many tutorials in their website.\nHere, I will focus on a recent paper which explored transcriptome of lung cells from Pulmonary fibrosis patients by scRNAseq.\n\rPulmonary fibrosis is a progressive scarring of the lung tissue leading to death within 3-4 years.\rCurrent therapies do not increase the survival\rMany Biotech companies are developing novel drugs\r\r What types of cells those drugs are acting on?   Our workflow will be in three steps\nBasic Seurat workflow\rIdentify cell types on a tSNE plot\rVisualize drug targets on single cell plots\r\rI picked up scRNA-seq data from a patient with Polymyositis associated interstitial lung disease.\n—\u0026gt; Download Single Cell RNA seq data\nSeurat Workflow\rLoad the packages that will be used.\nlibrary(Seurat)\rlibrary(dplyr)\rlibrary(ggplot2)\r\rImporting the h5 file\rHere the data comes as an h5 file. But no worries, we can handle that with Read10X_h5 function in Seurat. Since the file is directly under my working directory I can apply Read10X_h5(\"Filename.h5\")\nMyoILD_01 \u0026lt;- Read10X_h5(\u0026quot;posts_data/GSM3489196_Myositis-ILD_01_filtered_gene_bc_matrices_h5.h5\u0026quot;)\rFib \u0026lt;- CreateSeuratObject(counts = MyoILD_01, min.cells = 3, project= \u0026quot;FightFibrosis\u0026quot;, min.features = 200)\rFib\r## An object of class Seurat ## 20246 features across 7163 samples within 1 assay ## Active assay: RNA (20246 features)\rOur Data matrix now contains 20246 genes and 7163 cells\n\rA Quick look at the raw count data\r# Lets examine a few genes in the first thirty cells\rMyoILD_01[c(\u0026quot;SPP1\u0026quot;,\u0026quot;TGFB1\u0026quot;,\u0026quot;CCL2\u0026quot;), 1:30]\r## 3 x 30 sparse Matrix of class \u0026quot;dgCMatrix\u0026quot;\r## ## SPP1 1 . . 2 . . . 33 . . 1 . . . . . . . . . . . . . . . 9 . . 20\r## TGFB1 . . . . 1 . . . . . . . . . 1 . 3 . . . . . . . 1 2 . . . .\r## CCL2 5 . . 2 . . . . . . . . . . . . . . . . . . . . . . . . . .\r\rStandard pre-processing workflow for scRNA-seq data\rPre-processing involves filtration of cells based on Quality control metrics (e.g .mitochondrial contamination, Coverage). We will proceed by normalization and identification of the highly variable features then at the end we will scale the data.\nQC and selecting cells for further analysis\r# Let\u0026#39;s add Mitochondrial stats to Fib data. [[ operator can add # columns to object metadata. Fib[[\u0026quot;percent.mt\u0026quot;]] \u0026lt;- PercentageFeatureSet(object = Fib, pattern = \u0026quot;^MT-\u0026quot;)\r# Quick look at the Quality control metrics for the first 5 cells\rhead(x = Fib@meta.data, 5)\r## orig.ident nCount_RNA nFeature_RNA percent.mt\r## AAACCTGAGATCCCGC-1 FightFibrosis 2257 1010 4.386354\r## AAACCTGAGCAGCCTC-1 FightFibrosis 3367 713 66.914167\r## AAACCTGAGCCAGAAC-1 FightFibrosis 7354 1943 3.929834\r## AAACCTGAGGGATGGG-1 FightFibrosis 9323 2786 8.430763\r## AAACCTGAGTCGTACT-1 FightFibrosis 2937 1125 6.775621\r\rViolin plots to visualize QC metrics\r#Visualize QC metrics as a violin plot\rVlnPlot(object = Fib, features = c(\u0026quot;nFeature_RNA\u0026quot;, \u0026quot;nCount_RNA\u0026quot;, \u0026quot;percent.mt\u0026quot;),\rncol = 3)\r# We will use FeatureScatter to visualize feature-feature relationships like # RNA counts or percentage of mitochondiral contamination\rplot1 \u0026lt;- FeatureScatter(object = Fib, feature1 = \u0026quot;nCount_RNA\u0026quot;, feature2 = \u0026quot;percent.mt\u0026quot;) plot2 \u0026lt;- FeatureScatter(object = Fib, feature1 = \u0026quot;nCount_RNA\u0026quot;, feature2 = \u0026quot;nFeature_RNA\u0026quot;) CombinePlots(plots = list(plot1,plot2))\r# As you see on the left plot cells with high percentage of mitochondrial genes\r# have very low numbers of RNA indicating that they are low quality/dead cells.\r# Let\u0026#39;s remove them.\rFib \u0026lt;- subset(x = Fib, subset = nFeature_RNA \u0026gt; 200 \u0026amp; nFeature_RNA \u0026lt; 4000 \u0026amp; percent.mt \u0026lt; 12.5)\r\rNormalize the data\rLog normalization helps to reduce the influences of the outliers.\rNormalized values will be stored in Fib[[\"RNA\"]]@data.\nFib \u0026lt;- NormalizeData(object = Fib, normalization.method = \u0026quot;LogNormalize\u0026quot;, scale.factor = 1e4)\r\rIdentify highly variable features\rLet’s calculate the features that exhibit high cell-to-cell variation in the dataset. Focusing on those genes will help to highlight biological signal.\nFib \u0026lt;- FindVariableFeatures(object = Fib, selection.method = \u0026#39;vst\u0026#39;, nfeatures = 2000)\r# Identify the 10 most highly variable genes\rtop10 \u0026lt;- head(x = VariableFeatures(object = Fib), 10)\rtop10\r## [1] \u0026quot;SCGB1A1\u0026quot; \u0026quot;SCGB3A1\u0026quot; \u0026quot;SFTPC\u0026quot; \u0026quot;SCGB3A2\u0026quot; \u0026quot;HBB\u0026quot; \u0026quot;PLA2G2A\u0026quot; \u0026quot;MT1G\u0026quot; ## [8] \u0026quot;TPSB2\u0026quot; \u0026quot;FABP4\u0026quot; \u0026quot;MT1H\u0026quot;\r# Plot variable features with and without labels\rplot1 \u0026lt;- VariableFeaturePlot(object = Fib)\rplot2 \u0026lt;- LabelPoints(plot = plot1, points = top10, repel = TRUE)\rCombinePlots(plots = list(plot1, plot2))\r\rScale the data\rNext, apply a linear transformation that is a standard pre-processing step prior to dimensional reduction techniques like PCA. The ScaleData function:\nHighly-expressed genes might dominate downstream analyses since their expression range is much higher than most other genes. In order to prevent this we need to;\n\rshift the expression of each gene, so that the mean expression across cells is 0\rscale the expression of each gene, so that the variance across cells is;\r\rApply the ScaleData function\nall.genes \u0026lt;- rownames(x = Fib)\rFib \u0026lt;- ScaleData(object = Fib, features = all.genes)\r# Another factor that can influence downstream analyses is cell cycle status, # cells that arenormally cycling but in different phases of cell cycle # might appear as separate populations.\r# Seurat is preloaded with list of cell cycle markers, from Tirosh et al. 2015, # segregate this list into markers of G2/M phase and markers of S phase\rs.genes \u0026lt;- cc.genes$s.genes\rg2m.genes \u0026lt;- cc.genes$g2m.genes\rFib \u0026lt;- CellCycleScoring(Fib, s.features = s.genes, g2m.features = g2m.genes, set.ident = TRUE)\r# We can view cell cycle scores and phase assignments with\rhead(Fib[[]])\r## orig.ident nCount_RNA nFeature_RNA percent.mt\r## AAACCTGAGATCCCGC-1 FightFibrosis 2257 1010 4.386354\r## AAACCTGAGCCAGAAC-1 FightFibrosis 7354 1943 3.929834\r## AAACCTGAGGGATGGG-1 FightFibrosis 9323 2786 8.430763\r## AAACCTGAGTCGTACT-1 FightFibrosis 2937 1125 6.775621\r## AAACCTGAGTCGTTTG-1 FightFibrosis 5447 1719 4.149073\r## AAACCTGCAAGTCTAC-1 FightFibrosis 6453 1836 2.773904\r## S.Score G2M.Score Phase old.ident\r## AAACCTGAGATCCCGC-1 0.006739441 -0.016659228 S FightFibrosis\r## AAACCTGAGCCAGAAC-1 -0.040762255 -0.010151464 G1 FightFibrosis\r## AAACCTGAGGGATGGG-1 -0.048223904 -0.025580453 G1 FightFibrosis\r## AAACCTGAGTCGTACT-1 0.076114887 0.009717348 S FightFibrosis\r## AAACCTGAGTCGTTTG-1 -0.045171005 -0.053868788 G1 FightFibrosis\r## AAACCTGCAAGTCTAC-1 0.003354484 -0.009033119 S FightFibrosis\r# Regress out cell cycle scores during data scaling\rFib \u0026lt;- ScaleData(Fib, vars.to.regress = c(\u0026quot;S.Score\u0026quot;, \u0026quot;G2M.Score\u0026quot;), features =rownames(Fib))\r\r\rPerform linear dimensional reduction\rEach gene creates another dimension in our dataset, but most of those do not play a role in differentiating subgroups of cells. Principal components analyses (PCA) helps us by reducing the dimensions of our data into components which explains most of the variation.\nFib \u0026lt;- RunPCA(object = Fib, features = VariableFeatures(object = Fib), ndims.print = 10, nfeatures.print = 10)\r## PC_ 10 ## Positive: VCAN, MCEMP1, S100A8, FCN1, CD2, IL32, S100A6, TIMP1, CCL5, CD3D ## Negative: CHIT1, A2M, CHCHD6, AC079767.4, SEPP1, ALOX15B, GPNMB, DNASE2B, LIPA, SLC40A1\rVisualize the PCA results\r# Examine and visualize PCA results a few different ways\rprint(x = Fib[[\u0026#39;pca\u0026#39;]], dims = 1:5, nfeatures = 5)\r## PC_ 1 ## Positive: TMSB4X, TYROBP, FTL, FCER1G, VIM ## Negative: RSPH1, C9orf24, TMEM190, C20orf85, FAM183A ## PC_ 2 ## Positive: FTH1, CTSS, TYROBP, B2M, FCER1G ## Negative: GPRC5A, SFTPB, CEACAM6, HOPX, MUC1 ## PC_ 3 ## Positive: PLA2G2A, GFPT2, MEDAG, COL3A1, COL1A2 ## Negative: SFTPB, SFTPA2, SFTA2, SFTPA1, SLC34A2 ## PC_ 4 ## Positive: SFTPC, NAPSA, SFTPD, LRRK2, LAMP3 ## Negative: S100A2, KRT19, KRT5, AKR1C1, BPIFB1 ## PC_ 5 ## Positive: CPA3, TPSAB1, TPSB2, MS4A2, GATA2 ## Negative: S100A6, CSTB, S100A10, GCHFR, TXN\rVizDimLoadings(object = Fib, dims = 1:2, reduction = \u0026#39;pca\u0026#39;)\rDimPlot(object = Fib, reduction = \u0026#39;pca\u0026#39;)\rDimHeatmap(object = Fib, dims = 1:9, cells = 500, balanced = TRUE)\r\r\rWhat is the ‘dimensionality’ of the data?\rFor clustering analyses we will choose the principal components which explain most of the variation in our data. Elbow plot uses a ranking of principle components based on the percentage explained by each one.\nElbowPlot(object = Fib)\rWe did not get a clear elbow shape here, but after 10th principal component additional dimensions do not explain big amount of the variance. So we will use first 10 dimensions for the subsequent analyses.\n\rCluster the cells\rA graph-based clustering approach will be performed, built upon initial strategies in (Macosko et al).\nFib \u0026lt;- FindNeighbors(object = Fib, dims = 1:10)\rFib \u0026lt;- FindClusters(object = Fib, resolution = 0.5)\r## Modularity Optimizer version 1.3.0 by Ludo Waltman and Nees Jan van Eck\r## ## Number of nodes: 6320\r## Number of edges: 196713\r## ## Running Louvain algorithm...\r## Maximum modularity in 10 random starts: 0.8872\r## Number of communities: 15\r## Elapsed time: 0 seconds\r# Look at cluster IDs of the first 5 cells\rhead(Idents(Fib), 5)\r## AAACCTGAGATCCCGC-1 AAACCTGAGCCAGAAC-1 AAACCTGAGGGATGGG-1 ## 2 2 8 ## AAACCTGAGTCGTACT-1 AAACCTGAGTCGTTTG-1 ## 0 10 ## Levels: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14\r\rTSNE plot\rFib \u0026lt;- RunTSNE(object = Fib, dims = 1:6)\rDimPlot(object = Fib, reduction = \u0026#39;tsne\u0026#39;, label = TRUE, label.size = 5)\r\rHow to find genes that differentiate each cluster?\rWe can use FindMarkers() function to search for cluster biomarkers.\n# Find all markers of cluster 0\rcluster0.markers \u0026lt;- FindMarkers(object = Fib, ident.1 = 0, min.pct = 0.25)\rhead(x = cluster0.markers, n = 10)\r## p_val avg_logFC pct.1 pct.2 p_val_adj\r## GPNMB 0 1.0814399 0.947 0.450 0\r## CSTB 0 1.0377160 0.994 0.892 0\r## FTL 0 1.0160962 1.000 0.998 0\r## APOC1 0 0.9919471 0.991 0.819 0\r## SH3BGRL3 0 0.9252345 0.990 0.798 0\r## CTSD 0 0.9105122 0.995 0.836 0\r## FTH1 0 0.8873914 1.000 0.997 0\r## PSAP 0 0.8835586 1.000 0.876 0\r## CTSB 0 0.8773872 0.986 0.752 0\r## LGALS1 0 0.8622147 0.992 0.637 0\r# Find markers for every cluster compared to all remaining cells, report only # the positive ones\rFib.markers \u0026lt;- FindAllMarkers(object = Fib, only.pos = TRUE, min.pct = 0.25, logfc.threshold = 0.25)\r# Make a table containing markers for each cluster set. # We will use this table to assign cell types to clusters.\rFib.markers %\u0026gt;% group_by(cluster) %\u0026gt;% top_n(n = 5, wt = avg_logFC) %\u0026gt;% print(n = 85)\r## # A tibble: 75 x 7\r## # Groups: cluster [15]\r## p_val avg_logFC pct.1 pct.2 p_val_adj cluster gene ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; ## 1 0. 1.08 0.947 0.45 0. 0 GPNMB ## 2 2.60e-293 1.13 0.935 0.582 5.27e-289 0 FABP5 ## 3 8.54e-289 1.12 0.814 0.365 1.73e-284 0 CTSL ## 4 3.43e-188 1.43 0.46 0.133 6.95e-184 0 SPP1 ## 5 2.78e-110 1.37 0.288 0.078 5.64e-106 0 CCL2 ## 6 2.69e-231 1.33 0.996 0.658 5.45e-227 1 APOE ## 7 1.91e-204 1.02 0.926 0.403 3.86e-200 1 MS4A6A ## 8 1.35e-174 0.918 0.932 0.423 2.74e-170 1 C1QC ## 9 1.59e-119 1.07 0.86 0.457 3.22e-115 1 CCL18 ## 10 1.56e- 56 0.916 0.576 0.311 3.15e- 52 1 SEPP1 ## 11 0. 2.26 0.663 0.108 0. 2 FABP4 ## 12 0. 1.43 0.846 0.165 0. 2 MCEMP1 ## 13 5.19e-222 1.24 0.969 0.496 1.05e-217 2 GCHFR ## 14 4.41e-218 1.36 0.98 0.505 8.93e-214 2 C1QA ## 15 4.43e-210 1.35 0.967 0.483 8.97e-206 2 C1QB ## 16 0. 3.18 0.964 0.339 0. 3 SCGB3A2 ## 17 0. 2.01 0.995 0.376 0. 3 SFTPB ## 18 0. 1.73 0.943 0.188 0. 3 CEACAM6 ## 19 2.27e-273 2.23 0.929 0.364 4.59e-269 3 SFTPA2 ## 20 8.38e-271 1.72 0.813 0.203 1.70e-266 3 SFTPA1 ## 21 0. 2.70 0.896 0.035 0. 4 C9orf24 ## 22 0. 2.46 0.87 0.026 0. 4 TMEM190 ## 23 0. 2.44 0.892 0.026 0. 4 RSPH1 ## 24 0. 2.42 0.868 0.025 0. 4 C20orf85\r## 25 0. 2.39 0.907 0.104 0. 4 TPPP3 ## 26 0. 2.76 0.975 0.227 0. 5 S100A2 ## 27 0. 2.19 0.797 0.083 0. 5 KRT15 ## 28 0. 1.72 0.684 0.048 0. 5 KRT5 ## 29 6.72e-278 1.93 0.471 0.035 1.36e-273 5 MMP1 ## 30 9.96e-220 1.75 0.925 0.319 2.02e-215 5 AQP3 ## 31 0. 2.45 0.737 0.077 0. 6 BPIFB1 ## 32 6.07e-262 2.14 0.984 0.305 1.23e-257 6 WFDC2 ## 33 2.03e-221 2.21 0.982 0.385 4.11e-217 6 SLPI ## 34 1.17e-163 3.21 0.948 0.624 2.36e-159 6 SCGB3A1 ## 35 8.07e-152 2.91 0.797 0.293 1.63e-147 6 SCGB1A1 ## 36 0. 2.57 0.797 0.072 0. 7 S100B ## 37 9.62e-289 1.76 0.829 0.156 1.95e-284 7 FAM26F ## 38 2.50e-224 2.18 1 0.728 5.06e-220 7 HLA-DPB1\r## 39 2.51e-213 1.99 0.992 0.658 5.07e-209 7 HLA-DPA1\r## 40 1.75e-191 1.71 0.955 0.449 3.55e-187 7 HLA-DQA1\r## 41 4.11e- 50 1.01 0.546 0.231 8.31e- 46 8 STMN1 ## 42 1.29e- 41 0.917 0.819 0.529 2.60e- 37 8 TUBA1B ## 43 1.29e- 40 0.915 0.807 0.517 2.61e- 36 8 TUBB ## 44 1.32e- 29 0.973 0.825 0.615 2.67e- 25 8 H2AFZ ## 45 5.14e- 29 1.16 0.77 0.503 1.04e- 24 8 HIST1H4C\r## 46 0. 2.28 0.984 0.113 0. 9 NAPSA ## 47 3.65e-299 2.20 0.984 0.159 7.39e-295 9 SFTA2 ## 48 8.60e-252 2.99 1 0.226 1.74e-247 9 SFTPA1 ## 49 3.66e-220 4.21 0.961 0.265 7.42e-216 9 SFTPC ## 50 4.19e-168 2.33 0.992 0.39 8.47e-164 9 SFTPA2 ## 51 0. 2.61 0.570 0.01 0. 10 COL3A1 ## 52 0. 2.56 0.604 0.005 0. 10 SPARCL1 ## 53 0. 2.42 0.456 0.005 0. 10 PLA2G2A ## 54 0. 2.40 0.544 0.005 0. 10 COL1A2 ## 55 1.09e-278 2.57 0.537 0.02 2.20e-274 10 COL1A1 ## 56 0. 4.23 0.993 0.035 0. 11 TPSB2 ## 57 0. 3.74 0.993 0.02 0. 11 TPSAB1 ## 58 0. 3.32 1 0.02 0. 11 CPA3 ## 59 0. 2.78 0.911 0.017 0. 11 MS4A2 ## 60 0. 2.33 0.856 0.016 0. 11 KIT ## 61 1.98e-262 2.08 0.624 0.023 4.01e-258 12 FCN1 ## 62 5.17e- 70 1.95 0.475 0.059 1.05e- 65 12 IL1R2 ## 63 2.47e- 66 1.98 0.634 0.119 5.01e- 62 12 VCAN ## 64 4.37e- 58 3.03 0.812 0.258 8.85e- 54 12 S100A8 ## 65 2.45e- 46 2.67 0.901 0.466 4.96e- 42 12 S100A9 ## 66 2.37e- 68 2.24 0.593 0.055 4.80e- 64 13 IL32 ## 67 1.03e- 66 2.00 0.296 0.013 2.08e- 62 13 KLRB1 ## 68 4.82e- 52 3.79 0.259 0.013 9.77e- 48 13 IGHG1 ## 69 3.98e- 44 3.37 0.259 0.015 8.06e- 40 13 IGHG4 ## 70 3.42e- 42 2.22 0.352 0.03 6.92e- 38 13 CCL5 ## 71 6.32e-208 1.85 0.978 0.036 1.28e-203 14 CPA3 ## 72 1.19e-176 1.28 0.822 0.03 2.41e-172 14 KIT ## 73 3.11e-176 1.40 0.844 0.032 6.30e-172 14 MS4A2 ## 74 7.63e-172 1.75 0.889 0.036 1.54e-167 14 TPSAB1 ## 75 2.72e-158 2.05 0.978 0.051 5.50e-154 14 TPSB2\rFor more details consult to DE vignette.\nAccording to the table above I generated a cluster id vector to assign cell type to each cluster. I have a group NA which I could not assign a cell type, which is probably a technical artifact from cell cycle regression since the top genes expressed in this population are cell cycle related. I also found 3 subpopulations of macrophages and 2 types of mast cells in thiS patient.\n# Assigning Cell ids\rnew.cluster.ids \u0026lt;- c(\u0026quot;Macrophages(1)\u0026quot;, \u0026quot;Macrophages(2)\u0026quot;, \u0026quot;Macrophages(3)\u0026quot;,\r\u0026quot;AT2(1) Cells\u0026quot;, \u0026quot;Cliated Cells\u0026quot;, \u0026quot;Basal Cells\u0026quot;, \u0026quot;Club Cells\u0026quot;, \u0026quot;Dendritic Cells\u0026quot;, \u0026quot;NA\u0026quot;, \u0026quot;AT2(2) Cells\u0026quot;, \u0026quot;Fibroblasts\u0026quot;, \u0026quot;Mast(1) Cells\u0026quot;, \u0026quot;Monocytes\u0026quot;, \u0026quot;T/NKT Cells\u0026quot;, \u0026quot;Mast(2) Cells\u0026quot;)\rnames(new.cluster.ids) \u0026lt;- levels(Fib)\rFib \u0026lt;- RenameIdents(Fib, new.cluster.ids)\rDimPlot(Fib, reduction = \u0026quot;tsne\u0026quot;, label = TRUE, label.size = 8, pt.size = 1.0) + NoLegend()\r# Let\u0026#39;s look at the expression of a few genes of interest accross different subtypes\rVlnPlot(Fib, features = c(\u0026quot;SPP1\u0026quot;, \u0026quot;CHIT1\u0026quot;))\rFeaturePlot(Fib, features = c(\u0026quot;SPP1\u0026quot;, \u0026quot;APOE\u0026quot;, \u0026quot;SCGB3A2\u0026quot;, \u0026quot;C9orf24\u0026quot;, \u0026quot;S100A2\u0026quot;, \u0026quot;BPIFB1\u0026quot;, \u0026quot;S100B\u0026quot;, \u0026quot;COL3A1\u0026quot;, \u0026quot;FCN1\u0026quot;))\r\rWhat cells Biotech Companies are targeting in lung fibrosis?\rNovel therapies in pulmonary fibrosis are urgently needed. Let’s look at few promising drugs currently in clinical trials.\nFibrinogen\rFibrinogen’s lead compound pamrevlumab blocks CTGF. Let’s have a look which cells produce it.\nVlnPlot(Fib, features =c(\u0026quot;CTGF\u0026quot;))\rFeaturePlot(Fib, features = c(\u0026quot;CTGF\u0026quot;), pt.size = 1)\rMain source for the CTGF production here seems the Ciliated cells.\n\rGalapagos\rGalapagos GLPG1690 has entered Phase 3 clinical trials and it targets Autotaxin (ENPP2).\nVlnPlot(Fib, features = c(\u0026quot;ENPP2\u0026quot;))\rFeaturePlot(Fib, features = c(\u0026quot;ENPP2\u0026quot;), pt.size = 1)\rWe see a low expression of ENPP mainly in Macrophages, Dendritic cells and Fibroblasts.\nThe other clinical candidate of Galapagos is GLPG1205 which targets GPR84. It has entered Phase 2 clinical trials.\nVlnPlot(Fib, features = c(\u0026quot;GPR84\u0026quot;))\rFeaturePlot(Fib, features = c(\u0026quot;GPR84\u0026quot;), pt.size = 1)\rGPR84 has very low expression throughout the lung cells of this patient. The gene is mainly expressed in Type 1 Macrophages.\n\rBristol-Myers-Squibb: BMS-986020\rBMS-986020 is an anti-fibrotic drug being developed by Bristol-Myers Squibb, and is a lysophosphatidic acid (LPA) receptor antagonist.\nLet’s look at the expression of a LPAR1.\nVlnPlot(Fib, features = c(\u0026quot;LPAR1\u0026quot;))\rFeaturePlot(Fib, features =c(\u0026quot;LPAR1\u0026quot;), pt.size = 1)\rIt is expressed at low levels but consistently on many types of lung cells.\n\rRoche nintedanib\rNintedanib is one of the two new medicines approved for IPF patients. It targets multiple receptor tyrosine kinase receptors such as VEGF, FGF and PDGF.\nVlnPlot(Fib, features = c(\u0026quot;VEGFA\u0026quot;, \u0026quot;PDGFA\u0026quot;))\rFeaturePlot(Fib, features = c(\u0026quot;VEGFA\u0026quot;, \u0026quot;PDGFA\u0026quot;), pt.size = 1)\rVEGFA, PDGFA is highly expressed in multiple types of cells in the lung microenvironment.\n\r\rFinal thoughts\rSeurat package allowed us to cluster different types of lung cells so that we can visualize their specific gene expression. We used those plots to see which cells express the drug targets in clinical trials or in use.\nAlthough changes associated with fibrosis are well defined the causal factors are not known. Reflecting this we saw that those drugs target many different components of the lung microenvironment. The conquest is on and we hope promising therapies arrive soon.\n\r","date":1568678400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568748206,"objectID":"96b33a2cf8c08c5e0be037d11cb626a3","permalink":"/r/genomics-at-superresolution/","publishdate":"2019-09-17T00:00:00Z","relpermalink":"/r/genomics-at-superresolution/","section":"post","summary":"Advances in microfluidic technologies enabled us to barcode single cells in lipid droplets and to resolve genomes of individual cells from a sequencing mixture (e.g, 10X Genomics). By using Single cell RNA sequencing (scRNA-seq) we can discover rare cell populations and genes that are specifically acting in those. Potential is high and the list of publications growing daily.\n\rIf you are a scientist in a biotech exploring novel targets those might be a great source to gather specific information.","tags":["scRNA-seq","Seurat","h5 file","R","R Markdown","Clustering","PCA","tSNE","IPF"],"title":"Genomics at superresolution: Mapping Drug targets on single cell resolution in Fibrosis","type":"post"},{"authors":[],"categories":[],"content":"\rThis guide will help you to get your website online in a few minutes. Then, customize and add your own material in RStudio environment, push it to your Github repository and benefit from the continuous deployment feature of Netlify.\rIt took me many days of work, reading tens of blog posts, YouTube videos and a lot of testing to figure out all of this.\nHere is an up to date workflow of how I created my Blog on Github and deployed at Netlify.\nTools we need;\r\rRStudio\rHugo\rBlogdown\rGit\rGithub\rNetlify\r\rHugo is the actual website builder and blogdown is an R package that allows us to use Hugo in R environment.\n\rOn RStudio: Build your website locally.\r\rSelect File menu\n\rSelect New Project -\u0026gt; New Directory -\u0026gt; Website using blogdown\r\rFor example, to create a new site with the academic theme replace default lithium Hugo theme with gcushen/hugo-academic\nFor other themes go to http://themes.gohugo.io, choose a theme you like, click homepage and you will be redirected to its github repository.\nReplace its repository name with “gcushen/hugo-academic” above)\nblogdown::serve_site()\ncommand will create the website and it should be visible on the viewer pane.\nUse list.files() to see the files/folders generated.\nPrepare for GitHub compatibility\rTo be able to publish the website on github we need to specify a docs folder instead of the default public folder where the website is created.\nOpen your config.toml file under the Website folder, and add the lines; \nbaseurl: \"https://yourusername.github.io/page/\"\npublishDir = \"docs\"\rSave the config file. This will rebuild the site and create the docs folder.\rNow you can go back to your folder and delete the public directory.\n\r\rOn Github account\rCreate a new repository\rLog in to your Github account and create a new repository.\rHere I called it page.\rThen your remote repository will be at\nhttps://github.com/yourusername/page.git\rCopy that link.\r\r\rOn Git\rCreate your local .git repository\rBy using pwd and cd commands navigate your directory to the Website folder\nInitialize your local repository\ngit init\nUse the copied link from above\ngit remote add origin https://github.com/yourusername/page.git\ngit add .\ngit commit -m createmywebsite\nFollowing command will synchronize all your files to your Github repository\ngit push origin master\n\r\rPublishing your website on Github pages\rIn your Github repository. Go to settings\n\rScroll down to Github pages, choose source \u0026gt; master branch/docs folder\rThis will update the page. Scroll down to Github pages again click the link:\ne.g. https://yourusername.github.io/page/.\r\rHoorray! In this step your website should be up and running on Github!\n\rDeploying your Website to Netlify\rIf you want to benefit from the advantages like continous deployment you can\ruse Netlify.\nGo to your Netlify, Click Get started for free and sign up with Github.\n\rChoose New site from Git\rChoose GitHub at the bottom\rClick on Configure the Netlify app on GitHub.\rOn github select your target repository. Save.\rOn Netlify pick up that repository.\rModify Basic build settings: Publish directory should be docs\rClick on Show advanced \u0026gt; New Variable\n\rAdd a New Variable\nModify as Key = HUGO_VERSION and Value = 0.58.1\nThis step is important otherwise your site will not be built.\r\rIf you dont know your hugo version; on Rstudio \u0026gt; type\nblogdown::hugo_version()\n\rClick Deploy\r\rIt will allocate you a random link. You have to go back to your config.toml and modify baseurl as we did for Github above but with this netlify link.\nAfter this step you have to push changes to github from your local git. Similarly to the steps above without git init this time.\nNetlify will detect those changes you pushed to your github repository and your site will be published in a few seconds.\nHoorray! In this step your website should be up and running on Netlify!\n\rHow to use your own domain name\rGo to your domain provider e.g I did on Go Daddy\rMy Products \u0026gt; Scroll down to your domain \u0026gt; Click DNS\nCreate new or modify if existing an A record pointing your root domain to Netlify load balancer’s IP address 104.198.14.52 as in below;\rAdd a CNAME file as here;\r\rGo to domain settings on your deploy at Netlify\r\rAdd custom domain\rFill in your domain name\n\rClick Verify \u0026gt; Yes, add domain \u0026gt; Verify DNS configuration\r\rVoila, in this step your website should be online at www.yourdomain.com!!\n\r\r","date":1568505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568551079,"objectID":"772beaeb0af61021c2693fe7ebe26a63","permalink":"/r/deploy-your-blog-in-5-minutes/","publishdate":"2019-09-15T00:00:00Z","relpermalink":"/r/deploy-your-blog-in-5-minutes/","section":"post","summary":"This guide will help you to get your website online in a few minutes. Then, customize and add your own material in RStudio environment, push it to your Github repository and benefit from the continuous deployment feature of Netlify.\rIt took me many days of work, reading tens of blog posts, YouTube videos and a lot of testing to figure out all of this.\nHere is an up to date workflow of how I created my Blog on Github and deployed at Netlify.","tags":["RStudio","R Markdown","Hugo","Blogdown","Git","Github","Netlify","Domain","Blog","Hugo-academic theme"],"title":"Start blogging in 5 minutes on Netlify with Hugo and blogdown (September 2019 Update)","type":"post"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"}]