<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Randomforest | SERDAR KORUR</title>
    <link>/tags/randomforest/</link>
      <atom:link href="/tags/randomforest/index.xml" rel="self" type="application/rss+xml" />
    <description>Randomforest</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 01 Oct 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/logo.png</url>
      <title>Randomforest</title>
      <link>/tags/randomforest/</link>
    </image>
    
    <item>
      <title>Can machine learning change your lifestyle? Predicting Diabetes</title>
      <link>/r/predict-diseases/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/r/predict-diseases/</guid>
      <description>


&lt;p&gt;Would you be taking care of yourself better if your doctor told today that you have &lt;strong&gt;high risk of diabetes?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Advances in fields, such as omics and internet of things (sensors that collect data), and centralization of healthcare information (e.g. &lt;a href=&#34;https://www.ohdsi.org/data-standardization/the-common-data-model/&#34;&gt;OMOP common data model&lt;/a&gt;) enable us to access much wider data sources.&lt;/p&gt;
&lt;p&gt;Gaining insights from those we can improve our well being with better healthcare. Some applications of machine learning tools are;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Diagnosing diseases earlier&lt;/li&gt;
&lt;li&gt;Identifiying drugs with reduced side effects&lt;/li&gt;
&lt;li&gt;Select patient groups responding to an experimental therapy&lt;/li&gt;
&lt;li&gt;Utilize existing therapies better&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Let’s look at an example and try to help some doctors in diagnosing their patients.&lt;/strong&gt;&lt;/p&gt;
&lt;div id=&#34;problem-formulation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Problem formulation&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;Can we build a Machine learning algorithm to predict which patients will develop Diabetes?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The goal is to predict a Binary &lt;strong&gt;Outcome&lt;/strong&gt;: &lt;code&gt;Diabetes&lt;/code&gt; vs &lt;code&gt;Healthy&lt;/code&gt;
by using 8 medical indicators.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;overview-of-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview of the data&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;The Pima Indians of Arizona and Mexico&lt;/strong&gt; have contributed to numerous scientific gains. Their involvement has led to significant findings on genetics of both &lt;strong&gt;type 2 diabetes&lt;/strong&gt; and &lt;strong&gt;obesity.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The medical indicators recorded are;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pregnancies:&lt;/strong&gt; Number of times pregnant&lt;br /&gt;
&lt;strong&gt;Glucose:&lt;/strong&gt; Plasma glucose concentration a 2 hours in an oral glucose tolerance test&lt;br /&gt;
&lt;strong&gt;BloodPressure:&lt;/strong&gt; Diastolic blood pressure (mm Hg)&lt;br /&gt;
&lt;strong&gt;SkinThickness:&lt;/strong&gt; Triceps skin fold thickness (mm)&lt;br /&gt;
&lt;strong&gt;Insulin:&lt;/strong&gt; 2-Hour serum insulin (mu U/ml)&lt;br /&gt;
&lt;strong&gt;BMI:&lt;/strong&gt; Body mass index (weight in kg/(height in m)^2)&lt;br /&gt;
&lt;strong&gt;DiabetesPedigreeFunction:&lt;/strong&gt; Diabetes pedigree function&lt;br /&gt;
&lt;strong&gt;Age:&lt;/strong&gt; Age (years)&lt;br /&gt;
&lt;strong&gt;Outcome:&lt;/strong&gt; Class variable (0 or 1)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-acquisition&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data acquisition&lt;/h1&gt;
&lt;p&gt;I downloaded the &lt;a href=&#34;https://www.kaggle.com/uciml/pima-indians-diabetes-database&#34;&gt;Pima Indians Diabetes Dataset&lt;/a&gt; from Kaggle and imported it from my local directory.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diabetes &amp;lt;- read.csv(&amp;quot;posts_data/diabetes.csv&amp;quot;)

library(dplyr)
library(tidyr)
library(reshape2)
library(ggplot2)
library(ggcorrplot)
library(pROC)
library(lattice)
library(caret)
library(waffle)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-quality-control&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data Quality control&lt;/h1&gt;
&lt;p&gt;Before counting on any algorithm a good starting point is to &lt;strong&gt;check obvious mistakes and abnormalities in your data.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Here, I would look at &lt;strong&gt;Missing values&lt;/strong&gt;, variable ranges (&lt;strong&gt;min, max values&lt;/strong&gt;). A very extreme value might be basically a sign of typing error.&lt;/p&gt;
&lt;div id=&#34;understand-your-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Understand your Data&lt;/h2&gt;
&lt;p&gt;How big is the data? Checking the size and also type of variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(diabetes)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 768   9&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(sapply(diabetes, class))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;x&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Pregnancies&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;integer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Glucose&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;integer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;BloodPressure&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;integer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;SkinThickness&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;integer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Insulin&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;integer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;BMI&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;numeric&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;DiabetesPedigreeFunction&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;numeric&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Age&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;integer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Outcome&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;integer&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Next, what catches my attention is &lt;strong&gt;unexpected zero values in Insulin&lt;/strong&gt; below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(head(diabetes))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;Pregnancies&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Glucose&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;BloodPressure&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SkinThickness&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Insulin&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;BMI&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;DiabetesPedigreeFunction&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Age&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Outcome&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;148&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;72&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;35&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;33.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.627&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;85&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;66&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;29&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;26.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.351&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;31&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;183&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;64&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;23.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.672&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;32&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;89&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;66&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;23&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;94&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;28.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.167&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;21&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;137&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;40&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;35&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;168&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;43.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.288&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;33&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;116&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;74&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;25.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.201&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;30&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;missing-values&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Missing Values&lt;/h2&gt;
&lt;p&gt;Summary gives a good overview of the variables. Any missing data will show up here listed as &lt;strong&gt;“NA’s”&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(diabetes)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Pregnancies        Glucose      BloodPressure    SkinThickness  
##  Min.   : 0.000   Min.   :  0.0   Min.   :  0.00   Min.   : 0.00  
##  1st Qu.: 1.000   1st Qu.: 99.0   1st Qu.: 62.00   1st Qu.: 0.00  
##  Median : 3.000   Median :117.0   Median : 72.00   Median :23.00  
##  Mean   : 3.845   Mean   :120.9   Mean   : 69.11   Mean   :20.54  
##  3rd Qu.: 6.000   3rd Qu.:140.2   3rd Qu.: 80.00   3rd Qu.:32.00  
##  Max.   :17.000   Max.   :199.0   Max.   :122.00   Max.   :99.00  
##     Insulin           BMI        DiabetesPedigreeFunction      Age       
##  Min.   :  0.0   Min.   : 0.00   Min.   :0.0780           Min.   :21.00  
##  1st Qu.:  0.0   1st Qu.:27.30   1st Qu.:0.2437           1st Qu.:24.00  
##  Median : 30.5   Median :32.00   Median :0.3725           Median :29.00  
##  Mean   : 79.8   Mean   :31.99   Mean   :0.4719           Mean   :33.24  
##  3rd Qu.:127.2   3rd Qu.:36.60   3rd Qu.:0.6262           3rd Qu.:41.00  
##  Max.   :846.0   Max.   :67.10   Max.   :2.4200           Max.   :81.00  
##     Outcome     
##  Min.   :0.000  
##  1st Qu.:0.000  
##  Median :0.000  
##  Mean   :0.349  
##  3rd Qu.:1.000  
##  Max.   :1.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I will make visualizations of the variables to see how they are distributed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg &amp;lt;- melt(diabetes)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## No id variables; using all as measure variables&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(gg, aes(x=value, fill=variable)) +
  geom_histogram(binwidth=5)+ 
  facet_wrap(~variable) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-01-can-machine-learning-change-your-life-style_files/figure-html/Check%20out%20how%20variables%20are%20distributed-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Peaks at zero of Skin Thickness and Insulin is obvious here.&lt;/p&gt;
&lt;p&gt;In cases where the numbers are small we might remove them. &lt;strong&gt;Let’s figure it out with a for loop.&lt;/strong&gt; and then visualize on a waffle plot.&lt;/p&gt;
&lt;p&gt;This will bring me the number of zero containing rows in variables from 2 to 6.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;zero_rows &amp;lt;- list()
for(i in 2:6){
zero_rows[[i]] &amp;lt;- length(which(diabetes[,i] == 0))  
}
rows_with_zero &amp;lt;- unlist(zero_rows)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Feed those numbers to a waffle plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;zeros &amp;lt;- c(&amp;quot;Glucose&amp;quot; =rows_with_zero[1], &amp;quot;Blood Pressure&amp;quot; = rows_with_zero[2], 
           &amp;quot;Skin Thickness&amp;quot;= rows_with_zero[3], &amp;quot;Insulin&amp;quot; =rows_with_zero[4], 
           &amp;quot;BMI&amp;quot; = rows_with_zero[5])
waffle(zeros, rows=20) + theme(text = element_text(size=15)) + ggtitle(&amp;quot;Number of rows with zero&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-01-can-machine-learning-change-your-life-style_files/figure-html/waffle%20plot%20of%20zero%20values-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- data.frame(rows_with_zero, row.names = names(diabetes[2:6]))
df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               rows_with_zero
## Glucose                    5
## BloodPressure             35
## SkinThickness            227
## Insulin                  374
## BMI                       11&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For instance, in Insulin &lt;strong&gt;374&lt;/strong&gt; values are zero. Other variables also contain zeros. It is impossible to have Blood Pressure or Glucose levels at 0. It is unlikely that those are simply entry mistakes. It seems missing values are filled with &lt;strong&gt;zeros&lt;/strong&gt; in the data collection phase.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How to circumwent this?&lt;/strong&gt;&lt;/p&gt;
&lt;div id=&#34;convert-all-zeroes-to-nas-and-then-perform-median-imputation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Convert all &lt;strong&gt;zeroes&lt;/strong&gt; to &lt;strong&gt;NAs&lt;/strong&gt; and then perform &lt;strong&gt;Median Imputation&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;I will use a for loop to remove zeros in all the predictors except Pregnancy and Pedigree function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(i in 2:6){
# Convert zeros to NAs
diabetes[, i][diabetes[, i] == 0] &amp;lt;- NA
# Calculate median
median &amp;lt;- median(diabetes[, i], na.rm = TRUE)
diabetes[, i][is.na(diabetes[, i])] &amp;lt;- median
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check if it really happened&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(head(diabetes))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;Pregnancies&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Glucose&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;BloodPressure&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SkinThickness&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Insulin&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;BMI&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;DiabetesPedigreeFunction&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Age&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Outcome&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;148&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;72&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;35&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;125&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;33.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.627&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;85&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;66&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;29&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;125&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;26.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.351&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;31&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;183&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;64&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;29&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;125&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;23.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.672&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;32&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;89&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;66&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;23&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;94&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;28.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.167&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;21&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;137&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;40&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;35&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;168&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;43.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.288&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;33&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;116&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;74&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;29&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;125&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;25.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.201&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;30&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;For instance, I see that zero values in the insulin variable is replaced with median of insulin which is 125.&lt;/p&gt;
&lt;p&gt;Now, the data is clean and ready for the modeling phase.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;modeling-the-data-build-fit-and-validate-a-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Modeling the data (build, fit and validate a model)&lt;/h1&gt;
&lt;p&gt;Before going into any complicated model starting with a simple mode is a good idea. It might do surprisingly well and will give us more insights about the data.&lt;/p&gt;
&lt;div id=&#34;linear-regression-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Linear Regression Model&lt;/h2&gt;
&lt;p&gt;We will create two random subsets of our data in 80/20 proportion as &lt;strong&gt;training and test data.&lt;/strong&gt;&lt;br /&gt;
Training data will be used to build our model and test data will be reserved to validate it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(22)
# Create train test split
sample_rows &amp;lt;- sample(nrow(diabetes), nrow(diabetes) * 0.8)
# Create the training dataset
dia_train &amp;lt;- diabetes[sample_rows, ]
# Create the test dataset
dia_test &amp;lt;- diabetes[-sample_rows, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Build a linear model with the train data
lm_dia &amp;lt;- lm(Outcome ~ .,data = dia_train)
summary(lm_dia)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Outcome ~ ., data = dia_train)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.01862 -0.28264 -0.07603  0.29983  0.86403 
## 
## Coefficients:
##                            Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)              -1.0441425  0.1180483  -8.845  &amp;lt; 2e-16 ***
## Pregnancies               0.0173561  0.0056433   3.076 0.002196 ** 
## Glucose                   0.0063034  0.0006150  10.250  &amp;lt; 2e-16 ***
## BloodPressure            -0.0013212  0.0014700  -0.899 0.369137    
## SkinThickness             0.0008966  0.0023291   0.385 0.700395    
## Insulin                  -0.0003233  0.0002088  -1.548 0.122077    
## BMI                       0.0150019  0.0030523   4.915 1.15e-06 ***
## DiabetesPedigreeFunction  0.1661357  0.0489591   3.393 0.000736 ***
## Age                       0.0036157  0.0017283   2.092 0.036845 *  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.3966 on 605 degrees of freedom
## Multiple R-squared:  0.3174, Adjusted R-squared:  0.3084 
## F-statistic: 35.17 on 8 and 605 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# We will predict the Outcome for the test data
p&amp;lt;-predict(lm_dia, dia_test)
# Choose a threshold 0.5 to calculate the accuracy of our model
p_05 &amp;lt;- ifelse(p &amp;gt; 0.5, 1, 0)
table(p_05, dia_test$Outcome)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     
## p_05  0  1
##    0 85 17
##    1 15 37&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will build a confusion matrix to calculate how accurate our model is in this particular random train/test split and at 0.5 thereshold level.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;conf_mat &amp;lt;- table(p_05, dia_test$Outcome)
accuracy &amp;lt;- sum(diag(conf_mat))/sum(conf_mat)
accuracy&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7922078&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;roc&lt;/strong&gt; function pROC package, can plot us a ROC curve which tests accuracy of our model at multiple threshold levels and is a good estimate on how well our model is performing.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Calculate AUC(Area under the curve)
roc(dia_test$Outcome, p)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Setting levels: control = 0, case = 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Setting direction: controls &amp;lt; cases&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## roc.default(response = dia_test$Outcome, predictor = p)
## 
## Data: p in 100 controls (dia_test$Outcome 0) &amp;lt; 54 cases (dia_test$Outcome 1).
## Area under the curve: 0.8498&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, this process is little fragile, presence or absence of a single outlier might vastly change the results you might get from a given random train/test split.&lt;/p&gt;
&lt;p&gt;A better approach than a simple train/test split is using multiple test sets and averaging their accuracies.&lt;/p&gt;
&lt;p&gt;Let’s test that. I will create 1, 30 or 1000 random test sets, build models and compare their accuracies.&lt;/p&gt;
&lt;div id=&#34;how-to-apply-linear-model-with-multiple-traintest-split&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;How to apply linear model with multiple train/test split&lt;/h4&gt;
&lt;p&gt;To do this, I will &lt;strong&gt;write a function&lt;/strong&gt; where I can choose number of independent train/test splits.&lt;/p&gt;
&lt;p&gt;It will return me an average value of the accuracy(auc) of the model after chosen number of iteration. The higher the number of random splits the more stable your estimated AUC will be.&lt;/p&gt;
&lt;p&gt;Let’s see how it will work out for our diabetes patients.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# I will define my function as follows
multi_split &amp;lt;- function(x){
sample_rows &amp;lt;- list()
dia_train &amp;lt;- list()
dia_test &amp;lt;- list()
lm &amp;lt;- list()
p &amp;lt;-  list()
roc_auc &amp;lt;- list()
for(i in 1:x){
  sample_rows[[i]] &amp;lt;- sample(nrow(diabetes), nrow(diabetes) * 0.8)
  # Create the training dataset
  dia_train[[i]] &amp;lt;- diabetes[sample_rows[[i]], ]
  # Create the test dataset
  dia_test[[i]] &amp;lt;- diabetes[-sample_rows[[i]], ]
  lm[[i]] &amp;lt;- lm(Outcome ~ .,data = dia_train[[i]])
  p[[i]] &amp;lt;- predict(lm[[i]], dia_test[[i]])
  
  # Calculate AUC for all &amp;quot;x&amp;quot; number of random splits
  roc_auc[[i]] &amp;lt;- roc(dia_test[[i]]$Outcome, p[[i]])$auc[1]
  lm_mean &amp;lt;- mean(unlist(roc_auc))
}
print(mean(unlist(roc_auc)))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s calculate the average AUC of our model after different number of random splits.&lt;/p&gt;
&lt;p&gt;I will run my &lt;strong&gt;multi_split() function&lt;/strong&gt; 3x for 1, 30 and 1000 random train/test splits. I can then compare variances at each level of sampling.&lt;/p&gt;
&lt;p&gt;Here are the results from my multi_site function at each randomization.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;auc_1_1 &amp;lt;- multi_split(1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8769133&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;auc_1_2 &amp;lt;- multi_split(1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8283688&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;auc_1_3 &amp;lt;- multi_split(1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8216108&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;auc_30_1 &amp;lt;- multi_split(30)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.840713&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;auc_30_2 &amp;lt;- multi_split(30)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8501071&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;auc_30_3 &amp;lt;- multi_split(30)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8294741&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;auc_1000_1 &amp;lt;- multi_split(1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8362065&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;auc_1000_2 &amp;lt;- multi_split(1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8364364&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;auc_1000_3 &amp;lt;- multi_split(1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8350323&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s compare Variance levels at &lt;strong&gt;1, 30 and 1000&lt;/strong&gt; random splits&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var(c(auc_1_1, auc_1_2, auc_1_3))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0009101001&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var(c(auc_30_1, auc_30_2, auc_30_3))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0001067148&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var(c(auc_1000_1, auc_1000_2, auc_1000_3))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5.672518e-07&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What we see here as we increase the number of iterations to 30 and 1000 the variability
gradually stabilizes around a trustable AUC of &lt;code&gt;0.836&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Seeing is believing. Let’s plot it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a data.frame containing accuracies
random_1X &amp;lt;- c(auc_1_1, auc_1_2, auc_1_3)
random_30X &amp;lt;- c(auc_30_1, auc_30_2, auc_30_3)
random_1000X &amp;lt;- c(auc_1000_1, auc_1000_2, auc_1000_3)

df_r &amp;lt;- data.frame(random_1X, random_30X, random_1000X)
df_r&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   random_1X random_30X random_1000X
## 1 0.8769133  0.8407130    0.8362065
## 2 0.8283688  0.8501071    0.8364364
## 3 0.8216108  0.8294741    0.8350323&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Here, I will reformat my data for easy plotting by using gather() function from tidyr
# It takes multiple columns, and gathers them into key-value pairs: it makes “wide” data longer.
df_long &amp;lt;- gather(df_r, sampling, auc)
df_long&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       sampling       auc
## 1    random_1X 0.8769133
## 2    random_1X 0.8283688
## 3    random_1X 0.8216108
## 4   random_30X 0.8407130
## 5   random_30X 0.8501071
## 6   random_30X 0.8294741
## 7 random_1000X 0.8362065
## 8 random_1000X 0.8364364
## 9 random_1000X 0.8350323&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_long$sampling &amp;lt;- factor(df_long$sampling, levels = c(&amp;quot;random_1X&amp;quot;, &amp;quot;random_30X&amp;quot;, &amp;quot;random_1000X&amp;quot;))

# 
model_variation &amp;lt;- ggplot(df_long, aes(y=auc, x=sampling, fill=sampling)) + geom_boxplot() + theme(text = element_text(size=15), axis.title.x=element_blank(), legend.position = &amp;quot;none&amp;quot;) + ggtitle(&amp;quot;Variation in model performance&amp;quot;)
model_variation&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-01-can-machine-learning-change-your-life-style_files/figure-html/change%20in%20variance%20plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Great. We have a good estimate of our linear model performance.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;logistic-regression-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Logistic regression model&lt;/h2&gt;
&lt;p&gt;I will switch here to caret package. With the &lt;strong&gt;Train()&lt;/strong&gt; function we can test different types of machine learning algorithms.&lt;/p&gt;
&lt;p&gt;I will start with a logistic regression model. But before I need to convert Outcome variable into a factor with 2 levels.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Convert Outcome to a factor with two levels
diabetes$Outcome &amp;lt;- factor(diabetes$Outcome)
levels(diabetes$Outcome) &amp;lt;- c(&amp;quot;H&amp;quot;, &amp;quot;D&amp;quot;)

# MyControl
myControl &amp;lt;- trainControl(
  method = &amp;quot;cv&amp;quot;, 
  number = 10,
  summaryFunction = twoClassSummary,
  classProbs = TRUE, # IMPORTANT!
  verboseIter = TRUE
)
# Model
model_glm &amp;lt;- train(
  Outcome~.,
  method = &amp;quot;glm&amp;quot;,
  data = diabetes,
  trControl = myControl
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## + Fold01: parameter=none 
## - Fold01: parameter=none 
## + Fold02: parameter=none 
## - Fold02: parameter=none 
## + Fold03: parameter=none 
## - Fold03: parameter=none 
## + Fold04: parameter=none 
## - Fold04: parameter=none 
## + Fold05: parameter=none 
## - Fold05: parameter=none 
## + Fold06: parameter=none 
## - Fold06: parameter=none 
## + Fold07: parameter=none 
## - Fold07: parameter=none 
## + Fold08: parameter=none 
## - Fold08: parameter=none 
## + Fold09: parameter=none 
## - Fold09: parameter=none 
## + Fold10: parameter=none 
## - Fold10: parameter=none 
## Aggregating results
## Fitting final model on full training set&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_glm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Generalized Linear Model 
## 
## 768 samples
##   8 predictor
##   2 classes: &amp;#39;H&amp;#39;, &amp;#39;D&amp;#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 691, 691, 691, 691, 692, 691, ... 
## Resampling results:
## 
##   ROC        Sens   Spec     
##   0.8369744  0.874  0.5521368&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, My model performance is &lt;code&gt;0.8369744&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;glmnet-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Glmnet model&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Model
model_glmnet &amp;lt;- train(
  Outcome~.,
  method = &amp;quot;glmnet&amp;quot;,
  data = diabetes,
  trControl = myControl
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_glmnet&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## glmnet 
## 
## 768 samples
##   8 predictor
##   2 classes: &amp;#39;H&amp;#39;, &amp;#39;D&amp;#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 691, 691, 691, 691, 691, 691, ... 
## Resampling results across tuning parameters:
## 
##   alpha  lambda        ROC        Sens   Spec     
##   0.10   0.0004697604  0.8383818  0.878  0.5592593
##   0.10   0.0046976036  0.8392821  0.878  0.5592593
##   0.10   0.0469760360  0.8405043  0.892  0.5368946
##   0.55   0.0004697604  0.8383105  0.876  0.5629630
##   0.55   0.0046976036  0.8395954  0.878  0.5555556
##   0.55   0.0469760360  0.8388917  0.902  0.5216524
##   1.00   0.0004697604  0.8383846  0.876  0.5629630
##   1.00   0.0046976036  0.8402650  0.882  0.5554131
##   1.00   0.0469760360  0.8266125  0.904  0.4809117
## 
## ROC was used to select the optimal model using the largest value.
## The final values used for the model were alpha = 0.1 and lambda
##  = 0.04697604.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Glmnet model performance is &lt;code&gt;0.8405043&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-forest-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Random forest model&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Random forest model
model_rf &amp;lt;- train(
  Outcome~.,
  method = &amp;quot;ranger&amp;quot;,
  data = diabetes,
  trControl = myControl
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_rf&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Random Forest 
## 
## 768 samples
##   8 predictor
##   2 classes: &amp;#39;H&amp;#39;, &amp;#39;D&amp;#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 691, 691, 691, 691, 691, 692, ... 
## Resampling results across tuning parameters:
## 
##   mtry  splitrule   ROC        Sens   Spec     
##   2     gini        0.8343091  0.844  0.6272080
##   2     extratrees  0.8354615  0.860  0.5820513
##   5     gini        0.8295798  0.838  0.6273504
##   5     extratrees  0.8339772  0.850  0.6084046
##   8     gini        0.8270470  0.832  0.6236467
##   8     extratrees  0.8359687  0.852  0.6195157
## 
## Tuning parameter &amp;#39;min.node.size&amp;#39; was held constant at a value of 1
## ROC was used to select the optimal model using the largest value.
## The final values used for the model were mtry = 8, splitrule =
##  extratrees and min.node.size = 1.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Random forest performance is &lt;code&gt;0.8359687&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gradient-boost-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gradient boost model&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_gbm &amp;lt;- train(
    Outcome~.,
    method = &amp;quot;gbm&amp;quot;,
    data = diabetes,
    trControl = myControl
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_gbm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Stochastic Gradient Boosting 
## 
## 768 samples
##   8 predictor
##   2 classes: &amp;#39;H&amp;#39;, &amp;#39;D&amp;#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 691, 691, 691, 691, 691, 691, ... 
## Resampling results across tuning parameters:
## 
##   interaction.depth  n.trees  ROC        Sens   Spec     
##   1                   50      0.8360014  0.890  0.5373219
##   1                  100      0.8422849  0.876  0.5749288
##   1                  150      0.8407721  0.868  0.5787749
##   2                   50      0.8362051  0.864  0.5559829
##   2                  100      0.8399658  0.856  0.6047009
##   2                  150      0.8346097  0.842  0.6195157
##   3                   50      0.8330570  0.852  0.5896011
##   3                  100      0.8310883  0.846  0.6008547
##   3                  150      0.8254929  0.846  0.5864672
## 
## Tuning parameter &amp;#39;shrinkage&amp;#39; was held constant at a value of 0.1
## 
## Tuning parameter &amp;#39;n.minobsinnode&amp;#39; was held constant at a value of 10
## ROC was used to select the optimal model using the largest value.
## The final values used for the model were n.trees = 100,
##  interaction.depth = 1, shrinkage = 0.1 and n.minobsinnode = 10.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Gradient boost model performance is &lt;code&gt;0.8422849&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;naive-bayes-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Naive Bayes model&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_nb &amp;lt;- train(
    Outcome~.,
    method = &amp;quot;nb&amp;quot;,
    data = diabetes,
    trControl = myControl
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_nb&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Naive Bayes 
## 
## 768 samples
##   8 predictor
##   2 classes: &amp;#39;H&amp;#39;, &amp;#39;D&amp;#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 691, 691, 691, 691, 692, 691, ... 
## Resampling results across tuning parameters:
## 
##   usekernel  ROC        Sens   Spec     
##   FALSE      0.8089573  0.828  0.5964387
##    TRUE      0.8104530  0.822  0.6113960
## 
## Tuning parameter &amp;#39;fL&amp;#39; was held constant at a value of 0
## Tuning
##  parameter &amp;#39;adjust&amp;#39; was held constant at a value of 1
## ROC was used to select the optimal model using the largest value.
## The final values used for the model were fL = 0, usekernel = TRUE
##  and adjust = 1.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Naive Bayes model performance is &lt;code&gt;0.810453&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;models &amp;lt;- c(&amp;quot;lm&amp;quot;, &amp;quot;glm&amp;quot;, &amp;quot;glmnet&amp;quot;, &amp;quot;rf&amp;quot;, &amp;quot;gbm&amp;quot;, &amp;quot;naive&amp;quot;)
lm &amp;lt;- round(mean(c(auc_1000_1, auc_1000_2, auc_1000_3)), 3)
glm &amp;lt;- max(model_glm$results$ROC)
glmnet &amp;lt;- max(model_glmnet$results$ROC)
rf &amp;lt;- max(model_rf$results$ROC)
gbm &amp;lt;- max(model_gbm$results$ROC)
naive &amp;lt;- max(model_nb$results$ROC)
AUC &amp;lt;- c(lm, glm, glmnet, rf, gbm, naive)
df &amp;lt;- data.frame(models, AUC)
knitr::kable(df[order(df[,2], decreasing=TRUE), ])&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;models&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AUC&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;gbm&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8422849&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;glmnet&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8405043&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;glm&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8369744&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;lm&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8360000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;rf&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8359687&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;naive&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8104530&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here, we found Gradient boost model performed the best, and also there are not big differences between the models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;future-thoughts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Future thoughts&lt;/h2&gt;
&lt;p&gt;I used different machine learning algorithms to predict Diabetes. Models showed similar performances except the naives bayes which performed worst here. &lt;strong&gt;As we saw, sometimes a simple model can get us far.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We can help doctors to predict &lt;strong&gt;Diabetes with accuracy around 84%&lt;/strong&gt; by using 8 simple medical parameters.&lt;/p&gt;
&lt;p&gt;Given current speed in generation and collection of types data by including additonal predictors we can build even better models.&lt;/p&gt;
&lt;p&gt;Until next time!&lt;/p&gt;
&lt;p&gt;Serdar&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
