<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Feature engineering | SERDAR KORUR</title>
    <link>/tags/feature-engineering/</link>
      <atom:link href="/tags/feature-engineering/index.xml" rel="self" type="application/rss+xml" />
    <description>Feature engineering</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 01 Oct 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/logo.png</url>
      <title>Feature engineering</title>
      <link>/tags/feature-engineering/</link>
    </image>
    
    <item>
      <title>Why not everyone who smokes develop cancer or who eats a lot develop fatty liver disease? Predicting diseases with machine learning</title>
      <link>/r/predict-diseases/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/r/predict-diseases/</guid>
      <description>


&lt;p&gt;We are much better at handling diseases than 30 years ago. For example &lt;a href=&#34;https://dataatomic.com/r/ggplot2-waffle/&#34;&gt;cancer survival rates&lt;/a&gt; are much higher now. The significant portion of this increase &lt;strong&gt;can be attributed directly to our ability to detect and diagnose cancer earlier.&lt;/strong&gt; Also, use of insulin and other drugs to control blood glucose in diabetic patients reduced the risk of developing coronary diseases.&lt;/p&gt;
We are at constant hunt for finding new evidence &lt;strong&gt;which environmental factors put us at risk for which diseases.&lt;/strong&gt; Exposure to certain chemicals (smoking, industry workers) can cause certain types of cancer and our eating habits might put us at higher risk for developing diseases such us diabetes or liver fibrosis.
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/heather-ford-Fq54FqucgCE-unsplash.jpg&#34; alt=&#34;Photo by Heather Ford on Unsplash&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Photo by Heather Ford on Unsplash
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;But not everyone who smokes develop lung cancer or who eats a lot of sugar develop fatty liver disease.&lt;/strong&gt; Our genetic background makes us prone or immune to develop different diseases. &lt;strong&gt;This is where data science might enable us with new insights connecting the risk factors to our genetic make up.&lt;/strong&gt; And the better we are at identifying high risk patients the better treatment for the patients.&lt;/p&gt;
&lt;p&gt;We are developing new technologies at lightning speed. We can now analyse genes from a single cell which was not even possible couple of years back, and extract the knowledge hidden in rare genes or rare cell types.&lt;/p&gt;
&lt;p&gt;Sensors collect real time data from patients e.g. in diabetic patients, we started to pool data (real world evidence) generated during the actual use of drugs by patients (e.g. &lt;a href=&#34;https://www.ohdsi.org/data-standardization/the-common-data-model/&#34;&gt;OMOP common data model&lt;/a&gt;. &lt;strong&gt;The data that we have access now is enormous and is growing rapidly.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data Science&lt;/strong&gt; tools are more than ever needed to mold this data into better therapies.&lt;/p&gt;
&lt;p&gt;We can build new machine learning models and;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;diagnose diseases earlier&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;develop better drugs which are more effective and have less side effects&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;identify patient groups which benefit most from existing drugs&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will be able to understand why certain diseases develop and their connections to our lifestyle by using multiple types of data we accumulate.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Let’s look at an example&lt;/strong&gt; and see how we can do this.&lt;/p&gt;
&lt;p&gt;First part in any data science process is &lt;strong&gt;problem formulation.&lt;/strong&gt;&lt;/p&gt;
&lt;div id=&#34;problem-formulation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Problem formulation&lt;/h1&gt;
&lt;p&gt;Our problem, or question that we want to answer is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Can we predict which patients will develop Diabetes by building a Machine learning algorithm?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We will predict a Binary &lt;strong&gt;Outcome&lt;/strong&gt;: &lt;code&gt;Diabetes&lt;/code&gt; vs &lt;code&gt;Healthy&lt;/code&gt;
by using 8 medical indicators.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;overview-of-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview of the data&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;The Pima Indians of Arizona and Mexico&lt;/strong&gt; have contributed to numerous scientific gains. Their involvement has led to significant findings on genetics of both &lt;strong&gt;type 2 diabetes&lt;/strong&gt; and &lt;strong&gt;obesity.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The medical indicators recorded are;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pregnancies:&lt;/strong&gt; Number of times pregnant&lt;br /&gt;
&lt;strong&gt;Glucose:&lt;/strong&gt; Plasma glucose concentration a 2 hours in an oral glucose tolerance test&lt;br /&gt;
&lt;strong&gt;BloodPressure:&lt;/strong&gt; Diastolic blood pressure (mm Hg)&lt;br /&gt;
&lt;strong&gt;SkinThickness:&lt;/strong&gt; Triceps skin fold thickness (mm)&lt;br /&gt;
&lt;strong&gt;Insulin:&lt;/strong&gt; 2-Hour serum insulin (mu U/ml)&lt;br /&gt;
&lt;strong&gt;BMI:&lt;/strong&gt; Body mass index (weight in kg/(height in m)^2)&lt;br /&gt;
&lt;strong&gt;DiabetesPedigreeFunction:&lt;/strong&gt; Diabetes pedigree function&lt;br /&gt;
&lt;strong&gt;Age:&lt;/strong&gt; Age (years)&lt;br /&gt;
&lt;strong&gt;Outcome:&lt;/strong&gt; Class variable (0 or 1)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-acquisition&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data acquisition&lt;/h1&gt;
&lt;p&gt;You can download the &lt;a href=&#34;https://www.kaggle.com/uciml/pima-indians-diabetes-database&#34;&gt;Pima Indians Diabetes Dataset&lt;/a&gt; from Kaggle and load it in RStudio.&lt;/p&gt;
&lt;p&gt;Setting up and loading in the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diabetes &amp;lt;- read.csv(&amp;quot;posts_data/diabetes.csv&amp;quot;)
library(tidyverse) # Includes packages: ggplot2, dplyr, tidyr, readr, 
                   # purrr, tibble, stringr, forcats
library(reshape2) # Main function: melt()
library(ggcorrplot)
library(pROC)
library(lattice)
library(caret)
library(waffle)
library(compareGroups) # Main functions: compareGroups(), createTable()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next step in our data science process is to check whether the data quality is good.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-quality-control&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data Quality control&lt;/h1&gt;
&lt;p&gt;Before running any algorithm a good starting point is to &lt;strong&gt;check obvious mistakes and abnormalities in your data.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I would first look at &lt;strong&gt;Missing values&lt;/strong&gt;, &lt;strong&gt;NAs&lt;/strong&gt;, variable ranges (&lt;strong&gt;min, max values&lt;/strong&gt;). A very extreme value might be basically a typing error.&lt;/p&gt;
&lt;div id=&#34;understand-your-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Understand your Data&lt;/h2&gt;
&lt;p&gt;How big is the data? Classes of variables?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(diabetes)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 768   9&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(sapply(diabetes, class))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
x
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Pregnancies
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
integer
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Glucose
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
integer
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
BloodPressure
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
integer
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SkinThickness
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
integer
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Insulin
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
integer
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
BMI
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
numeric
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
DiabetesPedigreeFunction
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
numeric
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Age
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
integer
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Outcome
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
integer
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;I look which atomic data types my variables are. I see that the outcome variable is represented as an &lt;strong&gt;integer.&lt;/strong&gt; We will keep this in mind because many machine learning models will accept the binary outcome when converted to a factor atomic data type.&lt;/p&gt;
&lt;p&gt;Next, what catches my attention is &lt;strong&gt;unexpected zeros in Insulin&lt;/strong&gt;. See below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(head(diabetes))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Pregnancies
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Glucose
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
BloodPressure
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
SkinThickness
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Insulin
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
BMI
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
DiabetesPedigreeFunction
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Age
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Outcome
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
148
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
72
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
35
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
33.6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.627
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
85
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
66
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
26.6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.351
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
31
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
183
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
64
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23.3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.672
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
32
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
89
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
66
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
94
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
28.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.167
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
21
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
137
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
35
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
168
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
43.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.288
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
33
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
116
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
74
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
25.6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.201
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
30
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;missing-values&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Missing Values&lt;/h2&gt;
&lt;p&gt;Summary gives a good overview of the variables. Any missing data would show up here listed as &lt;strong&gt;“NA’s”&lt;/strong&gt;. But we have none here.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(diabetes)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Pregnancies        Glucose      BloodPressure    SkinThickness  
##  Min.   : 0.000   Min.   :  0.0   Min.   :  0.00   Min.   : 0.00  
##  1st Qu.: 1.000   1st Qu.: 99.0   1st Qu.: 62.00   1st Qu.: 0.00  
##  Median : 3.000   Median :117.0   Median : 72.00   Median :23.00  
##  Mean   : 3.845   Mean   :120.9   Mean   : 69.11   Mean   :20.54  
##  3rd Qu.: 6.000   3rd Qu.:140.2   3rd Qu.: 80.00   3rd Qu.:32.00  
##  Max.   :17.000   Max.   :199.0   Max.   :122.00   Max.   :99.00  
##     Insulin           BMI        DiabetesPedigreeFunction      Age       
##  Min.   :  0.0   Min.   : 0.00   Min.   :0.0780           Min.   :21.00  
##  1st Qu.:  0.0   1st Qu.:27.30   1st Qu.:0.2437           1st Qu.:24.00  
##  Median : 30.5   Median :32.00   Median :0.3725           Median :29.00  
##  Mean   : 79.8   Mean   :31.99   Mean   :0.4719           Mean   :33.24  
##  3rd Qu.:127.2   3rd Qu.:36.60   3rd Qu.:0.6262           3rd Qu.:41.00  
##  Max.   :846.0   Max.   :67.10   Max.   :2.4200           Max.   :81.00  
##     Outcome     
##  Min.   :0.000  
##  1st Qu.:0.000  
##  Median :0.000  
##  Mean   :0.349  
##  3rd Qu.:1.000  
##  Max.   :1.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Plotting how the variables are distributed will give a good overview to spot problems.&lt;/p&gt;
&lt;p&gt;I will change the data format so that I can plot all the variables in different facets. &lt;strong&gt;melt()&lt;/strong&gt; function from reshape2 package can create a tall version of my data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;This function will collect all variable names in one column and corresponding values in the next column.&lt;/strong&gt; This data structure will allow me to plot all variables together.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg &amp;lt;- melt(diabetes)

# Check how the new data structure looks like
head(gg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      variable value
## 1 Pregnancies     6
## 2 Pregnancies     1
## 3 Pregnancies     8
## 4 Pregnancies     1
## 5 Pregnancies     0
## 6 Pregnancies     5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plot all variables
ggplot(gg, aes(x=value, fill=variable)) +
  geom_histogram(binwidth=5) + 
  theme(legend.position = &amp;quot;none&amp;quot;) +
  facet_wrap(~variable) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-01-can-machine-learning-change-your-life-style_files/figure-html/Check%20out%20how%20the%20variables%20are%20distributed-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Multiple variables have peaks at zero. E.g. Skin Thickness and Insulin. It is not possible that those variables are zero.&lt;/p&gt;
&lt;p&gt;I want to know how many zeros each variables has. In cases where the numbers are small we might remove them. &lt;strong&gt;Let’s figure it out with a for loop.&lt;/strong&gt; and then visualize on a waffle plot.&lt;/p&gt;
&lt;p&gt;I am selecting only variables from 2 to 6 because only those can’t be zero. Let’s count number of zeros.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How to count zeros in each column?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Approach 1:&lt;/strong&gt; Using a for loop, which() function and []&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;zero_rows &amp;lt;- list()
for(i in 2:6){
zero_rows[[i]] &amp;lt;- length(which(diabetes[,i] == 0))  
}
rows_with_zero &amp;lt;- unlist(zero_rows)
rows_with_zero&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]   5  35 227 374  11&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a bonus, I recommend using dplyr package.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Approach 2:&lt;/strong&gt; Much simpler with dplyr &lt;strong&gt;summarise_all()&lt;/strong&gt; function&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;zeros &amp;lt;- diabetes[,2:6] %&amp;gt;%  summarise_all(funs(sum(.==0)))
t(zeros)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               [,1]
## Glucose          5
## BloodPressure   35
## SkinThickness  227
## Insulin        374
## BMI             11&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Feed those numbers into a waffle plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;zeros &amp;lt;- c(&amp;quot;Glucose&amp;quot; =zeros[1,1], &amp;quot;Blood Pressure&amp;quot; = zeros[1,2], 
           &amp;quot;Skin Thickness&amp;quot;= zeros[1,3], &amp;quot;Insulin&amp;quot; =zeros[1,4], 
           &amp;quot;BMI&amp;quot; = zeros[1,5])
waffle(zeros, rows=20) + 
  theme(text = element_text(size=15)) + 
  ggtitle(&amp;quot;Number of rows with zero&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-01-can-machine-learning-change-your-life-style_files/figure-html/waffle%20plot%20of%20zero%20values-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For instance, &lt;strong&gt;374&lt;/strong&gt; rows of Insulin are zero. Other variables also contain zeros. Something is wrong. It is impossible to have Blood Pressure or Glucose levels at 0. It is unlikely that those are simply entry mistakes. It seems missing values are filled with &lt;strong&gt;zeros&lt;/strong&gt; in the data collection phase.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How to circumvent this?&lt;/strong&gt;&lt;/p&gt;
&lt;div id=&#34;convert-all-zeroes-to-nas-and-then-perform-median-imputation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Convert all &lt;strong&gt;zeroes&lt;/strong&gt; to &lt;strong&gt;NAs&lt;/strong&gt; and then perform &lt;strong&gt;Median Imputation&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Most models require numbers, and can’t handle missing data. Throwing out rows is not a good idea since it can lead to biases in your dataset and generate overconfident models.&lt;/p&gt;
&lt;p&gt;Median imputation lets you model data with missing values. By replacing them with their medians.&lt;/p&gt;
&lt;p&gt;To do this, I need to change zeros to missing values. I will do this for all the predictors which zero is not plausible(columns 2 to 6).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(i in 2:6){
# Convert zeros to NAs
diabetes[, i][diabetes[, i] == 0] &amp;lt;- NA
# Calculate median
median &amp;lt;- median(diabetes[, i], na.rm = TRUE)
diabetes[, i][is.na(diabetes[, i])] &amp;lt;- median
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check if it really happened.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(head(diabetes))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Pregnancies
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Glucose
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
BloodPressure
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
SkinThickness
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Insulin
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
BMI
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
DiabetesPedigreeFunction
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Age
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Outcome
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
148
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
72
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
35
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
125
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
33.6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.627
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
85
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
66
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
125
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
26.6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.351
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
31
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
183
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
64
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
125
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23.3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.672
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
32
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
89
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
66
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
94
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
28.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.167
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
21
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
137
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
35
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
168
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
43.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.288
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
33
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
116
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
74
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
125
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
25.6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.201
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
30
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;For instance, I see that zero values in the insulin variable is replaced with median of insulin which is 125.&lt;/p&gt;
&lt;p&gt;I will also look at the differences between variables in diabetic versus healthy groups so that I know more which variables might play a role in the Outcome.&lt;/p&gt;
&lt;p&gt;I can also use dplyr functions but I will use &lt;strong&gt;compareGroups&lt;/strong&gt; package because it creates a nice output of the summary statistics in a table format. compareGroups() function will do the analysis and createTable() will output it in a nice format.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;base &amp;lt;- compareGroups(Outcome~Pregnancies+Glucose+BloodPressure+
                        SkinThickness+Insulin+BMI +
                        DiabetesPedigreeFunction+Age, 
                        data = diabetes)

summary_stats &amp;lt;- createTable(base, show.ratio = FALSE, show.p.overall=TRUE)
summary_stats&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## --------Summary descriptives table by &amp;#39;Outcome&amp;#39;---------
## 
## __________________________________________________________ 
##                               0           1      p.overall 
##                             N=500       N=268              
## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ 
## Pregnancies              3.30 (3.02) 4.87 (3.74)  &amp;lt;0.001   
## Glucose                  111 (24.7)  142 (29.6)   &amp;lt;0.001   
## BloodPressure            70.9 (11.9) 75.1 (12.0)  &amp;lt;0.001   
## SkinThickness            27.7 (8.55) 31.7 (8.66)  &amp;lt;0.001   
## Insulin                  128 (74.4)   165 (101)   &amp;lt;0.001   
## BMI                      30.9 (6.50) 35.4 (6.60)  &amp;lt;0.001   
## DiabetesPedigreeFunction 0.43 (0.30) 0.55 (0.37)  &amp;lt;0.001   
## Age                      31.2 (11.7) 37.1 (11.0)  &amp;lt;0.001   
## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see here that couple of variables are significantly higher in diabetic patients, such as Pregnancies, glucose, insulin, bmi and so on. This can give hints on what to expect from the linear model.&lt;/p&gt;
&lt;p&gt;So far, we made some visualizations to understand the dataset better, made some quality checks and cleaning.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Now, the data is ready for the modeling phase.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;modeling-the-data-build-fit-and-validate-a-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Modeling the data (build, fit and validate a model)&lt;/h1&gt;
&lt;p&gt;Before going into any complicated model starting with a simple model is a good idea. It might do surprisingly well and will give us more insights.&lt;/p&gt;
&lt;div id=&#34;model-assumptions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model assumptions&lt;/h2&gt;
&lt;p&gt;One of the assumptions of logistic regression that it requires large sample size.&lt;/p&gt;
&lt;div id=&#34;what-should-be-the-minimum-sample-size-for-running-logistic-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What should be the minimum sample size for running logistic regression?&lt;/h3&gt;
&lt;p&gt;Minimum sample size is given by the following formula:&lt;/p&gt;
&lt;p&gt;N = 10 k / p&lt;/p&gt;
&lt;p&gt;where,&lt;/p&gt;
&lt;p&gt;p is the proportion of the least frequent class of the Outcome variable. We have 768 cases of which 500 are diabetic and 268 non diabetic.&lt;/p&gt;
&lt;p&gt;p = 268/768 = 0.34&lt;/p&gt;
&lt;p&gt;And k is the number of covariates ( the number of predictor variables)&lt;/p&gt;
&lt;p&gt;k = 8&lt;/p&gt;
&lt;p&gt;N = 10 * 8 / 268/768
N = 229&lt;/p&gt;
&lt;p&gt;Since we have a total of 768 cases we can apply logistic regression model.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;logistic-regression-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Logistic Regression Model&lt;/h2&gt;
&lt;p&gt;We will create two random subsets of our data in 80/20 proportion as &lt;strong&gt;training and test data.&lt;/strong&gt; Training data will be used to build our model and test data will be reserved to validate it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(22)
# Create train test split
sample_rows &amp;lt;- sample(nrow(diabetes), nrow(diabetes) * 0.8)
# Create the training dataset
dia_train &amp;lt;- diabetes[sample_rows, ]
# Create the test dataset
dia_test &amp;lt;- diabetes[-sample_rows, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Build a logistic regression model with the train data
glm_dia &amp;lt;- glm(Outcome ~ .,data = dia_train, family = &amp;quot;binomial&amp;quot;)
summary(glm_dia)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = Outcome ~ ., family = &amp;quot;binomial&amp;quot;, data = dia_train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.5941  -0.7143  -0.3887   0.7330   2.1417  
## 
## Coefficients:
##                           Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)              -9.371525   0.926425 -10.116  &amp;lt; 2e-16 ***
## Pregnancies               0.108782   0.036022   3.020 0.002529 ** 
## Glucose                   0.036465   0.004309   8.462  &amp;lt; 2e-16 ***
## BloodPressure            -0.010359   0.009470  -1.094 0.273974    
## SkinThickness             0.005730   0.014928   0.384 0.701105    
## Insulin                  -0.002056   0.001279  -1.607 0.107995    
## BMI                       0.102303   0.021009   4.870 1.12e-06 ***
## DiabetesPedigreeFunction  1.154731   0.327099   3.530 0.000415 ***
## Age                       0.021617   0.010619   2.036 0.041780 *  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 793.94  on 613  degrees of freedom
## Residual deviance: 570.22  on 605  degrees of freedom
## AIC: 588.22
## 
## Number of Fisher Scoring iterations: 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The summary shows us not all the variables play a role in predicting outcome. The significant correlations was found for Pregnancies, Glucose, BMI and Pedigree function.&lt;/p&gt;
&lt;p&gt;The predict function will give us probabilities. To compute our model accuracy we need to &lt;strong&gt;convert them to class predictions by setting a threshold level.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# We will predict the Outcome for the test data
p&amp;lt;-predict(glm_dia, dia_test)
# Choose a threshold 0.5 to calculate the accuracy of our model
p_05 &amp;lt;- ifelse(p &amp;gt; 0.5, 1, 0)
table(p_05, dia_test$Outcome)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     
## p_05  0  1
##    0 88 23
##    1 12 31&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will build a confusion matrix to calculate how accurate our model is in this particular random train/test split and at 0.5 threshold level.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;conf_mat &amp;lt;- table(p_05, dia_test$Outcome)
accuracy &amp;lt;- sum(diag(conf_mat))/sum(conf_mat)
accuracy&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7727273&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;roc&lt;/strong&gt; function pROC package, can plot us a ROC curve which tests accuracy of our model at multiple threshold levels and is a good estimate on how well our model is performing.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Calculate AUC(Area under the curve)
roc(dia_test$Outcome, p)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Setting levels: control = 0, case = 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Setting direction: controls &amp;lt; cases&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## roc.default(response = dia_test$Outcome, predictor = p)
## 
## Data: p in 100 controls (dia_test$Outcome 0) &amp;lt; 54 cases (dia_test$Outcome 1).
## Area under the curve: 0.8446&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, this process is little fragile, presence or absence of a single outlier might vastly change the results you might get from a given random train/test split.&lt;/p&gt;
&lt;p&gt;A better approach than a simple train/test split is using multiple test sets and averaging their accuracies.&lt;/p&gt;
&lt;p&gt;Let’s test that. I will create 1, 30 or 1000 random test sets, build models and compare their accuracies.&lt;/p&gt;
&lt;div id=&#34;how-to-apply-multiple-traintest-split&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;How to apply multiple train/test split&lt;/h4&gt;
&lt;p&gt;To do this, I will &lt;strong&gt;write a function&lt;/strong&gt; where I can choose number of independent train/test splits.&lt;/p&gt;
&lt;p&gt;It will return me an average value of the accuracy(auc) of the model after chosen number of iteration. The higher the number of random splits the more stable your estimated AUC will be.&lt;/p&gt;
&lt;p&gt;Let’s see how it will work out for our diabetes patients.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# I will define my function as follows
multi_split &amp;lt;- function(x){
sample_rows &amp;lt;- list()
dia_train &amp;lt;- list()
dia_test &amp;lt;- list()
glm &amp;lt;- list()
p &amp;lt;-  list()
roc_auc &amp;lt;- list()
for(i in 1:x){
  sample_rows[[i]] &amp;lt;- sample(nrow(diabetes), nrow(diabetes) * 0.8)
  # Create the training dataset
  dia_train[[i]] &amp;lt;- diabetes[sample_rows[[i]], ]
  # Create the test dataset
  dia_test[[i]] &amp;lt;- diabetes[-sample_rows[[i]], ]
  glm[[i]] &amp;lt;- glm(Outcome ~ .,data = dia_train[[i]], family = &amp;quot;binomial&amp;quot;)
  p[[i]] &amp;lt;- predict(glm[[i]], dia_test[[i]])
  
  # Calculate AUC for all &amp;quot;x&amp;quot; number of random splits
  roc_auc[[i]] &amp;lt;- roc(dia_test[[i]]$Outcome, p[[i]])$auc[1]
  glm_mean &amp;lt;- mean(unlist(roc_auc))
}
print(mean(unlist(roc_auc)))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s calculate the average AUC of our model after different number of random splits.&lt;/p&gt;
&lt;p&gt;I will run my &lt;strong&gt;multi_split() function&lt;/strong&gt; 3x for 1, 30 and 1000 random train/test splits. I can then compare variances at each level of sampling.&lt;/p&gt;
&lt;p&gt;Here are the results from my multi_site function at each randomization.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;auc_1_1 &amp;lt;- multi_split(1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8737245&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;auc_1_2 &amp;lt;- multi_split(1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8271277&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;auc_1_3 &amp;lt;- multi_split(1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.824344&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;auc_30_1 &amp;lt;- multi_split(30)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8399321&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;auc_30_2 &amp;lt;- multi_split(30)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8492238&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;auc_30_3 &amp;lt;- multi_split(30)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8294277&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;auc_1000_1 &amp;lt;- multi_split(1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8356283&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;auc_1000_2 &amp;lt;- multi_split(1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.836047&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;auc_1000_3 &amp;lt;- multi_split(1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8345809&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s compare Variance levels at &lt;strong&gt;1, 30 and 1000&lt;/strong&gt; random splits&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var(c(auc_1_1, auc_1_2, auc_1_3))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0007695739&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var(c(auc_30_1, auc_30_2, auc_30_3))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 9.809343e-05&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var(c(auc_1000_1, auc_1000_2, auc_1000_3))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5.702925e-07&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What we see here as we increase the number of iterations to 30 and 1000 the variability
gradually stabilizes around a trustable AUC of &lt;code&gt;0.835&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Seeing is believing. Let’s plot it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a data.frame containing accuracies
random_1X &amp;lt;- c(auc_1_1, auc_1_2, auc_1_3)
random_30X &amp;lt;- c(auc_30_1, auc_30_2, auc_30_3)
random_1000X &amp;lt;- c(auc_1000_1, auc_1000_2, auc_1000_3)

df_r &amp;lt;- data.frame(random_1X, random_30X, random_1000X)
df_r&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   random_1X random_30X random_1000X
## 1 0.8737245  0.8399321    0.8356283
## 2 0.8271277  0.8492238    0.8360470
## 3 0.8243440  0.8294277    0.8345809&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Here, I will reformat my data for easy plotting by using gather() function from tidyr
# It takes multiple columns, and gathers them into key-value pairs: it makes “wide” data longer.
df_long &amp;lt;- gather(df_r, sampling, auc)
df_long&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       sampling       auc
## 1    random_1X 0.8737245
## 2    random_1X 0.8271277
## 3    random_1X 0.8243440
## 4   random_30X 0.8399321
## 5   random_30X 0.8492238
## 6   random_30X 0.8294277
## 7 random_1000X 0.8356283
## 8 random_1000X 0.8360470
## 9 random_1000X 0.8345809&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_long$sampling &amp;lt;- factor(df_long$sampling, levels = c(&amp;quot;random_1X&amp;quot;, &amp;quot;random_30X&amp;quot;, &amp;quot;random_1000X&amp;quot;))

# 
model_variation &amp;lt;- ggplot(df_long, aes(y=auc, x=sampling, fill=sampling)) + geom_boxplot() + theme(text = element_text(size=15), axis.title.x=element_blank(), legend.position = &amp;quot;none&amp;quot;) + ggtitle(&amp;quot;Variation in model performance&amp;quot;)
model_variation&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-01-can-machine-learning-change-your-life-style_files/figure-html/change%20in%20variance%20plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Great. We have an estimate of our model performance after 1000 random train/test splits. This process is also called &lt;strong&gt;Monte-Carlo Cross validation.&lt;/strong&gt; This approach might give you a less variable, but more biased estimate.&lt;/p&gt;
&lt;p&gt;A more common approach to estimate model performance is &lt;strong&gt;k-Fold cross Validation.&lt;/strong&gt; Where the samples divided into k-folds and one fold is used as a test set, and the remaining k-1 as the training set. This process is run k times until all folds appear once in the test sample.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;logistic-regression-model-with-k-fold-cross-validation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Logistic regression model with k-fold Cross Validation&lt;/h2&gt;
&lt;p&gt;I will switch here to caret package. With the &lt;strong&gt;Train()&lt;/strong&gt; function we can test different types of machine learning algorithms and set the cross validation parameters.&lt;/p&gt;
&lt;p&gt;To make the models below comparable I will create &lt;strong&gt;a custom cross validation fold object (d_folds)&lt;/strong&gt; that I can apply to multiple models.&lt;/p&gt;
&lt;p&gt;I will repeat the logistic regression model with 5 fold cross validation and then we can compare it to monte carlo cross validation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Convert Outcome to a factor with two levels
diabetes$Outcome &amp;lt;- ifelse(diabetes$Outcome == 1, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;)
outcome &amp;lt;- diabetes$Outcome
d_folds &amp;lt;- createFolds(outcome, k=5)

# Create a dataframe without the outcome column
diab &amp;lt;- diabetes[,-9]

# MyControl
myControl &amp;lt;- trainControl(
    summaryFunction = twoClassSummary,
    classProbs = TRUE,
    verboseIter = TRUE,
    savePredictions = TRUE,
    index = d_folds
)
# Model_glm
model_glm &amp;lt;- train(x = diab, y = outcome,
                   metric = &amp;quot;ROC&amp;quot;,
                   method = &amp;quot;glm&amp;quot;,
                   family = binomial(),
                   trControl = myControl
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## + Fold1: parameter=none 
## - Fold1: parameter=none 
## + Fold2: parameter=none 
## - Fold2: parameter=none 
## + Fold3: parameter=none 
## - Fold3: parameter=none 
## + Fold4: parameter=none 
## - Fold4: parameter=none 
## + Fold5: parameter=none 
## - Fold5: parameter=none 
## Aggregating results
## Fitting final model on full training set&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_glm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Generalized Linear Model 
## 
## 768 samples
##   8 predictor
##   2 classes: &amp;#39;No&amp;#39;, &amp;#39;Yes&amp;#39; 
## 
## No pre-processing
## Resampling: Bootstrapped (5 reps) 
## Summary of sample sizes: 154, 153, 153, 154, 154 
## Resampling results:
## 
##   ROC        Sens   Spec    
##   0.8136093  0.844  0.577431&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, My model performance is &lt;code&gt;0.8136093&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;glmnet-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Glmnet model&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Model
model_glmnet &amp;lt;- train(x = diab, y = outcome,
                   metric = &amp;quot;ROC&amp;quot;,
                   method = &amp;quot;glmnet&amp;quot;, tuneGrid = expand.grid(
                          alpha = 0:1,
                          lambda = seq(0.0001, 1, length = 20)
                      ),
                   trControl = myControl
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_glmnet&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## glmnet 
## 
## 768 samples
##   8 predictor
##   2 classes: &amp;#39;No&amp;#39;, &amp;#39;Yes&amp;#39; 
## 
## No pre-processing
## Resampling: Bootstrapped (5 reps) 
## Summary of sample sizes: 154, 153, 153, 154, 154 
## Resampling results across tuning parameters:
## 
##   alpha  lambda      ROC        Sens    Spec       
##   0      0.00010000  0.8201210  0.8615  0.554096935
##   0      0.05272632  0.8227469  0.8725  0.541977831
##   0      0.10535263  0.8239613  0.8880  0.506537709
##   0      0.15797895  0.8242214  0.9005  0.482290806
##   0      0.21060526  0.8240544  0.9115  0.444025212
##   0      0.26323158  0.8238034  0.9210  0.420725929
##   0      0.31585789  0.8233937  0.9290  0.395548794
##   0      0.36848421  0.8230260  0.9365  0.357313627
##   0      0.42111053  0.8228119  0.9430  0.331201913
##   0      0.47373684  0.8224972  0.9505  0.306955010
##   0      0.52636316  0.8222639  0.9575  0.276166051
##   0      0.57898947  0.8220894  0.9615  0.254705499
##   0      0.63161579  0.8219008  0.9670  0.238852423
##   0      0.68424211  0.8217353  0.9690  0.217396218
##   0      0.73686842  0.8215489  0.9705  0.200604216
##   0      0.78949474  0.8214394  0.9725  0.178235166
##   0      0.84212105  0.8212995  0.9755  0.157704847
##   0      0.89474737  0.8210990  0.9785  0.138109107
##   0      0.94737368  0.8209896  0.9800  0.125976962
##   0      1.00000000  0.8208614  0.9805  0.111980004
##   1      0.00010000  0.8141708  0.8450  0.575553141
##   1      0.05272632  0.8232373  0.9040  0.505646599
##   1      0.10535263  0.8064789  0.9490  0.348028689
##   1      0.15797895  0.7982626  0.9865  0.118687242
##   1      0.21060526  0.7913587  0.9995  0.002803738
##   1      0.26323158  0.5552114  1.0000  0.000000000
##   1      0.31585789  0.5000000  1.0000  0.000000000
##   1      0.36848421  0.5000000  1.0000  0.000000000
##   1      0.42111053  0.5000000  1.0000  0.000000000
##   1      0.47373684  0.5000000  1.0000  0.000000000
##   1      0.52636316  0.5000000  1.0000  0.000000000
##   1      0.57898947  0.5000000  1.0000  0.000000000
##   1      0.63161579  0.5000000  1.0000  0.000000000
##   1      0.68424211  0.5000000  1.0000  0.000000000
##   1      0.73686842  0.5000000  1.0000  0.000000000
##   1      0.78949474  0.5000000  1.0000  0.000000000
##   1      0.84212105  0.5000000  1.0000  0.000000000
##   1      0.89474737  0.5000000  1.0000  0.000000000
##   1      0.94737368  0.5000000  1.0000  0.000000000
##   1      1.00000000  0.5000000  1.0000  0.000000000
## 
## ROC was used to select the optimal model using the largest value.
## The final values used for the model were alpha = 0 and lambda = 0.1579789.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(model_glmnet)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-01-can-machine-learning-change-your-life-style_files/figure-html/print%20Glmnet%20model-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we see in the plot, ridge regression (alpha = 0) performed better than the lasso at all lambda values.&lt;/p&gt;
&lt;p&gt;Glmnet model performance is &lt;code&gt;0.8242214&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-forest-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Random forest model&lt;/h2&gt;
&lt;p&gt;One of the big diferences between random forest and linear models is that they require “tuning.”&lt;/p&gt;
&lt;p&gt;Hyperparameters –&amp;gt; How the model is fit. Selected by hand.&lt;/p&gt;
&lt;p&gt;advantages: no need to log transform or normalize,
but they are less interpretable and slower than glmnet.&lt;/p&gt;
&lt;p&gt;Random forests &lt;strong&gt;capture threshold effects and variable interactions. both of which occur often in real world data&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;mtry&lt;/strong&gt; is the number of variables used at each split point in individual decision tree that make up the rf. Default is 3, I will use here 8.&lt;/p&gt;
&lt;p&gt;tuneLength = how many different mtry values to be tested.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Random forest model
model_rf &amp;lt;- train(x = diab, y = outcome,
                   tuneLength = 8,
                   metric = &amp;quot;ROC&amp;quot;,
                   method = &amp;quot;ranger&amp;quot;,
                   trControl = myControl
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_rf&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Random Forest 
## 
## 768 samples
##   8 predictor
##   2 classes: &amp;#39;No&amp;#39;, &amp;#39;Yes&amp;#39; 
## 
## No pre-processing
## Resampling: Bootstrapped (5 reps) 
## Summary of sample sizes: 154, 153, 153, 154, 154 
## Resampling results across tuning parameters:
## 
##   mtry  splitrule   ROC        Sens    Spec     
##   2     gini        0.8155919  0.8565  0.5410389
##   2     extratrees  0.8247223  0.8730  0.5419474
##   3     gini        0.8142703  0.8490  0.5596827
##   3     extratrees  0.8241664  0.8650  0.5550185
##   4     gini        0.8098886  0.8440  0.5559617
##   4     extratrees  0.8244122  0.8595  0.5671376
##   5     gini        0.8096393  0.8475  0.5596914
##   5     extratrees  0.8244900  0.8550  0.5708498
##   6     gini        0.8075402  0.8460  0.5578309
##   6     extratrees  0.8231397  0.8530  0.5755401
##   7     gini        0.8063892  0.8415  0.5662117
##   7     extratrees  0.8213101  0.8515  0.5652728
##   8     gini        0.8058247  0.8335  0.5717931
##   8     extratrees  0.8209827  0.8485  0.5773788
## 
## Tuning parameter &amp;#39;min.node.size&amp;#39; was held constant at a value of 1
## ROC was used to select the optimal model using the largest value.
## The final values used for the model were mtry = 2, splitrule =
##  extratrees and min.node.size = 1.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Random forest performance is &lt;code&gt;0.8247223&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gradient-boost-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gradient boost model&lt;/h2&gt;
&lt;p&gt;I will define manualy a grid to test hyperparameter values wider than set in default.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grid &amp;lt;- expand.grid(interaction.depth = c(1, 2, 3, 4, 5),
                 n.trees = (1:20)*50,  shrinkage = 0.01,
             n.minobsinnode = 10)
model_gbm &amp;lt;- train(x = diab, y = outcome,
                   metric = &amp;quot;ROC&amp;quot;,
                   method = &amp;quot;gbm&amp;quot;,
                   tuneGrid = grid,
                   trControl = myControl
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_gbm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Stochastic Gradient Boosting 
## 
## 768 samples
##   8 predictor
##   2 classes: &amp;#39;No&amp;#39;, &amp;#39;Yes&amp;#39; 
## 
## No pre-processing
## Resampling: Bootstrapped (5 reps) 
## Summary of sample sizes: 154, 153, 153, 154, 154 
## Resampling results across tuning parameters:
## 
##   interaction.depth  n.trees  ROC        Sens    Spec      
##   1                    50     0.7996312  0.9850  0.09252336
##   1                   100     0.8103617  0.9505  0.32756357
##   1                   150     0.8153748  0.9275  0.42350793
##   1                   200     0.8180549  0.9100  0.48598131
##   1                   250     0.8213254  0.8965  0.51863943
##   1                   300     0.8221740  0.8860  0.52983699
##   1                   350     0.8210830  0.8815  0.53729189
##   1                   400     0.8218884  0.8740  0.55409694
##   1                   450     0.8222721  0.8680  0.55874375
##   1                   500     0.8212831  0.8625  0.55593567
##   1                   550     0.8222189  0.8595  0.56525973
##   1                   600     0.8210507  0.8540  0.56620300
##   1                   650     0.8206810  0.8510  0.56806781
##   1                   700     0.8197731  0.8495  0.56806781
##   1                   750     0.8194073  0.8495  0.57180178
##   1                   800     0.8179990  0.8460  0.57459683
##   1                   850     0.8173591  0.8435  0.57459683
##   1                   900     0.8167653  0.8400  0.57272332
##   1                   950     0.8159870  0.8395  0.57366225
##   1                  1000     0.8155800  0.8375  0.57738752
##   2                    50     0.8097645  0.9705  0.19896110
##   2                   100     0.8153114  0.9225  0.42633341
##   2                   150     0.8182821  0.9040  0.47947837
##   2                   200     0.8214800  0.8910  0.51491415
##   2                   250     0.8209545  0.8780  0.53262769
##   2                   300     0.8209278  0.8655  0.54569007
##   2                   350     0.8203395  0.8595  0.55967398
##   2                   400     0.8193341  0.8560  0.56620735
##   2                   450     0.8187081  0.8520  0.57274071
##   2                   500     0.8177495  0.8475  0.58300369
##   2                   550     0.8172341  0.8470  0.58206912
##   2                   600     0.8160518  0.8430  0.58579005
##   2                   650     0.8150557  0.8335  0.58580309
##   2                   700     0.8142389  0.8285  0.58765486
##   2                   750     0.8129551  0.8265  0.59045860
##   2                   800     0.8114235  0.8235  0.58859378
##   2                   850     0.8106544  0.8230  0.58951967
##   2                   900     0.8098655  0.8225  0.58952402
##   2                   950     0.8089927  0.8195  0.59231471
##   2                  1000     0.8085903  0.8185  0.59044990
##   3                    50     0.8124471  0.9675  0.23434471
##   3                   100     0.8166505  0.9160  0.43008042
##   3                   150     0.8183159  0.8875  0.50374701
##   3                   200     0.8170682  0.8700  0.52424690
##   3                   250     0.8167332  0.8610  0.54665942
##   3                   300     0.8170788  0.8535  0.55690067
##   3                   350     0.8155400  0.8470  0.57181917
##   3                   400     0.8140688  0.8410  0.57181482
##   3                   450     0.8124477  0.8355  0.57367529
##   3                   500     0.8114718  0.8325  0.57367963
##   3                   550     0.8100906  0.8275  0.57740926
##   3                   600     0.8092354  0.8230  0.58113888
##   3                   650     0.8082420  0.8220  0.58859813
##   3                   700     0.8065477  0.8190  0.58766355
##   3                   750     0.8062263  0.8160  0.58858944
##   3                   800     0.8044913  0.8130  0.58765486
##   3                   850     0.8029262  0.8125  0.58765486
##   3                   900     0.8020108  0.8110  0.58858944
##   3                   950     0.8016405  0.8105  0.59045860
##   3                  1000     0.8011373  0.8090  0.59232775
##   4                    50     0.8139509  0.9640  0.24734188
##   4                   100     0.8150242  0.9120  0.43657900
##   4                   150     0.8179537  0.8850  0.49533145
##   4                   200     0.8176002  0.8710  0.53356227
##   4                   250     0.8162216  0.8615  0.55220170
##   4                   300     0.8159643  0.8490  0.56901543
##   4                   350     0.8141087  0.8445  0.57553141
##   4                   400     0.8131285  0.8430  0.57832645
##   4                   450     0.8114524  0.8385  0.58020430
##   4                   500     0.8104583  0.8335  0.58671158
##   4                   550     0.8089872  0.8265  0.58764182
##   4                   600     0.8078067  0.8240  0.59137144
##   4                   650     0.8064856  0.8230  0.59418387
##   4                   700     0.8051274  0.8200  0.59044121
##   4                   750     0.8038530  0.8205  0.59324495
##   4                   800     0.8030680  0.8205  0.59699196
##   4                   850     0.8023061  0.8190  0.59793088
##   4                   900     0.8011695  0.8185  0.59606607
##   4                   950     0.8000060  0.8160  0.59326668
##   4                  1000     0.7988982  0.8140  0.59045860
##   5                    50     0.8124004  0.9640  0.23430124
##   5                   100     0.8128960  0.9085  0.43658770
##   5                   150     0.8164498  0.8875  0.50560313
##   5                   200     0.8159795  0.8755  0.53263638
##   5                   250     0.8154252  0.8595  0.55782221
##   5                   300     0.8146149  0.8500  0.56622473
##   5                   350     0.8132774  0.8420  0.57460987
##   5                   400     0.8123588  0.8405  0.58207781
##   5                   450     0.8111443  0.8355  0.58207781
##   5                   500     0.8090119  0.8295  0.58767225
##   5                   550     0.8078390  0.8255  0.58488589
##   5                   600     0.8068005  0.8255  0.58954575
##   5                   650     0.8047019  0.8230  0.59045425
##   5                   700     0.8033723  0.8215  0.59231906
##   5                   750     0.8024794  0.8210  0.58951967
##   5                   800     0.8019000  0.8190  0.59232341
##   5                   850     0.8006418  0.8185  0.59512715
##   5                   900     0.7998935  0.8170  0.59605303
##   5                   950     0.7984835  0.8170  0.59698326
##   5                  1000     0.7970453  0.8165  0.59418822
## 
## Tuning parameter &amp;#39;shrinkage&amp;#39; was held constant at a value of 0.01
## 
## Tuning parameter &amp;#39;n.minobsinnode&amp;#39; was held constant at a value of 10
## ROC was used to select the optimal model using the largest value.
## The final values used for the model were n.trees = 450,
##  interaction.depth = 1, shrinkage = 0.01 and n.minobsinnode = 10.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Gradient boost model performance is &lt;code&gt;0.8222721&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;naive-bayes-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Naive Bayes model&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_nb &amp;lt;- train(x = diab, y = outcome,
                   metric = &amp;quot;ROC&amp;quot;,
                   method = &amp;quot;nb&amp;quot;,
                   trControl = myControl
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_nb&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Naive Bayes 
## 
## 768 samples
##   8 predictor
##   2 classes: &amp;#39;No&amp;#39;, &amp;#39;Yes&amp;#39; 
## 
## No pre-processing
## Resampling: Bootstrapped (5 reps) 
## Summary of sample sizes: 154, 153, 153, 154, 154 
## Resampling results across tuning parameters:
## 
##   usekernel  ROC        Sens    Spec     
##   FALSE      0.8029047  0.8280  0.5801608
##    TRUE      0.7895701  0.8145  0.5745447
## 
## Tuning parameter &amp;#39;fL&amp;#39; was held constant at a value of 0
## Tuning
##  parameter &amp;#39;adjust&amp;#39; was held constant at a value of 1
## ROC was used to select the optimal model using the largest value.
## The final values used for the model were fL = 0, usekernel = FALSE
##  and adjust = 1.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Naive Bayes model performance is &lt;code&gt;0.8029047&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;models &amp;lt;- c(&amp;quot;glm&amp;quot;, &amp;quot;glmnet&amp;quot;, &amp;quot;rf&amp;quot;, &amp;quot;gbm&amp;quot;, &amp;quot;naive&amp;quot;)
glm &amp;lt;- max(model_glm$results$ROC)
glmnet &amp;lt;- max(model_glmnet$results$ROC)
rf &amp;lt;- max(model_rf$results$ROC)
gbm &amp;lt;- max(model_gbm$results$ROC)
naive &amp;lt;- max(model_nb$results$ROC)
AUC &amp;lt;- c(glm, glmnet, rf, gbm, naive)
df &amp;lt;- data.frame(models, AUC)
df&amp;lt;- df[order(df[,2], decreasing=TRUE), ]
knitr::kable(df)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
models
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
AUC
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
rf
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8247223
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
glmnet
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8242214
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
gbm
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8222721
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
glm
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8136093
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
naive
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8029047
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here, we found &lt;code&gt;rf&lt;/code&gt; model performed the best, and also there are not big differences between the models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;future-thoughts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Future thoughts&lt;/h2&gt;
&lt;p&gt;I used different machine learning algorithms to predict Diabetes. Models showed similar performances except the naives bayes which performed worst. &lt;strong&gt;As we saw, our simple glm model performance was very close to other more advanced algorithms.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We can help doctors to predict &lt;strong&gt;Diabetes with accuracy around 83%&lt;/strong&gt; by using 8 simple medical parameters.&lt;/p&gt;
&lt;p&gt;Given current speed in generation and collection of types data by including additional predictors we can build even better models.&lt;/p&gt;
&lt;p&gt;Until next time!&lt;/p&gt;
&lt;p&gt;Serdar&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
